{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of Week6_IR_vector_space_model_2022.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Malherbe/Text_Mining/blob/main/Copy_of_Week6_IR_vector_space_model_2022.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ckz7473QXAch"
      },
      "source": [
        "#Week 5- Information Retrieval - Vector Space Model "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6m-aUwyPldPA"
      },
      "source": [
        "#Part 2.More complex example\n",
        "\n",
        "**Query and find the most Relevant NTHU GEC Course syllabuses**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0X896putz-zX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "af87f849-7687-4f7d-c3f2-60d80e1e6560"
      },
      "source": [
        "#Mount google drive to google colab virtual machine\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "mydrive =\"/content/drive/My Drive/Colab Notebooks/\""
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7z5J7dhxAF2X"
      },
      "source": [
        "**Step2: run the following code create a folder name \"txt\", and convert all file in folder \"pdf\"  to folder \"txt\"**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3rELK0HbAFEH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6f2d7029-195b-4c34-c947-bed91edc7644"
      },
      "source": [
        "#Run to load pdf to txt function\n",
        "!pip3 install pdfminer.six #package pdf to text\n",
        "import os\n",
        "from io import StringIO\n",
        "from pdfminer.pdfinterp import PDFResourceManager, PDFPageInterpreter\n",
        "from pdfminer.converter import TextConverter\n",
        "from pdfminer.layout import LAParams\n",
        "from pdfminer.pdfpage import PDFPage\n",
        "\n",
        "def convert(fname, pages=None):\n",
        "    if not pages:\n",
        "        pagenums = set()\n",
        "    else:\n",
        "        pagenums = set(pages)\n",
        "\n",
        "    output = StringIO()\n",
        "    manager = PDFResourceManager()\n",
        "    converter = TextConverter(manager, output, codec='utf-8', laparams=LAParams())\n",
        "    interpreter = PDFPageInterpreter(manager, converter)\n",
        "    infile = open(fname, 'rb')\n",
        "    for page in PDFPage.get_pages(infile, pagenums):\n",
        "        interpreter.process_page(page)\n",
        "    infile.close()\n",
        "    converter.close()\n",
        "    text = output.getvalue()\n",
        "    output.close\n",
        "    return text\n",
        "    \n",
        "        \n",
        "def convertMultiple(pdfDir, txtDir):\n",
        "    for pdf in os.listdir(pdfDir): #iterate through pdfs in pdf directory\n",
        "        fileExtension = pdf.split(\".\")[-1]\n",
        "        if fileExtension == \"pdf\":\n",
        "            pdfFilename = pdfDir + pdf \n",
        "            text = convert(pdfFilename) #get string of text content of pdf\n",
        "            textFilename = txtDir + pdf[:-4] + \".txt\"\n",
        "            textFile = open(textFilename, \"w\") #make text file\n",
        "            textFile.write(text) #write text to text file\n",
        "            textFile.close()\n",
        "            print(\"finish convert to txt\", pdf)\n",
        "    print(\"finish convert all file\")\n",
        "\n",
        "\n",
        "#create directory txt\n",
        "#os.mkdir(mydrive+ 'txt') # you have to remove this line if the txt folder is exist.\n",
        "# covert all file in folder pdf to text file and store in folder \"txt\"\n",
        "mydrive =\"/content/drive/My Drive/Colab Notebooks/\"\n",
        "os.makedirs(mydrive+\"txt\",exist_ok=True) #make txt folder to save text file after convert pdf to txt\n",
        "pdfdir= mydrive + \"pdf/\"\n",
        "txtdir= mydrive +\"txt/\"\n",
        "convertMultiple(pdfdir,txtdir)"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pdfminer.six in /usr/local/lib/python3.7/dist-packages (20220319)\n",
            "Requirement already satisfied: cryptography in /usr/local/lib/python3.7/dist-packages (from pdfminer.six) (36.0.2)\n",
            "Requirement already satisfied: chardet in /usr/local/lib/python3.7/dist-packages (from pdfminer.six) (3.0.4)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.7/dist-packages (from cryptography->pdfminer.six) (1.15.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi>=1.12->cryptography->pdfminer.six) (2.21)\n",
            "finish convert to txt Week3-From Textual Information to Numerical Vector.pdf\n",
            "finish convert to txt Retrieval model - vector space.pdf\n",
            "finish convert to txt IR Basic.pdf\n",
            "finish convert to txt Week5 _ Regular expression - NER .pdf\n",
            "finish convert to txt Chapter 2- From Textual Information to Numerical Vectors.pdf\n",
            "finish convert all file\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P44tQKGFBEen"
      },
      "source": [
        "**Begin Information Retrieval processing** . \n",
        "\n",
        "This code will find the best relevant syllabuses from data with your query search. (user input text)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tglF_Jbr4_VF"
      },
      "source": [
        "# create tf function\n",
        "def tf(term, token_doc):\n",
        "    tf = token_doc.count(term)/len(token_doc)\n",
        "    return tf\n",
        "\n",
        "# create function to calculate how many doc contain the term \n",
        "def numDocsContaining(word, token_doclist):\n",
        "    doccount = 0\n",
        "    for doc_token in token_doclist:\n",
        "        if doc_token.count(word) > 0:\n",
        "            doccount +=1\n",
        "    return doccount\n",
        "  \n",
        "import math\n",
        "# create function to calculate  Inverse Document Frequency in doclist - this list of all documents\n",
        "def idf(word, token_doclist):\n",
        "    n = len(token_doclist)\n",
        "    df = numDocsContaining(word, token_doclist)\n",
        "    return math.log10(n/df)\n",
        "\n",
        "#define a function to do cosine normalization a data dictionary\n",
        "def cos_norm(dic): # dic is distionary data structure\n",
        "  import numpy as np\n",
        "  dic_norm={}\n",
        "  factor=1.0/np.sqrt(sum([np.square(i) for i in dic.values()]))\n",
        "  for k in dic:\n",
        "    dic_norm[k] = dic[k]*factor\n",
        "  return dic_norm\n",
        "\n",
        "#create function to calculate normalize tfidf \n",
        "def compute_tfidf(token_doc,bag_words_idf):\n",
        "  tfidf_doc={}\n",
        "  for word in set(token_doc):\n",
        "    tfidf_doc[word]= tf(word,token_doc) * bag_words_idf[word]   \n",
        "  tfidf_norm = cos_norm(tfidf_doc)\n",
        "  return tfidf_norm\n",
        "\n",
        "# create normalize term frequency\n",
        "def tf_norm(token_doc):\n",
        "  tf_norm={}\n",
        "  for term in token_doc:\n",
        "    tf = token_doc.count(term)/len(token_doc)\n",
        "    tf_norm[term]=tf\n",
        "  tf_max = max(tf_norm.values())\n",
        "  for term, value in tf_norm.items():\n",
        "    tf_norm[term]= 0.5 + 0.5*value/tf_max\n",
        "  return tf_norm\n",
        "\n",
        "def compute_tfidf_query(query_token,bag_words_idf):\n",
        "  tfidf_query={}\n",
        "  tf_norm_query = tf_norm(query_token)\n",
        "  for term, value in tf_norm_query.items():\n",
        "    tfidf_query[term]=value*bag_words_idf[term]   \n",
        "  return tfidf_query"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fqFwk2jUGlZY"
      },
      "source": [
        "*We use the bigger dictionary that has better support tokenization for traditional Chinese (繁體) at \n",
        "https://github.com/fxsjy/jieba/raw/master/extra_dict/dict.txt.big. we download the jieba big dictionary and save to /content/drive/My Drive/Colab Notebooks/chinese/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rnTmdwmKFbn8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "d486096c-edc3-44da-95e5-9422de9a2634"
      },
      "source": [
        "#install wget if you not yet install wget\n",
        "!pip install wget\n",
        "#Download jieba big dictionary from github\n",
        "import wget,os\n",
        "#os.mkdir(mydrive+ \"chinese\") # you have to remove this line if the chinese folder is exist.\n",
        "url_bigdict = 'https://github.com/fxsjy/jieba/raw/master/extra_dict/dict.txt.big'\n",
        "wget.download(url_bigdict, mydrive)"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: wget in /usr/local/lib/python3.7/dist-packages (3.2)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/drive/My Drive/Colab Notebooks//dict.txt (1).big'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mi0KL9aF4qeW"
      },
      "source": [
        "## **Build our stopwords (both chinese and English)**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Regular Expression and Stopwords"
      ],
      "metadata": {
        "id": "usgbwLJbyX8I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re # Import regular Expression Package to deal with term \n",
        "\n",
        "def clean_word(word):\n",
        "  word = word.replace('\\\"','').replace('\\r','').replace('\\n','').replace('\\t','').replace(' ','')\n",
        "  word = re.sub(\"[\\s+\\.\\!\\/_,$%^*(+\\\"\\']+|[+——！，。？、~@#￥%……&*（）]\", \"\",word)\n",
        "  return word"
      ],
      "metadata": {
        "id": "3zQztUEpybpQ"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lbdeipv4J45A",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "00cb3a32-71d2-4f07-d711-1c120a931068"
      },
      "source": [
        "#download chinese stopwords file at https://raw.githubusercontent.com/stopwords-iso/stopwords-zh/master/stopwords-zh.txt\n",
        "# and save to your computer, you also add more chinese stopwords\n",
        "import requests\n",
        "\n",
        "!pip3 install opencc-python-reimplemented\n",
        "from opencc import OpenCC\n",
        "cc = OpenCC('s2t')\n",
        "\n",
        "#url = 'https://raw.githubusercontent.com/stopwords-iso/stopwords-zh/master/stopwords-zh.txt'\n",
        "#r = requests.get(url, allow_redirects=True)\n",
        "#open('chinese_stopwords.txt', 'wb').write(r.content)\n",
        "\n",
        "url = 'https://github.com/fxsjy/jieba/raw/master/extra_dict/dict.txt.big'\n",
        "r = requests.get(url, allow_redirects=True)\n",
        "\n",
        "open('dict.txt.big', 'wb').write(r.content)\n",
        "\n",
        "zh_stopwords_path=\"dict.txt.big\"\n",
        "zh_stopwords_path = [cc.convert(line.strip()) for line in open(zh_stopwords_path, 'r', encoding='utf-8').readlines()]\n",
        "\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "## Merge chinese stopwords and english stopwords\n",
        "en_stopwords = stopwords.words('english')\n",
        "stopwords= zh_stopwords_path + en_stopwords"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: opencc-python-reimplemented in /usr/local/lib/python3.7/dist-packages (0.1.6)\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(stopwords)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n556AoU9Uf1V",
        "outputId": "ccce488b-518c-4101-d899-b2eb808e1730"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "584608"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Use CKIP transformer "
      ],
      "metadata": {
        "id": "DJdrD51mQCFT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 install -U transformers\n",
        "!pip3 install -U ckip-transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "moia7suG8CIv",
        "outputId": "60f4a34f-c488-4138-b71c-e4afef520027"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.17.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.6.0)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.49)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.11.6)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.63.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.11.3)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.4.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.5)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.7.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: ckip-transformers in /usr/local/lib/python3.7/dist-packages (0.2.8)\n",
            "Requirement already satisfied: transformers>=3.5.0 in /usr/local/lib/python3.7/dist-packages (from ckip-transformers) (4.17.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from ckip-transformers) (4.63.0)\n",
            "Requirement already satisfied: torch>=1.5.0 in /usr/local/lib/python3.7/dist-packages (from ckip-transformers) (1.10.0+cu111)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.5.0->ckip-transformers) (3.10.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers>=3.5.0->ckip-transformers) (2019.12.20)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers>=3.5.0->ckip-transformers) (1.21.5)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers>=3.5.0->ckip-transformers) (4.11.3)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers>=3.5.0->ckip-transformers) (6.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers>=3.5.0->ckip-transformers) (3.6.0)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers>=3.5.0->ckip-transformers) (0.0.49)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers>=3.5.0->ckip-transformers) (0.4.0)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from transformers>=3.5.0->ckip-transformers) (0.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers>=3.5.0->ckip-transformers) (2.23.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers>=3.5.0->ckip-transformers) (21.3)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers>=3.5.0->ckip-transformers) (3.0.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers>=3.5.0->ckip-transformers) (3.7.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers>=3.5.0->ckip-transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers>=3.5.0->ckip-transformers) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers>=3.5.0->ckip-transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers>=3.5.0->ckip-transformers) (2021.10.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers>=3.5.0->ckip-transformers) (1.1.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers>=3.5.0->ckip-transformers) (7.1.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers>=3.5.0->ckip-transformers) (1.15.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import (\n",
        "  BertTokenizerFast,\n",
        "  AutoModel\n",
        ")\n",
        "\n",
        "tokenizer = BertTokenizerFast.from_pretrained('bert-base-chinese')\n",
        "model = AutoModel.from_pretrained('ckiplab/albert-tiny-chinese-ner')\n",
        "\n",
        "# Initialize drivers\n",
        "from ckip_transformers.nlp import CkipWordSegmenter, CkipPosTagger, CkipNerChunker\n",
        "\n",
        "ws_driver = CkipWordSegmenter(level=1)\n",
        "pos_driver = CkipPosTagger(level=1)\n",
        "ner_driver = CkipNerChunker(level=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WhljbUfe7zuJ",
        "outputId": "1223bffc-e7c3-4830-e801-3c0a7ae6c4a1"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at ckiplab/albert-tiny-chinese-ner were not used when initializing AlbertModel: ['classifier.weight', 'classifier.bias']\n",
            "- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of AlbertModel were not initialized from the model checkpoint at ckiplab/albert-tiny-chinese-ner and are newly initialized: ['albert.pooler.bias', 'albert.pooler.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# The data return from CKIP is in multiple level structure, so I have to transform it into 1 tier list\n",
        "\n",
        "flat_list = list()\n",
        "def flatten_list(list_of_lists): # This function is iterative function, which means it will call itself iteratively. \n",
        "    for item in list_of_lists:\n",
        "        if type(item) == list:\n",
        "            flatten_list(item)\n",
        "        else:\n",
        "            flat_list.append(item)  \n",
        "    return flat_list"
      ],
      "metadata": {
        "id": "0DX4svfgsTbm"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w170WPKFmbjX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a7968212-8130-4dc4-908e-fdd69a323ee1"
      },
      "source": [
        "# this code will read all file in txt folder, tokenize using jieba, remove punctuation, remove stopword and combine all file into doc_all\n",
        "#import jieba,os\n",
        "#jieba.load_userdict(mydrive + \"dict.txt.big\")\n",
        "\n",
        "file_path = mydrive +\"txt/\"\n",
        "doc_all_2={}\n",
        "# Our unit of analysis is \"document(pdf)\"\n",
        "for filename in os.listdir(file_path ):\n",
        "  fileExtension = filename.split(\".\")[-1]\n",
        "  if fileExtension == \"txt\":\n",
        "    chinese_text = open(file_path+filename).read()\n",
        "    text = [p for p in chinese_text.split('\\n') if len(p) != 0]\n",
        "\n",
        "    #tokens = list(jieba.cut(text))\n",
        "    tokens  = list(ws_driver(text, use_delim=True, batch_size=256, max_length=128))\n",
        "    flat_list = list()\n",
        "    tokens = flatten_list(tokens)\n",
        "\n",
        "    token_filtered = [clean_word(w) for w in tokens if len(w)>1 and not w in stopwords] # Skip words in stopwords\n",
        "    doc_all_2[filename[:-4]]=tokens\n",
        "\n",
        "#    break\n"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Tokenization: 100%|██████████| 153/153 [00:00<00:00, 16475.28it/s]\n",
            "Inference: 100%|██████████| 1/1 [00:03<00:00,  3.80s/it]\n",
            "Tokenization: 100%|██████████| 586/586 [00:00<00:00, 30255.82it/s]\n",
            "Inference: 100%|██████████| 3/3 [00:11<00:00,  3.98s/it]\n",
            "Tokenization: 100%|██████████| 843/843 [00:00<00:00, 25665.25it/s]\n",
            "Inference: 100%|██████████| 4/4 [00:24<00:00,  6.08s/it]\n",
            "Tokenization: 100%|██████████| 121/121 [00:00<00:00, 18484.51it/s]\n",
            "Inference: 100%|██████████| 1/1 [00:02<00:00,  2.71s/it]\n",
            "Tokenization: 100%|██████████| 1358/1358 [00:00<00:00, 5155.65it/s]\n",
            "Inference: 100%|██████████| 8/8 [00:32<00:00,  4.03s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "doc_all_2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qb2a-iatM1fH",
        "outputId": "3f5d9f51-2afd-40ba-b515-f921ba6e1791"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'Chapter 2- From Textual Information to Numerical Vectors': ['2',\n",
              "  'From',\n",
              "  ' ',\n",
              "  'Textual',\n",
              "  ' ',\n",
              "  'Information',\n",
              "  ' to',\n",
              "  'Numerical',\n",
              "  ' ',\n",
              "  'Vectors',\n",
              "  'To mine',\n",
              "  ' text',\n",
              "  ',',\n",
              "  ' we',\n",
              "  ' ',\n",
              "  'ﬁrst',\n",
              "  ' need',\n",
              "  ' to',\n",
              "  ' process',\n",
              "  ' it',\n",
              "  ' into',\n",
              "  ' a',\n",
              "  ' form',\n",
              "  ' ',\n",
              "  'that',\n",
              "  ' data-mining',\n",
              "  'procedures',\n",
              "  ' can',\n",
              "  ' use. As',\n",
              "  ' mentioned',\n",
              "  ' in',\n",
              "  ' the',\n",
              "  ' previous',\n",
              "  ' chapter',\n",
              "  ',',\n",
              "  ' this',\n",
              "  ' typi',\n",
              "  '-',\n",
              "  'cally',\n",
              "  ' involves',\n",
              "  ' generating',\n",
              "  ' features',\n",
              "  ' in a',\n",
              "  ' spreadsheet',\n",
              "  ' format. ',\n",
              "  'Classical',\n",
              "  'data',\n",
              "  ' mining',\n",
              "  ' looks',\n",
              "  ' at',\n",
              "  ' highly',\n",
              "  ' structured',\n",
              "  ' data. ',\n",
              "  'Our',\n",
              "  ' spre',\n",
              "  'ad',\n",
              "  'sheet',\n",
              "  ' model',\n",
              "  'is',\n",
              "  ' the',\n",
              "  ' embodiment',\n",
              "  ' of',\n",
              "  ' a',\n",
              "  ' representation',\n",
              "  ' that',\n",
              "  ' is',\n",
              "  ' supportive',\n",
              "  ' of',\n",
              "  ' predictive',\n",
              "  'modeling. In',\n",
              "  ' some',\n",
              "  ' ways',\n",
              "  ',',\n",
              "  ' predictive',\n",
              "  ' text mining',\n",
              "  ' is',\n",
              "  ' simpler',\n",
              "  ' and',\n",
              "  ' more',\n",
              "  'restrictive',\n",
              "  ' than',\n",
              "  ' open-ended',\n",
              "  ' data',\n",
              "  ' mining. ',\n",
              "  'Because',\n",
              "  ' predictive',\n",
              "  ' mining',\n",
              "  'methods',\n",
              "  ' are so highly',\n",
              "  ' deve',\n",
              "  'loped',\n",
              "  ',',\n",
              "  ' most',\n",
              "  ' time',\n",
              "  ' spent',\n",
              "  ' on',\n",
              "  ' data-mining',\n",
              "  'projects',\n",
              "  ' is',\n",
              "  ' for',\n",
              "  ' data',\n",
              "  ' preparation. We',\n",
              "  ' say',\n",
              "  ' ',\n",
              "  'that',\n",
              "  ' text',\n",
              "  ' mining',\n",
              "  ' is',\n",
              "  ' unstruc',\n",
              "  '-',\n",
              "  'tured',\n",
              "  ' because',\n",
              "  ' it',\n",
              "  ' is',\n",
              "  ' very',\n",
              "  ' far',\n",
              "  ' from',\n",
              "  ' ',\n",
              "  'the',\n",
              "  ' spreadsheet',\n",
              "  ' model',\n",
              "  ' that we',\n",
              "  ' need',\n",
              "  'to',\n",
              "  ' process',\n",
              "  ' data',\n",
              "  ' for',\n",
              "  ' prediction. Yet',\n",
              "  ',',\n",
              "  ' the',\n",
              "  ' transformation',\n",
              "  ' of',\n",
              "  ' data',\n",
              "  ' from',\n",
              "  ' text',\n",
              "  'to',\n",
              "  ' the',\n",
              "  ' spreadsheet',\n",
              "  ' model',\n",
              "  ' can',\n",
              "  ' be',\n",
              "  ' highly',\n",
              "  ' methodical',\n",
              "  ',',\n",
              "  ' and',\n",
              "  ' we',\n",
              "  ' have',\n",
              "  ' a care',\n",
              "  '-',\n",
              "  'fully',\n",
              "  ' organized',\n",
              "  ' procedure',\n",
              "  ' to ',\n",
              "  'ﬁll',\n",
              "  ' in',\n",
              "  ' the',\n",
              "  ' cells of',\n",
              "  ' the',\n",
              "  ' spread',\n",
              "  'sheet. ',\n",
              "  'First',\n",
              "  ',',\n",
              "  'of',\n",
              "  ' course',\n",
              "  ',',\n",
              "  ' we',\n",
              "  ' have',\n",
              "  ' to determine',\n",
              "  ' the',\n",
              "  ' nature',\n",
              "  ' of',\n",
              "  ' the',\n",
              "  ' columns',\n",
              "  ' ',\n",
              "  '(',\n",
              "  'i',\n",
              "  '.e',\n",
              "  '.',\n",
              "  ',',\n",
              "  ' the',\n",
              "  'features)',\n",
              "  ' ',\n",
              "  'of',\n",
              "  ' the',\n",
              "  ' spread',\n",
              "  'sheet. ',\n",
              "  'Some',\n",
              "  ' use',\n",
              "  'ful',\n",
              "  ' features',\n",
              "  ' are',\n",
              "  ' easy',\n",
              "  ' to',\n",
              "  ' obtain',\n",
              "  '(',\n",
              "  'e.g.',\n",
              "  ',',\n",
              "  ' a',\n",
              "  ' word',\n",
              "  ' as',\n",
              "  ' it occurs',\n",
              "  ' in',\n",
              "  ' text)',\n",
              "  ' and',\n",
              "  ' some',\n",
              "  ' are ',\n",
              "  'much more',\n",
              "  ' dif',\n",
              "  'ﬁcult',\n",
              "  '(',\n",
              "  'e.g.',\n",
              "  ',',\n",
              "  ' the',\n",
              "  ' grammatical function',\n",
              "  ' of',\n",
              "  ' a',\n",
              "  ' word',\n",
              "  ' in',\n",
              "  ' a',\n",
              "  ' sentence',\n",
              "  ' such',\n",
              "  ' as',\n",
              "  ' subject',\n",
              "  ',',\n",
              "  'ob',\n",
              "  'ject',\n",
              "  ',',\n",
              "  ' etc.). In this',\n",
              "  ' chapter',\n",
              "  ',',\n",
              "  ' we',\n",
              "  ' will',\n",
              "  ' dis',\n",
              "  'cuss',\n",
              "  ' how',\n",
              "  ' to',\n",
              "  ' obtain',\n",
              "  ' ',\n",
              "  'the',\n",
              "  ' kinds',\n",
              "  ' of',\n",
              "  'features',\n",
              "  ' commonly',\n",
              "  ' generated',\n",
              "  ' from',\n",
              "  ' text',\n",
              "  '.',\n",
              "  '2.1',\n",
              "  ' Collecting',\n",
              "  ' ',\n",
              "  'Documents',\n",
              "  'Clearly',\n",
              "  ',',\n",
              "  ' the',\n",
              "  ' ﬁrst step',\n",
              "  ' in',\n",
              "  ' text',\n",
              "  ' mining',\n",
              "  ' is',\n",
              "  ' to',\n",
              "  ' collect',\n",
              "  ' the',\n",
              "  ' data',\n",
              "  ' ',\n",
              "  '(',\n",
              "  'i.e.',\n",
              "  ',',\n",
              "  ' the',\n",
              "  'relevant documents). In',\n",
              "  ' many',\n",
              "  ' text-mining',\n",
              "  ' scenarios',\n",
              "  ',',\n",
              "  ' the',\n",
              "  ' relevant doc',\n",
              "  '-',\n",
              "  '\\x0c16',\n",
              "  '2. From',\n",
              "  ' ',\n",
              "  'Textual',\n",
              "  ' ',\n",
              "  'Information',\n",
              "  ' to',\n",
              "  ' ',\n",
              "  'Numerical',\n",
              "  ' ',\n",
              "  'Vectors',\n",
              "  'uments',\n",
              "  ' may',\n",
              "  ' already',\n",
              "  ' be',\n",
              "  ' given',\n",
              "  ' or ',\n",
              "  'they',\n",
              "  ' may',\n",
              "  ' be',\n",
              "  ' part',\n",
              "  ' of',\n",
              "  ' the',\n",
              "  ' problem',\n",
              "  'description. ',\n",
              "  'For',\n",
              "  ' example',\n",
              "  ',',\n",
              "  ' a',\n",
              "  ' ',\n",
              "  'Web',\n",
              "  ' page',\n",
              "  ' retrieval',\n",
              "  ' application',\n",
              "  ' for an',\n",
              "  ' in',\n",
              "  '-',\n",
              "  'tranet',\n",
              "  ' implicitly speci',\n",
              "  'ﬁes',\n",
              "  ' the',\n",
              "  ' rele',\n",
              "  'vant',\n",
              "  ' documents',\n",
              "  ' to',\n",
              "  ' be',\n",
              "  ' the',\n",
              "  ' ',\n",
              "  'Web',\n",
              "  ' pages',\n",
              "  'on',\n",
              "  ' the',\n",
              "  ' intranet. If',\n",
              "  ' the',\n",
              "  ' documents',\n",
              "  ' are',\n",
              "  ' readily',\n",
              "  ' identi',\n",
              "  'ﬁed',\n",
              "  ',',\n",
              "  ' then',\n",
              "  ' they',\n",
              "  ' can',\n",
              "  'be',\n",
              "  ' obtained',\n",
              "  ',',\n",
              "  ' and',\n",
              "  ' the',\n",
              "  ' main',\n",
              "  ' issue',\n",
              "  ' is',\n",
              "  ' to',\n",
              "  ' cleanse',\n",
              "  ' ',\n",
              "  'the',\n",
              "  ' samples',\n",
              "  ' and',\n",
              "  ' ensure',\n",
              "  'that',\n",
              "  ' they',\n",
              "  ' are',\n",
              "  ' ',\n",
              "  'of',\n",
              "  ' high',\n",
              "  ' quality. As',\n",
              "  ' with',\n",
              "  ' nontextual',\n",
              "  ' data',\n",
              "  ',',\n",
              "  ' human',\n",
              "  ' interven',\n",
              "  '-',\n",
              "  'tion',\n",
              "  ' can',\n",
              "  ' compromise',\n",
              "  ' the',\n",
              "  ' integrity',\n",
              "  ' of',\n",
              "  ' the',\n",
              "  ' document',\n",
              "  ' collection',\n",
              "  ' process',\n",
              "  ',',\n",
              "  'and',\n",
              "  ' hence',\n",
              "  ' extreme',\n",
              "  ' care',\n",
              "  ' must',\n",
              "  ' be',\n",
              "  ' exercised. Sometimes',\n",
              "  ',',\n",
              "  ' the',\n",
              "  ' documents',\n",
              "  'may',\n",
              "  ' be',\n",
              "  ' obtained',\n",
              "  ' from',\n",
              "  ' document',\n",
              "  ' ware',\n",
              "  'h',\n",
              "  'ouses',\n",
              "  ' or databases. In',\n",
              "  ' these',\n",
              "  'scenarios',\n",
              "  ',',\n",
              "  ' it',\n",
              "  ' is reasona',\n",
              "  'ble',\n",
              "  ' to',\n",
              "  ' expect',\n",
              "  ' ',\n",
              "  'that',\n",
              "  ' data',\n",
              "  ' cle',\n",
              "  'ansing',\n",
              "  ' was',\n",
              "  ' done',\n",
              "  ' before',\n",
              "  'deposit',\n",
              "  ' and',\n",
              "  ' we',\n",
              "  ' can',\n",
              "  ' be',\n",
              "  ' con',\n",
              "  'ﬁdent',\n",
              "  ' in',\n",
              "  ' ',\n",
              "  'the',\n",
              "  ' quality',\n",
              "  ' ',\n",
              "  'of',\n",
              "  ' the',\n",
              "  ' documents',\n",
              "  '.',\n",
              "  'In some',\n",
              "  ' applications',\n",
              "  ',',\n",
              "  ' one',\n",
              "  ' may',\n",
              "  ' need',\n",
              "  ' to',\n",
              "  ' have',\n",
              "  ' a',\n",
              "  ' data',\n",
              "  ' collection',\n",
              "  ' pro',\n",
              "  '-',\n",
              "  'cess. ',\n",
              "  'For',\n",
              "  ' instance',\n",
              "  ',',\n",
              "  ' for a ',\n",
              "  'Web',\n",
              "  ' application',\n",
              "  ' comprising',\n",
              "  ' a',\n",
              "  ' number',\n",
              "  ' of',\n",
              "  'autonom',\n",
              "  'ous',\n",
              "  ' ',\n",
              "  'Web',\n",
              "  ' sites',\n",
              "  ',',\n",
              "  ' one',\n",
              "  ' may',\n",
              "  ' deploy',\n",
              "  ' a',\n",
              "  ' software',\n",
              "  ' tool',\n",
              "  ' ',\n",
              "  'such',\n",
              "  ' as',\n",
              "  ' a',\n",
              "  ' ',\n",
              "  'Web',\n",
              "  'crawler',\n",
              "  ' that collects',\n",
              "  ' ',\n",
              "  'the',\n",
              "  ' documents. In',\n",
              "  ' other',\n",
              "  ' applications',\n",
              "  ',',\n",
              "  ' one',\n",
              "  ' may',\n",
              "  'have',\n",
              "  ' a',\n",
              "  ' logging',\n",
              "  ' process',\n",
              "  ' attached',\n",
              "  ' to',\n",
              "  ' an',\n",
              "  ' input data',\n",
              "  ' stream',\n",
              "  ' for',\n",
              "  ' a',\n",
              "  ' leng',\n",
              "  'th',\n",
              "  'of',\n",
              "  ' time. For',\n",
              "  ' example',\n",
              "  ',',\n",
              "  ' an',\n",
              "  ' e-mail',\n",
              "  ' audit',\n",
              "  ' application',\n",
              "  ' may',\n",
              "  ' log',\n",
              "  ' all',\n",
              "  ' incoming',\n",
              "  'and',\n",
              "  ' outgoing',\n",
              "  ' messages',\n",
              "  ' at',\n",
              "  ' a',\n",
              "  ' mail',\n",
              "  ' server',\n",
              "  ' for',\n",
              "  ' a',\n",
              "  ' period',\n",
              "  ' of',\n",
              "  ' time',\n",
              "  '.',\n",
              "  'Sometimes',\n",
              "  ' the',\n",
              "  ' set',\n",
              "  ' of',\n",
              "  ' documents',\n",
              "  ' can',\n",
              "  ' be',\n",
              "  ' extremely',\n",
              "  ' large',\n",
              "  ' and',\n",
              "  ' data',\n",
              "  '-',\n",
              "  'sam',\n",
              "  'pling',\n",
              "  ' techniques',\n",
              "  ' can',\n",
              "  ' be',\n",
              "  ' used',\n",
              "  ' to',\n",
              "  ' select',\n",
              "  ' a',\n",
              "  ' manage',\n",
              "  'able',\n",
              "  ' set',\n",
              "  ' of',\n",
              "  ' relevant',\n",
              "  'document',\n",
              "  's. These',\n",
              "  ' sam',\n",
              "  'pling',\n",
              "  ' techniques',\n",
              "  ' will depend',\n",
              "  ' on the',\n",
              "  ' application',\n",
              "  '.',\n",
              "  'For instance',\n",
              "  ',',\n",
              "  ' documents',\n",
              "  ' may',\n",
              "  ' have',\n",
              "  ' a',\n",
              "  ' time',\n",
              "  ' stamp',\n",
              "  ',',\n",
              "  ' and',\n",
              "  ' more',\n",
              "  ' recent',\n",
              "  ' doc',\n",
              "  '-',\n",
              "  'uments',\n",
              "  ' may have',\n",
              "  ' a',\n",
              "  ' higher',\n",
              "  ' relevance. ',\n",
              "  'Depending',\n",
              "  ' on',\n",
              "  ' our',\n",
              "  ' resources',\n",
              "  ',',\n",
              "  ' w',\n",
              "  'e',\n",
              "  'may',\n",
              "  ' limit',\n",
              "  ' our',\n",
              "  ' sam',\n",
              "  'ple',\n",
              "  ' to',\n",
              "  ' documents',\n",
              "  ' ',\n",
              "  'that',\n",
              "  ' are',\n",
              "  ' ',\n",
              "  'more',\n",
              "  ' use',\n",
              "  'ful',\n",
              "  '.',\n",
              "  'For',\n",
              "  ' research and',\n",
              "  ' development',\n",
              "  ' of',\n",
              "  ' text-mining',\n",
              "  ' techniques',\n",
              "  ',',\n",
              "  ' more',\n",
              "  'generic',\n",
              "  ' data',\n",
              "  ' may',\n",
              "  ' be',\n",
              "  ' necessary. This',\n",
              "  ' is',\n",
              "  ' usually',\n",
              "  ' called',\n",
              "  ' a',\n",
              "  ' corpus. For',\n",
              "  'the',\n",
              "  ' accompanying',\n",
              "  ' sof',\n",
              "  'tware',\n",
              "  ',',\n",
              "  ' we',\n",
              "  ' mainly',\n",
              "  ' used',\n",
              "  ' the',\n",
              "  ' collection',\n",
              "  ' ',\n",
              "  'of',\n",
              "  ' ',\n",
              "  'Reuters',\n",
              "  'news stories',\n",
              "  ',',\n",
              "  ' referred',\n",
              "  ' to',\n",
              "  ' as',\n",
              "  ' ',\n",
              "  'Reuters',\n",
              "  ' corpus',\n",
              "  ' RCV1',\n",
              "  ',',\n",
              "  ' obtainable',\n",
              "  ' from',\n",
              "  ' the',\n",
              "  'Reuters',\n",
              "  ' ',\n",
              "  'Corporation',\n",
              "  ' ',\n",
              "  'Web site. ',\n",
              "  'However',\n",
              "  ',',\n",
              "  ' there',\n",
              "  ' are',\n",
              "  ' many',\n",
              "  ' other',\n",
              "  ' corpora',\n",
              "  'available',\n",
              "  ' ',\n",
              "  'that',\n",
              "  ' may',\n",
              "  ' be',\n",
              "  ' more',\n",
              "  ' appropriate',\n",
              "  ' for some',\n",
              "  ' studies',\n",
              "  '.',\n",
              "  'In',\n",
              "  ' the',\n",
              "  ' early',\n",
              "  ' days',\n",
              "  ' of',\n",
              "  ' text',\n",
              "  ' processing',\n",
              "  ' ',\n",
              "  '(',\n",
              "  '1950s',\n",
              "  ' and',\n",
              "  ' 1960s)',\n",
              "  ',',\n",
              "  ' one',\n",
              "  ' mill',\n",
              "  'ion',\n",
              "  'words',\n",
              "  ' was',\n",
              "  ' considered',\n",
              "  ' a',\n",
              "  ' very',\n",
              "  ' large',\n",
              "  ' collection. ',\n",
              "  'This',\n",
              "  ' was',\n",
              "  ' the',\n",
              "  ' size',\n",
              "  ' of',\n",
              "  ' one',\n",
              "  'of',\n",
              "  ' the',\n",
              "  ' ﬁrst widely',\n",
              "  ' available',\n",
              "  ' collections',\n",
              "  ',',\n",
              "  ' the',\n",
              "  ' ',\n",
              "  'Brown',\n",
              "  ' corpus',\n",
              "  ',',\n",
              "  ' consisting',\n",
              "  'of',\n",
              "  ' 500',\n",
              "  ' samples',\n",
              "  ' of',\n",
              "  ' about',\n",
              "  ' 2000',\n",
              "  ' words',\n",
              "  ' each',\n",
              "  ' ',\n",
              "  'of',\n",
              "  ' American',\n",
              "  ' ',\n",
              "  'English',\n",
              "  ' texts',\n",
              "  ' of',\n",
              "  'varying',\n",
              "  ' genres. A ',\n",
              "  'European',\n",
              "  ' corpus',\n",
              "  ',',\n",
              "  ' the',\n",
              "  ' Lancaster-Oslo-Bergen',\n",
              "  ' corpus',\n",
              "  '(',\n",
              "  'LOB)',\n",
              "  ',',\n",
              "  ' was',\n",
              "  ' modeled',\n",
              "  ' on the',\n",
              "  ' ',\n",
              "  'Brown',\n",
              "  ' corpus',\n",
              "  ' but',\n",
              "  ' was',\n",
              "  ' ',\n",
              "  'for British ',\n",
              "  'English',\n",
              "  '.',\n",
              "  'Both',\n",
              "  ' these',\n",
              "  ' are',\n",
              "  ' still',\n",
              "  ' available',\n",
              "  ' and',\n",
              "  ' still',\n",
              "  ' used. In',\n",
              "  ' the',\n",
              "  ' 1970s',\n",
              "  ' and',\n",
              "  ' 1980s',\n",
              "  ',',\n",
              "  'many',\n",
              "  ' more',\n",
              "  ' resources',\n",
              "  ' became',\n",
              "  ' available',\n",
              "  ',',\n",
              "  ' some',\n",
              "  ' from academic',\n",
              "  ' initiatives',\n",
              "  'and',\n",
              "  ' others',\n",
              "  ' as a result',\n",
              "  ' of',\n",
              "  ' government-s',\n",
              "  'ponsored',\n",
              "  ' research. Some',\n",
              "  ' widely',\n",
              "  'used',\n",
              "  ' corpora',\n",
              "  ' are',\n",
              "  ' the',\n",
              "  ' ',\n",
              "  'Penn Tree',\n",
              "  ' ',\n",
              "  'Bank',\n",
              "  ',',\n",
              "  ' a',\n",
              "  ' collection of',\n",
              "  ' manually',\n",
              "  ' parsed',\n",
              "  '\\x0c2.1',\n",
              "  ' ',\n",
              "  'Collecting',\n",
              "  ' ',\n",
              "  'Documents',\n",
              "  '17',\n",
              "  'sentences',\n",
              "  ' from',\n",
              "  ' the',\n",
              "  ' ',\n",
              "  'Wall',\n",
              "  ' ',\n",
              "  'Street',\n",
              "  ' ',\n",
              "  'Journal',\n",
              "  ';',\n",
              "  ' the',\n",
              "  ' TREC ',\n",
              "  '(',\n",
              "  'Text',\n",
              "  ' ',\n",
              "  'Retrieval',\n",
              "  'and',\n",
              "  ' ',\n",
              "  'Evaluation',\n",
              "  ' ',\n",
              "  'Conferences)',\n",
              "  ' collec',\n",
              "  'tions',\n",
              "  ',',\n",
              "  ' consisting',\n",
              "  ' of',\n",
              "  ' selections',\n",
              "  ' from',\n",
              "  'the',\n",
              "  ' Wall',\n",
              "  ' Street',\n",
              "  ' ',\n",
              "  'Journal',\n",
              "  ',',\n",
              "  ' the',\n",
              "  ' ',\n",
              "  'New York ',\n",
              "  'Times',\n",
              "  ',',\n",
              "  ' ',\n",
              "  'Ziff-Davis',\n",
              "  ' Publications',\n",
              "  ',',\n",
              "  'the',\n",
              "  ' ',\n",
              "  'Federal',\n",
              "  ' ',\n",
              "  'Register',\n",
              "  ',',\n",
              "  ' and',\n",
              "  ' others',\n",
              "  ';',\n",
              "  ' the',\n",
              "  ' proceedings',\n",
              "  ' of',\n",
              "  ' the',\n",
              "  ' ',\n",
              "  'Canadian',\n",
              "  'Parliament',\n",
              "  ' in',\n",
              "  ' parallel',\n",
              "  ' ',\n",
              "  'English',\n",
              "  '–',\n",
              "  'French',\n",
              "  ' ',\n",
              "  'translations',\n",
              "  ',',\n",
              "  ' widely',\n",
              "  ' used',\n",
              "  ' in',\n",
              "  'statistical',\n",
              "  ' machine',\n",
              "  ' translation',\n",
              "  ' research',\n",
              "  ';',\n",
              "  ' and',\n",
              "  ' the',\n",
              "  ' ',\n",
              "  'Gutenberg',\n",
              "  ' ',\n",
              "  'Project',\n",
              "  ',',\n",
              "  'a',\n",
              "  ' very',\n",
              "  ' large',\n",
              "  ' collection',\n",
              "  ' of',\n",
              "  ' literary',\n",
              "  ' and',\n",
              "  ' other',\n",
              "  ' texts',\n",
              "  ' put',\n",
              "  ' into machine',\n",
              "  '-',\n",
              "  'readable',\n",
              "  ' form',\n",
              "  ' as',\n",
              "  ' the',\n",
              "  ' material',\n",
              "  ' comes',\n",
              "  ' out',\n",
              "  ' ',\n",
              "  'of',\n",
              "  ' copyright. A',\n",
              "  ' collec',\n",
              "  'tion',\n",
              "  ' of',\n",
              "  'Reuters',\n",
              "  ' news',\n",
              "  ' stories',\n",
              "  ' called',\n",
              "  ' ',\n",
              "  'Reuters-21578',\n",
              "  ' ',\n",
              "  'Distribution',\n",
              "  ' ',\n",
              "  '1.0',\n",
              "  ' has',\n",
              "  ' been',\n",
              "  'widely',\n",
              "  ' used',\n",
              "  ' in',\n",
              "  ' studying',\n",
              "  ' methods',\n",
              "  ' for text',\n",
              "  ' categorization',\n",
              "  '.',\n",
              "  'As',\n",
              "  ' the',\n",
              "  ' importan',\n",
              "  'ce',\n",
              "  ' of large',\n",
              "  ' text',\n",
              "  ' corpora',\n",
              "  ' became',\n",
              "  ' evident',\n",
              "  ',',\n",
              "  ' a number',\n",
              "  'of',\n",
              "  ' organizations',\n",
              "  ' and',\n",
              "  ' initiatives',\n",
              "  ' arose',\n",
              "  ' to',\n",
              "  ' coordinate',\n",
              "  ' activity and',\n",
              "  ' pro',\n",
              "  '-',\n",
              "  'vide',\n",
              "  ' a',\n",
              "  ' distribution',\n",
              "  ' mechanism',\n",
              "  ' for',\n",
              "  ' corpora. Two',\n",
              "  ' of',\n",
              "  ' the',\n",
              "  ' main',\n",
              "  ' ones',\n",
              "  ' are',\n",
              "  'the',\n",
              "  ' ',\n",
              "  'Linguistic',\n",
              "  ' Data',\n",
              "  ' ',\n",
              "  'Consortium',\n",
              "  ' ',\n",
              "  '(',\n",
              "  'LDC)',\n",
              "  ' housed',\n",
              "  ' at the',\n",
              "  ' ',\n",
              "  'University',\n",
              "  ' of',\n",
              "  'Pennsylvania',\n",
              "  ' and',\n",
              "  ' the',\n",
              "  ' ',\n",
              "  'International',\n",
              "  ' ',\n",
              "  'Computer',\n",
              "  ' ',\n",
              "  'Archive',\n",
              "  ' of ',\n",
              "  'Modern',\n",
              "  ' and',\n",
              "  'Medieval',\n",
              "  ' English',\n",
              "  ' ',\n",
              "  '(',\n",
              "  'ICAME)',\n",
              "  ',',\n",
              "  ' which',\n",
              "  ' resides',\n",
              "  ' in ',\n",
              "  'Bergen',\n",
              "  ',',\n",
              "  ' Norway. Many',\n",
              "  'other',\n",
              "  ' centers',\n",
              "  ' of',\n",
              "  ' varying',\n",
              "  ' size',\n",
              "  ' exist',\n",
              "  ' in',\n",
              "  ' academic',\n",
              "  ' institution',\n",
              "  's. ',\n",
              "  'The',\n",
              "  ...],\n",
              " 'IR Basic': ['IR Basics',\n",
              "  'Fu-ren',\n",
              "  ' Lin',\n",
              "  'Institute',\n",
              "  ' ',\n",
              "  'of',\n",
              "  ' ',\n",
              "  'Service',\n",
              "  ' ',\n",
              "  'Science',\n",
              "  'National',\n",
              "  ' Tsing',\n",
              "  ' ',\n",
              "  'Hua',\n",
              "  ' ',\n",
              "  'University',\n",
              "  '\\x0cOutline',\n",
              "  'n ',\n",
              "  'Problem',\n",
              "  ' formulation',\n",
              "  'n ',\n",
              "  'Retrieval',\n",
              "  ' models',\n",
              "  'n ',\n",
              "  'Evaluat',\n",
              "  'ion',\n",
              "  'n',\n",
              "  ' ',\n",
              "  'Common',\n",
              "  ' IR ',\n",
              "  'components',\n",
              "  '2019/3/11',\n",
              "  '2',\n",
              "  '\\x0cP',\n",
              "  'roblem',\n",
              "  ' Formulation',\n",
              "  ':',\n",
              "  ' Infor',\n",
              "  'mal',\n",
              "  'n',\n",
              "  ' ',\n",
              "  'User',\n",
              "  ' has',\n",
              "  ' an',\n",
              "  ' information',\n",
              "  ' need',\n",
              "  'q ',\n",
              "  'Short-term',\n",
              "  ' ',\n",
              "  '(',\n",
              "  '“ad',\n",
              "  ' hoc”)',\n",
              "  ',',\n",
              "  ' e.g.',\n",
              "  ',',\n",
              "  ' ',\n",
              "  '“price',\n",
              "  ' of',\n",
              "  ' iPhone',\n",
              "  '(',\n",
              "  'cid',\n",
              "  ':',\n",
              "  '1',\n",
              "  ')',\n",
              "  'q ',\n",
              "  'Long-term',\n",
              "  ',',\n",
              "  ' e.g.',\n",
              "  ',',\n",
              "  ' “technology',\n",
              "  ' roadmap',\n",
              "  '(',\n",
              "  'cid',\n",
              "  ':',\n",
              "  '1',\n",
              "  ')',\n",
              "  'n ',\n",
              "  'There',\n",
              "  ' exists an',\n",
              "  ' information',\n",
              "  ' source',\n",
              "  'q ',\n",
              "  'Relative',\n",
              "  'ly',\n",
              "  ' static',\n",
              "  ',',\n",
              "  ' e.g.',\n",
              "  ',',\n",
              "  ' library',\n",
              "  ' system',\n",
              "  'q ',\n",
              "  'Inherent',\n",
              "  'ly dynamic',\n",
              "  ',',\n",
              "  ' e.g.',\n",
              "  ',',\n",
              "  ' news',\n",
              "  ' articles',\n",
              "  'n',\n",
              "  ' ',\n",
              "  'Goal',\n",
              "  ' is',\n",
              "  ' to',\n",
              "  ' find',\n",
              "  ' information',\n",
              "  ' items',\n",
              "  ' ',\n",
              "  'that',\n",
              "  ' can',\n",
              "  ' ',\n",
              "  'satis',\n",
              "  'fy',\n",
              "  ' a',\n",
              "  ' user',\n",
              "  '’s',\n",
              "  ' information',\n",
              "  ' need',\n",
              "  '2019/3/11',\n",
              "  '3',\n",
              "  '\\x0cUser task',\n",
              "  'n ',\n",
              "  'Retrie',\n",
              "  'val',\n",
              "  'q A',\n",
              "  ' query',\n",
              "  ' expression',\n",
              "  ' is',\n",
              "  ' used',\n",
              "  ' to',\n",
              "  ' convey',\n",
              "  ' the',\n",
              "  ' ',\n",
              "  'constraints',\n",
              "  ' that',\n",
              "  ' must',\n",
              "  ' be',\n",
              "  ' satisfied',\n",
              "  ' by',\n",
              "  ' objects',\n",
              "  ' in',\n",
              "  ' the',\n",
              "  ' ',\n",
              "  'answer set',\n",
              "  '.',\n",
              "  'q purpose',\n",
              "  'ful',\n",
              "  'n Browsing',\n",
              "  'q ',\n",
              "  'Glancing',\n",
              "  ' around',\n",
              "  'q',\n",
              "  ' ',\n",
              "  'Hypertext',\n",
              "  ' systems',\n",
              "  ' are',\n",
              "  ' usually',\n",
              "  ' tuned',\n",
              "  ' for providing',\n",
              "  ' ',\n",
              "  'quick',\n",
              "  ' browsing',\n",
              "  '.',\n",
              "  '2019/3/11',\n",
              "  '4',\n",
              "  '\\x0c',\n",
              "  'Logical',\n",
              "  ' view',\n",
              "  ' of',\n",
              "  '  ',\n",
              "  'the',\n",
              "  ' documents',\n",
              "  'Document',\n",
              "  's',\n",
              "  'Accents',\n",
              "  'spacing',\n",
              "  'Stopwords',\n",
              "  'Noun',\n",
              "  'groups',\n",
              "  'stemming',\n",
              "  'Automatic',\n",
              "  'or',\n",
              "  ' manual',\n",
              "  ' ',\n",
              "  'indexing',\n",
              "  'Structure',\n",
              "  'recognition',\n",
              "  'structure',\n",
              "  'Full',\n",
              "  '  text',\n",
              "  'Index',\n",
              "  ' terms',\n",
              "  '2019/3/11',\n",
              "  '5',\n",
              "  '\\x0cText',\n",
              "  'The',\n",
              "  ' ',\n",
              "  'Retrieval',\n",
              "  ' Process',\n",
              "  'User',\n",
              "  'Inter',\n",
              "  'face',\n",
              "  '4',\n",
              "  ',',\n",
              "  ' 10',\n",
              "  'user',\n",
              "  ' need',\n",
              "  'Text   ',\n",
              "  'Operations',\n",
              "  'logical',\n",
              "  ' vie',\n",
              "  'w',\n",
              "  'logical',\n",
              "  ' vie',\n",
              "  'w',\n",
              "  'Indexing',\n",
              "  'inverted',\n",
              "  ' file',\n",
              "  'Index',\n",
              "  'user',\n",
              "  ' feedback',\n",
              "  'Query',\n",
              "  ' ',\n",
              "  'Operations',\n",
              "  'query',\n",
              "  'Searching',\n",
              "  '5',\n",
              "  '8',\n",
              "  'retrieved',\n",
              "  ' docs',\n",
              "  'ranked',\n",
              "  ' docs',\n",
              "  ' ',\n",
              "  'Ranking',\n",
              "  '2',\n",
              "  '2019/3/11',\n",
              "  'Text',\n",
              "  '6',\n",
              "  ',',\n",
              "  ' 7',\n",
              "  '8',\n",
              "  'DB',\n",
              "  ' manager',\n",
              "  ' ',\n",
              "  'module',\n",
              "  'Text',\n",
              "  ' ',\n",
              "  'database',\n",
              "  '6',\n",
              "  '\\x0cAd',\n",
              "  ' hoc',\n",
              "  ' ',\n",
              "  'Retrieval',\n",
              "  ' vs',\n",
              "  '.',\n",
              "  ' ',\n",
              "  'Information',\n",
              "  ' ',\n",
              "  'Filtering',\n",
              "  'n Ad hoc',\n",
              "  ' retrieval',\n",
              "  'q ',\n",
              "  'Short-term',\n",
              "  ' information',\n",
              "  ' need',\n",
              "  ' + static',\n",
              "  ' source',\n",
              "  ' ',\n",
              "  'q ',\n",
              "  'User',\n",
              "  ' “pulls',\n",
              "  '”',\n",
              "  ' information',\n",
              "  ',',\n",
              "  ' e.g.',\n",
              "  ',',\n",
              "  ' web search',\n",
              "  'n',\n",
              "  ' ',\n",
              "  'Information',\n",
              "  ' filtering',\n",
              "  '(',\n",
              "  'routing)',\n",
              "  ' ',\n",
              "  'q ',\n",
              "  'Persistent',\n",
              "  ' information',\n",
              "  ' need',\n",
              "  ' + dynamic',\n",
              "  ' source',\n",
              "  'q ',\n",
              "  'System',\n",
              "  ' “pushes',\n",
              "  '”',\n",
              "  ' information',\n",
              "  ' to',\n",
              "  ' user',\n",
              "  ',',\n",
              "  ' e.g.',\n",
              "  ',',\n",
              "  ' news',\n",
              "  ' ',\n",
              "  'filter',\n",
              "  'q',\n",
              "  ' ',\n",
              "  'Traditional',\n",
              "  'ly',\n",
              "  ' callled',\n",
              "  ' “',\n",
              "  'Selective',\n",
              "  ' Dissemination',\n",
              "  ' of',\n",
              "  ' ',\n",
              "  'Information',\n",
              "  '”',\n",
              "  ',',\n",
              "  ' or',\n",
              "  ' SDI',\n",
              "  '2019/3/11',\n",
              "  '7',\n",
              "  '\\x0cAd hoc',\n",
              "  ' retrieval',\n",
              "  'Q1',\n",
              "  'Q2',\n",
              "  'Q3',\n",
              "  'Collection',\n",
              "  '“Fixed',\n",
              "  ' Size',\n",
              "  '”',\n",
              "  'Q4',\n",
              "  'Q5',\n",
              "  '2019/3/11',\n",
              "  '8',\n",
              "  '\\x0cFiltering',\n",
              "  'User  2',\n",
              "  'Profile',\n",
              "  'User',\n",
              "  '  1',\n",
              "  'Profile',\n",
              "  'Docs Filtered',\n",
              "  'for',\n",
              "  ' User',\n",
              "  ' 2',\n",
              "  'Docs',\n",
              "  ' for',\n",
              "  'User',\n",
              "  ' 1',\n",
              "  '2019/3/11',\n",
              "  '9',\n",
              "  'Documents',\n",
              "  '  Stream',\n",
              "  '\\x0cImportance',\n",
              "  ' ',\n",
              "  'of',\n",
              "  '  ',\n",
              "  'Ad',\n",
              "  ' hoc',\n",
              "  ' ',\n",
              "  'Retrieval',\n",
              "  'n',\n",
              "  ' ',\n",
              "  'Directly',\n",
              "  ' manages',\n",
              "  ' any',\n",
              "  ' existing',\n",
              "  ' large',\n",
              "  ' collection',\n",
              "  ' ',\n",
              "  'of',\n",
              "  ' information',\n",
              "  'n',\n",
              "  ' ',\n",
              "  'There',\n",
              "  ' are',\n",
              "  ' many',\n",
              "  ' many',\n",
              "  ' ',\n",
              "  '“ad',\n",
              "  ' hoc',\n",
              "  '”',\n",
              "  ' information',\n",
              "  ' ',\n",
              "  'needs.',\n",
              "  ' ',\n",
              "  'n',\n",
              "  ' A',\n",
              "  ' long-term',\n",
              "  ' information',\n",
              "  ' need',\n",
              "  ' can',\n",
              "  ' be',\n",
              "  ' satisfied',\n",
              "  ' ',\n",
              "  'through',\n",
              "  ' frequent',\n",
              "  ' ad',\n",
              "  ' hoc',\n",
              "  ' retrieval',\n",
              "  'n',\n",
              "  ' ',\n",
              "  'Basic',\n",
              "  ' techniques',\n",
              "  ' of',\n",
              "  ' ad',\n",
              "  ' hoc',\n",
              "  ' retrieval',\n",
              "  ' can',\n",
              "  ' be',\n",
              "  ' ',\n",
              "  'used',\n",
              "  ' for filtering',\n",
              "  ' and',\n",
              "  ' other “non-retrieval',\n",
              "  '”',\n",
              "  'tasks',\n",
              "  ',',\n",
              "  ' such',\n",
              "  ' as',\n",
              "  ' automatic',\n",
              "  ' summarization',\n",
              "  '.',\n",
              "  ' ',\n",
              "  '2019/3/11',\n",
              "  '10',\n",
              "  '\\x0c',\n",
              "  'The',\n",
              "  ' ',\n",
              "  'Ad',\n",
              "  ' hoc',\n",
              "  ' ',\n",
              "  'Retrieval',\n",
              "  ' ',\n",
              "  'Problem',\n",
              "  'n ',\n",
              "  'Query',\n",
              "  ':',\n",
              "  ' Description of',\n",
              "  ' information',\n",
              "  ' need',\n",
              "  ',',\n",
              "  ' e.g.',\n",
              "  ',',\n",
              "  'q ',\n",
              "  'Boole',\n",
              "  'an',\n",
              "  ':',\n",
              "  ' “iPhone',\n",
              "  '(',\n",
              "  'cid',\n",
              "  ':',\n",
              "  '1)',\n",
              "  ' AND ',\n",
              "  '(',\n",
              "  '“cheap',\n",
              "  '” OR “sale”)',\n",
              "  'q ',\n",
              "  'English',\n",
              "  ':',\n",
              "  ' “cheap',\n",
              "  ' iPhone',\n",
              "  ' on',\n",
              "  ' sale',\n",
              "  '”',\n",
              "  'n ',\n",
              "  'Document',\n",
              "  ':',\n",
              "  ' ',\n",
              "  'Information',\n",
              "  ' item',\n",
              "  ',',\n",
              "  ' e.g.',\n",
              "  ',',\n",
              "  ' ',\n",
              "  'q ',\n",
              "  'Textual',\n",
              "  ' ',\n",
              "  '(',\n",
              "  'possibly',\n",
              "  ' with',\n",
              "  ' structural',\n",
              "  ' information',\n",
              "  ')',\n",
              "  'q ',\n",
              "  'Multi-media',\n",
              "  'n Database/Collection/Corpus',\n",
              "  ':',\n",
              "  ' ',\n",
              "  'q a set',\n",
              "  ' of',\n",
              "  ' documents',\n",
              "  'n ',\n",
              "  'Retrie',\n",
              "  'val',\n",
              "  ' task',\n",
              "  ':',\n",
              "  ' ',\n",
              "  'q',\n",
              "  ' find',\n",
              "  ' documents',\n",
              "  ' relevant',\n",
              "  ' to',\n",
              "  ' a',\n",
              "  ' query',\n",
              "  ' in',\n",
              "  ' one',\n",
              "  ' or',\n",
              "  ' more',\n",
              "  ' collections',\n",
              "  '2019/3/11',\n",
              "  '11',\n",
              "  '\\x0c',\n",
              "  'Information retrieval',\n",
              "  'Index',\n",
              "  ' terms',\n",
              "  'doc',\n",
              "  'Information',\n",
              "  ' ',\n",
              "  'Need',\n",
              "  'Match',\n",
              "  'Ranking',\n",
              "  'query',\n",
              "  '2019/3/11',\n",
              "  '12',\n",
              "  '\\x0c',\n",
              "  'Formal',\n",
              "  ' ',\n",
              "  'Formulation',\n",
              "  ' of',\n",
              "  '  IR',\n",
              "  'n',\n",
              "  ' ',\n",
              "  'Vocabulary',\n",
              "  ' V',\n",
              "  '={w1',\n",
              "  ',',\n",
              "  ' w',\n",
              "  '2',\n",
              "  ',',\n",
              "  ' …',\n",
              "  ',',\n",
              "  ' wN} ',\n",
              "  'of',\n",
              "  ' language',\n",
              "  'n ',\n",
              "  'Query',\n",
              "  ' q = q1',\n",
              "  ',',\n",
              "  '…',\n",
              "  ',',\n",
              "  'qm',\n",
              "  ',',\n",
              "  ' where',\n",
              "  ' qi',\n",
              "  ' Î ',\n",
              "  'V',\n",
              "  'n',\n",
              "  ' ',\n",
              "  'Document',\n",
              "  ' di = di1',\n",
              "  ',',\n",
              "  '…',\n",
              "  ',',\n",
              "  'dimi',\n",
              "  ',',\n",
              "  ' where',\n",
              "  ' dij',\n",
              "  ' Î ',\n",
              "  'V',\n",
              "  'n',\n",
              "  ' Collection',\n",
              "  ' C',\n",
              "  '= {d1',\n",
              "  ',',\n",
              "  ' …',\n",
              "  ',',\n",
              "  ' dk}',\n",
              "  'n',\n",
              "  ' ',\n",
              "  'Set',\n",
              "  ' ',\n",
              "  'of',\n",
              "  ' relevant',\n",
              "  ' documents',\n",
              "  ' R',\n",
              "  '(',\n",
              "  'q) Í C',\n",
              "  'q ',\n",
              "  'Generally',\n",
              "  ' unknown',\n",
              "  ' and',\n",
              "  ' user-dependent',\n",
              "  'q ',\n",
              "  'Query',\n",
              "  ' is',\n",
              "  ' a “hint”',\n",
              "  ' on',\n",
              "  ' which document',\n",
              "  ' is',\n",
              "  ' in R',\n",
              "  '(',\n",
              "  'q',\n",
              "  ')',\n",
              "  'n',\n",
              "  ' Task',\n",
              "  ' =  com',\n",
              "  'pute',\n",
              "  ' R’',\n",
              "  '(',\n",
              "  'q)',\n",
              "  ',',\n",
              "  ' an',\n",
              "  ' approximate',\n",
              "  ' R',\n",
              "  '(',\n",
              "  'q',\n",
              "  ')',\n",
              "  '2019/3/11',\n",
              "  '13',\n",
              "  '\\x0cComputing',\n",
              "  ' R',\n",
              "  '(',\n",
              "  'q)',\n",
              "  'n Strategy',\n",
              "  ' 1',\n",
              "  ':',\n",
              "  ' ',\n",
              "  'Document',\n",
              "  ' selection',\n",
              "  'q R',\n",
              "  '(',\n",
              "  'q)',\n",
              "  '={dÎC|f',\n",
              "  '(',\n",
              "  'd',\n",
              "  ',',\n",
              "  'q)',\n",
              "  '=',\n",
              "  '1}',\n",
              "  ',',\n",
              "  ' where',\n",
              "  ' f',\n",
              "  '(',\n",
              "  'd',\n",
              "  ',',\n",
              "  'q)',\n",
              "  ' ',\n",
              "  'Î{0',\n",
              "  ',',\n",
              "  '1} is',\n",
              "  ' an',\n",
              "  ' ',\n",
              "  'indicator',\n",
              "  ' function',\n",
              "  ' or classifier',\n",
              "  'q ',\n",
              "  'System',\n",
              "  ' must decide',\n",
              "  ' if',\n",
              "  ' a',\n",
              "  ' doc',\n",
              "  ' is',\n",
              "  ' relevant',\n",
              "  ' or not',\n",
              "  ' ',\n",
              "  '(',\n",
              "  '“absolute',\n",
              "  ' relevance',\n",
              "  '”)',\n",
              "  'n Strategy',\n",
              "  ' 2',\n",
              "  ':',\n",
              "  ' ',\n",
              "  'Document',\n",
              "  ' ranking',\n",
              "  'q R',\n",
              "  '(',\n",
              "  'q)',\n",
              "  ' = {dÎC|f',\n",
              "  '(',\n",
              "  'd',\n",
              "  ',',\n",
              "  'q)',\n",
              "  '>',\n",
              "  'q}',\n",
              "  ',',\n",
              "  ' where',\n",
              "  ' f',\n",
              "  '(',\n",
              "  'd',\n",
              "  ',',\n",
              "  'q)',\n",
              "  ' ÎÂ is',\n",
              "  ' a ',\n",
              "  'relevance',\n",
              "  ' measure',\n",
              "  ' function',\n",
              "  ';',\n",
              "  ' q is',\n",
              "  ' a',\n",
              "  ' cut',\n",
              "  'of',\n",
              "  'f',\n",
              "  'q',\n",
              "  ' ',\n",
              "  'System',\n",
              "  ' must decide',\n",
              "  ' if',\n",
              "  ' one',\n",
              "  ' doc',\n",
              "  ' is',\n",
              "  ' more',\n",
              "  ' likely',\n",
              "  ' to',\n",
              "  ' be',\n",
              "  ' ',\n",
              "  'relevant',\n",
              "  ' than',\n",
              "  ' another',\n",
              "  ' ',\n",
              "  '(',\n",
              "  '“relative',\n",
              "  ' relevance',\n",
              "  '”',\n",
              "  ')',\n",
              "  '2019/3/11',\n",
              "  '14',\n",
              "  '\\x0c',\n",
              "  'Document',\n",
              "  ' ',\n",
              "  'Selection',\n",
              "  ' vs. Ranking',\n",
              "  'True',\n",
              "  ' R',\n",
              "  '(',\n",
              "  'q)',\n",
              "  '-',\n",
              "  '-',\n",
              "  '+ ',\n",
              "  '+',\n",
              "  '-',\n",
              "  '+',\n",
              "  '-',\n",
              "  '-',\n",
              "  '-',\n",
              "  '+',\n",
              "  '+',\n",
              "  '-',\n",
              "  '-',\n",
              "  '-',\n",
              "  '-',\n",
              "  '-',\n",
              "  '-',\n",
              "  '-',\n",
              "  '-',\n",
              "  '- -',\n",
              "  'Doc ',\n",
              "  'Selection',\n",
              "  'f',\n",
              "  '(',\n",
              "  'd',\n",
              "  ',',\n",
              "  'q)',\n",
              "  '=',\n",
              "  '?',\n",
              "  '1',\n",
              "  '0',\n",
              "  'Doc',\n",
              "  ' Ranking',\n",
              "  'f',\n",
              "  '(',\n",
              "  'd',\n",
              "  ',',\n",
              "  'q)',\n",
              "  '=',\n",
              "  '?',\n",
              "  '2019/3/11',\n",
              "  '-',\n",
              "  '-',\n",
              "  '+',\n",
              "  '+',\n",
              "  '+',\n",
              "  '+',\n",
              "  '+',\n",
              "  'R’',\n",
              "  '(',\n",
              "  'q)',\n",
              "  '-',\n",
              "  '-',\n",
              "  '-',\n",
              "  '-',\n",
              "  '+',\n",
              "  '-',\n",
              "  '-',\n",
              "  '-',\n",
              "  '-',\n",
              "  '-',\n",
              "  '0.98',\n",
              "  ' d1 ',\n",
              "  '+',\n",
              "  '0.95',\n",
              "  ' d2 ',\n",
              "  '+',\n",
              "  '0.83',\n",
              "  ' d3 -',\n",
              "  '0.80',\n",
              "  ' d4 ',\n",
              "  '+',\n",
              "  '0.76',\n",
              "  ' d5 ',\n",
              "  '-',\n",
              "  '0.56',\n",
              "  ' d6 ',\n",
              "  '-',\n",
              "  '0.34',\n",
              "  ' d7 -',\n",
              "  '0.21',\n",
              "  ' d8',\n",
              "  ' ',\n",
              "  '+',\n",
              "  '0.21',\n",
              "  ' d9 -',\n",
              "  'R’',\n",
              "  '(',\n",
              "  'q)',\n",
              "  '15',\n",
              "  '\\x0c',\n",
              "  'Problems',\n",
              "  ' ',\n",
              "  'of',\n",
              "  '  ',\n",
              "  'Doc',\n",
              "  ' ',\n",
              "  'Selection',\n",
              "  'n ',\n",
              "  'The',\n",
              "  ' classifier',\n",
              "  ' is',\n",
              "  ' unlikely',\n",
              "  ' accurate',\n",
              "  'q “',\n",
              "  'Over-constrained',\n",
              "  '” query',\n",
              "  ' ',\n",
              "  '(',\n",
              "  'terms',\n",
              "  ' are',\n",
              "  ' too',\n",
              "  ' specific)',\n",
              "  ':',\n",
              "  ' ',\n",
              "  'no',\n",
              "  ' relevant documents',\n",
              "  ' found',\n",
              "  'q “',\n",
              "  'Under-constrained',\n",
              "  '” query',\n",
              "  ' ',\n",
              "  '(',\n",
              "  'terms',\n",
              "  ' are',\n",
              "  ' too',\n",
              "  ' general)',\n",
              "  ':',\n",
              "  ' ',\n",
              "  'over',\n",
              "  ' delivery',\n",
              "  'q',\n",
              "  ' ',\n",
              "  'It',\n",
              "  ' is',\n",
              "  ' extremely',\n",
              "  ' hard',\n",
              "  ' to',\n",
              "  ' find',\n",
              "  ' the',\n",
              "  ' right',\n",
              "  ' position',\n",
              "  ' ',\n",
              "  'between',\n",
              "  ' these',\n",
              "  ' two',\n",
              "  ' extremes',\n",
              "  'n ',\n",
              "  'Even',\n",
              "  ' if',\n",
              "  ' it',\n",
              "  ' is',\n",
              "  ' accurate',\n",
              "  ',',\n",
              "  '  all',\n",
              "  ' relevant documents',\n",
              "  ' ',\n",
              "  'are',\n",
              "  ' not',\n",
              "  ' equally',\n",
              "  ' relevant',\n",
              "  'n',\n",
              "  ' ',\n",
              "  'Relevance',\n",
              "  ' is a',\n",
              "  ' matter',\n",
              "  ' of',\n",
              "  ' degree',\n",
              "  '!',\n",
              "  '2019/3/11',\n",
              "  '16',\n",
              "  '\\x0cRanking',\n",
              "  'n',\n",
              "  ' ',\n",
              "  'A',\n",
              "  ' ranking',\n",
              "  ' is an',\n",
              "  ' ordering',\n",
              "  ' of',\n",
              "  ' ',\n",
              "  'the',\n",
              "  ' documents',\n",
              "  ' retrieved',\n",
              "  ' ',\n",
              "  'that',\n",
              "  ' ',\n",
              "  '(',\n",
              "  'hopefully)',\n",
              "  ' reflects',\n",
              "  ' the',\n",
              "  ' relevance',\n",
              "  ' of',\n",
              "  ' ',\n",
              "  'the',\n",
              "  ' ',\n",
              "  'documents',\n",
              "  ' to',\n",
              "  ' the',\n",
              "  ' ',\n",
              "  ' user',\n",
              "  ' query',\n",
              "  ' ',\n",
              "  'n A',\n",
              "  ' ranking',\n",
              "  ' is',\n",
              "  ' based',\n",
              "  ' on',\n",
              "  ' fundamental',\n",
              "  ' premisses',\n",
              "  ' ',\n",
              "  'regarding',\n",
              "  ' the',\n",
              "  ' notion',\n",
              "  ' of',\n",
              "  ' relevance',\n",
              "  ',',\n",
              "  ' such',\n",
              "  ' as',\n",
              "  ':',\n",
              "  'q',\n",
              "  ' common',\n",
              "  ' sets',\n",
              "  ' of',\n",
              "  ' index',\n",
              "  ' terms',\n",
              "  'q sharing',\n",
              "  ' of',\n",
              "  ' weighted',\n",
              "  ' terms',\n",
              "  'q',\n",
              "  ' likelihood',\n",
              "  ' of',\n",
              "  ' relevance',\n",
              "  'n',\n",
              "  ' ',\n",
              "  'Each',\n",
              "  ' set',\n",
              "  ' ',\n",
              "  'of',\n",
              "  ' premisses',\n",
              "  ' leads',\n",
              "  ' to',\n",
              "  ' a',\n",
              "  ' distinct',\n",
              "  ' IR',\n",
              "  ' model',\n",
              "  '2019/3/11',\n",
              "  '17',\n",
              "  '\\x0c',\n",
              "  'Ranking',\n",
              "  ' is often',\n",
              "  ' preferred',\n",
              "  'n',\n",
              "  ' ',\n",
              "  'Relevance',\n",
              "  ' is a',\n",
              "  ' matter',\n",
              "  ' of',\n",
              "  ' degree',\n",
              "  'n A',\n",
              "  ' user',\n",
              "  ' can',\n",
              "  ' stop',\n",
              "  ' browsing',\n",
              "  ' any',\n",
              "  'where',\n",
              "  ...],\n",
              " 'Retrieval model - vector space': ['Retrieval',\n",
              "  ' ',\n",
              "  'Model',\n",
              "  ':',\n",
              "  ' Vector',\n",
              "  ' ',\n",
              "  'Space',\n",
              "  'Fu-ren',\n",
              "  ' Lin',\n",
              "  'Institute',\n",
              "  ' ',\n",
              "  'of',\n",
              "  ' ',\n",
              "  'Service',\n",
              "  ' ',\n",
              "  'Science',\n",
              "  'National',\n",
              "  ' Tsing',\n",
              "  ' ',\n",
              "  'Hua',\n",
              "  ' ',\n",
              "  'University',\n",
              "  '\\x0c',\n",
              "  'The',\n",
              "  ' Basic',\n",
              "  ' ',\n",
              "  'Question',\n",
              "  'n ',\n",
              "  'Given',\n",
              "  ' a',\n",
              "  ' query',\n",
              "  ',',\n",
              "  ' how',\n",
              "  ' do',\n",
              "  ' we',\n",
              "  ' know',\n",
              "  ' if',\n",
              "  ' document',\n",
              "  ' ',\n",
              "  'A is',\n",
              "  ' more',\n",
              "  ' relevant',\n",
              "  ' than',\n",
              "  ' B',\n",
              "  '?',\n",
              "  'n',\n",
              "  ' A possible',\n",
              "  ' answer',\n",
              "  ':',\n",
              "  'q',\n",
              "  ' ',\n",
              "  'If',\n",
              "  ' document',\n",
              "  ' A',\n",
              "  ' uses',\n",
              "  ' more',\n",
              "  ' query',\n",
              "  ' words',\n",
              "  ' than',\n",
              "  ' ',\n",
              "  'document',\n",
              "  ' B ',\n",
              "  '(',\n",
              "  'Word',\n",
              "  ' usage',\n",
              "  ' in',\n",
              "  ' document',\n",
              "  ' A',\n",
              "  ' is',\n",
              "  ' more',\n",
              "  ' ',\n",
              "  'similar to',\n",
              "  ' that',\n",
              "  ' in',\n",
              "  ' query',\n",
              "  ')',\n",
              "  '\\x0c',\n",
              "  'Relevance',\n",
              "  ' = Similarity',\n",
              "  'n Assumptions',\n",
              "  'q ',\n",
              "  'Query',\n",
              "  ' and document',\n",
              "  ' are',\n",
              "  ' represented',\n",
              "  ' similarly',\n",
              "  'q A',\n",
              "  ' query',\n",
              "  ' can',\n",
              "  ' be',\n",
              "  ' regarded',\n",
              "  ' as',\n",
              "  ' a ',\n",
              "  '“do',\n",
              "  'cument',\n",
              "  '”',\n",
              "  'q ',\n",
              "  'Rele',\n",
              "  'vance',\n",
              "  '(',\n",
              "  'd',\n",
              "  ',',\n",
              "  'q)',\n",
              "  ' µ similarity',\n",
              "  '(',\n",
              "  'd',\n",
              "  ',',\n",
              "  'q',\n",
              "  ')',\n",
              "  'n R',\n",
              "  '(',\n",
              "  'q)',\n",
              "  ' = {dÎC|f',\n",
              "  '(',\n",
              "  'd',\n",
              "  ',',\n",
              "  'q)',\n",
              "  '>',\n",
              "  'q}',\n",
              "  ',',\n",
              "  ' f',\n",
              "  '(',\n",
              "  'q',\n",
              "  ',',\n",
              "  'd)',\n",
              "  '=D',\n",
              "  '(',\n",
              "  'Rep',\n",
              "  '(',\n",
              "  'q)',\n",
              "  ',',\n",
              "  ' Rep',\n",
              "  '(',\n",
              "  'd)',\n",
              "  ')',\n",
              "  'n ',\n",
              "  'Key',\n",
              "  ' issues',\n",
              "  'q ',\n",
              "  'How',\n",
              "  ' to',\n",
              "  ' represent',\n",
              "  ' query/document',\n",
              "  '?',\n",
              "  'q',\n",
              "  ' ',\n",
              "  'How',\n",
              "  ' to',\n",
              "  ' define',\n",
              "  ' the',\n",
              "  ' similarity',\n",
              "  ' measure',\n",
              "  ' D',\n",
              "  '?',\n",
              "  '\\x0c',\n",
              "  'Word',\n",
              "  ' ',\n",
              "  'count',\n",
              "  ' profile',\n",
              "  'n A',\n",
              "  ' ',\n",
              "  'do',\n",
              "  'cument',\n",
              "  ' word',\n",
              "  ' profile',\n",
              "  ' tells',\n",
              "  ' us',\n",
              "  ' a',\n",
              "  ' lot',\n",
              "  ' about',\n",
              "  ' ',\n",
              "  'what',\n",
              "  ' ',\n",
              "  'the',\n",
              "  ' document',\n",
              "  ' might',\n",
              "  ' be',\n",
              "  ' about',\n",
              "  '.',\n",
              "  ' ',\n",
              "  'n A',\n",
              "  ' query',\n",
              "  ' word',\n",
              "  ' profile',\n",
              "  ' tells',\n",
              "  ' us',\n",
              "  ' a',\n",
              "  ' lot',\n",
              "  ' about',\n",
              "  ' what',\n",
              "  ' ',\n",
              "  'information',\n",
              "  ' the',\n",
              "  ' user',\n",
              "  ' might',\n",
              "  ' want',\n",
              "  'n So',\n",
              "  ',',\n",
              "  ' perhaps',\n",
              "  ' we',\n",
              "  ' could',\n",
              "  ' represent',\n",
              "  ' a ',\n",
              "  'do',\n",
              "  'cument/query',\n",
              "  ' by',\n",
              "  ' a',\n",
              "  ' word',\n",
              "  ' count',\n",
              "  ' vector',\n",
              "  '?',\n",
              "  '\\x0c',\n",
              "  'Vector',\n",
              "  ' ',\n",
              "  'Space',\n",
              "  ' Model',\n",
              "  'n ',\n",
              "  'Represent a doc/query',\n",
              "  ' by',\n",
              "  ' a term',\n",
              "  ' vector',\n",
              "  'q Term',\n",
              "  ':',\n",
              "  ' basic',\n",
              "  ' concept',\n",
              "  ',',\n",
              "  ' e.g.',\n",
              "  ',',\n",
              "  ' word',\n",
              "  ' or',\n",
              "  ' phrase',\n",
              "  'q ',\n",
              "  'Each term',\n",
              "  ' defines',\n",
              "  ' one',\n",
              "  ' dimension',\n",
              "  'q N terms',\n",
              "  ' define',\n",
              "  ' a',\n",
              "  ' high-dimensional',\n",
              "  ' space',\n",
              "  'q ',\n",
              "  'Element',\n",
              "  ' of',\n",
              "  ' vector',\n",
              "  ' corres',\n",
              "  'ponds',\n",
              "  ' to',\n",
              "  ' term',\n",
              "  ' weight',\n",
              "  'q e.g',\n",
              "  '.',\n",
              "  ',',\n",
              "  ' d',\n",
              "  '=(x1',\n",
              "  ',',\n",
              "  '…',\n",
              "  ',',\n",
              "  'xN)',\n",
              "  ',',\n",
              "  ' xi',\n",
              "  ' is',\n",
              "  ' ',\n",
              "  '“importance',\n",
              "  '”',\n",
              "  ' of',\n",
              "  ' term',\n",
              "  ' i',\n",
              "  'n',\n",
              "  ' ',\n",
              "  'Measure',\n",
              "  ' relevance',\n",
              "  ' by',\n",
              "  ' the',\n",
              "  ' distance',\n",
              "  ' between',\n",
              "  ' ',\n",
              "  'the',\n",
              "  ' query',\n",
              "  ' vector',\n",
              "  ' and',\n",
              "  ' document',\n",
              "  ' vector',\n",
              "  ' in',\n",
              "  ' ',\n",
              "  'the',\n",
              "  ' ',\n",
              "  'vector',\n",
              "  ' space',\n",
              "  '5',\n",
              "  '\\x0cVS ',\n",
              "  'Model',\n",
              "  ':',\n",
              "  ' illustration',\n",
              "  '?',\n",
              "  ' ',\n",
              "  '?',\n",
              "  'D3',\n",
              "  'D10',\n",
              "  'D7',\n",
              "  'D8',\n",
              "  'Micros',\n",
              "  'oft',\n",
              "  'Starbucks',\n",
              "  'D9',\n",
              "  'D11',\n",
              "  'D2 ',\n",
              "  '?',\n",
              "  ' ',\n",
              "  '?',\n",
              "  'D5',\n",
              "  'D4',\n",
              "  'D6',\n",
              "  'Query',\n",
              "  'D1',\n",
              "  '?',\n",
              "  ' ',\n",
              "  '?',\n",
              "  'Java',\n",
              "  '\\x0cWhat',\n",
              "  ' the',\n",
              "  ' VS ',\n",
              "  'model',\n",
              "  ' doesn',\n",
              "  '’t',\n",
              "  ' say',\n",
              "  'n',\n",
              "  ' ',\n",
              "  'How to',\n",
              "  ' define/select the',\n",
              "  ' “basic',\n",
              "  ' concept',\n",
              "  '”',\n",
              "  'q ',\n",
              "  'Concepts',\n",
              "  ' are',\n",
              "  ' assumed',\n",
              "  ' to',\n",
              "  ' be',\n",
              "  ' or',\n",
              "  'thogonal',\n",
              "  'n ',\n",
              "  'How',\n",
              "  ' to',\n",
              "  ' assign',\n",
              "  ' weights',\n",
              "  'q ',\n",
              "  'Weight',\n",
              "  ' in',\n",
              "  ' query',\n",
              "  ' indicates',\n",
              "  ' importance',\n",
              "  ' of',\n",
              "  ' term',\n",
              "  'q ',\n",
              "  'Weight',\n",
              "  ' in doc',\n",
              "  ' indicates',\n",
              "  ' how well',\n",
              "  ' ',\n",
              "  'the',\n",
              "  ' term',\n",
              "  ' ',\n",
              "  'characterizes',\n",
              "  ' the',\n",
              "  ' doc',\n",
              "  'n',\n",
              "  ' ',\n",
              "  'How to',\n",
              "  ' define',\n",
              "  ' the',\n",
              "  ' similarity/distance',\n",
              "  ' measure',\n",
              "  '?',\n",
              "  '\\x0cWhat',\n",
              "  '’s',\n",
              "  ' a',\n",
              "  ' good',\n",
              "  ' ',\n",
              "  '“basic',\n",
              "  ' concept',\n",
              "  '”',\n",
              "  '?',\n",
              "  'n Orthogonal',\n",
              "  'q ',\n",
              "  'Linearly',\n",
              "  ' independent',\n",
              "  ' basis',\n",
              "  ' vectors',\n",
              "  'q “',\n",
              "  'Non-over',\n",
              "  'lapping” in ',\n",
              "  'meaning',\n",
              "  'n No',\n",
              "  ' ambiguity',\n",
              "  'n',\n",
              "  ' ',\n",
              "  'Weights',\n",
              "  ' can',\n",
              "  ' be',\n",
              "  ' assigned',\n",
              "  ' automatically',\n",
              "  ' and',\n",
              "  ' ',\n",
              "  'hope',\n",
              "  'fully',\n",
              "  ' accurately',\n",
              "  'n',\n",
              "  ' ',\n",
              "  'Many',\n",
              "  ' possibilities',\n",
              "  ':',\n",
              "  ' Words',\n",
              "  ',',\n",
              "  ' stemmed',\n",
              "  ' words',\n",
              "  ',',\n",
              "  ' ',\n",
              "  'phrases',\n",
              "  ',',\n",
              "  ' “latent',\n",
              "  ' concept',\n",
              "  '”',\n",
              "  ',',\n",
              "  ' ',\n",
              "  '…',\n",
              "  '\\x0c',\n",
              "  'How to Assign ',\n",
              "  'Weights',\n",
              "  '?',\n",
              "  'n ',\n",
              "  'Very',\n",
              "  ' very',\n",
              "  ' important',\n",
              "  '!',\n",
              "  'n ',\n",
              "  'Why',\n",
              "  ' weighting',\n",
              "  'q ',\n",
              "  'Query',\n",
              "  ' side',\n",
              "  ':',\n",
              "  ' ',\n",
              "  'Not',\n",
              "  ' all',\n",
              "  ' terms',\n",
              "  ' are',\n",
              "  ' equally',\n",
              "  ' important',\n",
              "  'q ',\n",
              "  'Doc side',\n",
              "  ':',\n",
              "  ' Some',\n",
              "  ' terms',\n",
              "  ' carry',\n",
              "  ' more',\n",
              "  ' contents',\n",
              "  'n How',\n",
              "  '?',\n",
              "  ' ',\n",
              "  'q ',\n",
              "  'Two',\n",
              "  ' basic',\n",
              "  ' heuristics',\n",
              "  'n TF ',\n",
              "  '(',\n",
              "  'Term Frequency)',\n",
              "  ' = Within-doc-frequency',\n",
              "  'n',\n",
              "  ' IDF ',\n",
              "  '(',\n",
              "  'Inverse',\n",
              "  ' ',\n",
              "  'Document',\n",
              "  ' ',\n",
              "  'Frequency',\n",
              "  ')',\n",
              "  'q ',\n",
              "  'Document',\n",
              "  ' leng',\n",
              "  'th',\n",
              "  ' normalization',\n",
              "  ' ',\n",
              "  '\\x0cTF ',\n",
              "  'Weighting',\n",
              "  'n Idea',\n",
              "  ':',\n",
              "  ' A',\n",
              "  ' term',\n",
              "  ' is',\n",
              "  ' more',\n",
              "  ' important',\n",
              "  ' if',\n",
              "  ' it',\n",
              "  ' occurs',\n",
              "  ' ',\n",
              "  'more',\n",
              "  ' frequent',\n",
              "  'ly',\n",
              "  ' in',\n",
              "  ' a',\n",
              "  ' document',\n",
              "  'n ',\n",
              "  'Formulas',\n",
              "  ':',\n",
              "  ' Let',\n",
              "  ' f',\n",
              "  '(',\n",
              "  't',\n",
              "  ',',\n",
              "  'd)',\n",
              "  ' be the',\n",
              "  ' frequency',\n",
              "  ' count',\n",
              "  ' of',\n",
              "  ' ',\n",
              "  'term',\n",
              "  ' t',\n",
              "  ' in',\n",
              "  ' doc',\n",
              "  ' d',\n",
              "  'q Raw',\n",
              "  ' TF',\n",
              "  ':',\n",
              "  '  TF',\n",
              "  '(',\n",
              "  't',\n",
              "  ',',\n",
              "  'd)',\n",
              "  ' = f',\n",
              "  '(',\n",
              "  't',\n",
              "  ',',\n",
              "  'd',\n",
              "  ')',\n",
              "  'q ',\n",
              "  'Maximum',\n",
              "  ' frequen',\n",
              "  'cy',\n",
              "  ' normalization',\n",
              "  ':',\n",
              "  ' ',\n",
              "  'TF',\n",
              "  '(',\n",
              "  't',\n",
              "  ',',\n",
              "  'd)',\n",
              "  ' = 0.5 +0.5*f',\n",
              "  '(',\n",
              "  't',\n",
              "  ',',\n",
              "  'd)/Max',\n",
              "  'Freq',\n",
              "  '(',\n",
              "  'd)',\n",
              "  'q “Okapi/BM25 TF”',\n",
              "  ':',\n",
              "  ' ',\n",
              "  'TF',\n",
              "  '(',\n",
              "  't',\n",
              "  ',',\n",
              "  'd)',\n",
              "  '=kf',\n",
              "  '(',\n",
              "  't',\n",
              "  ',',\n",
              "  'd)',\n",
              "  '/',\n",
              "  '(',\n",
              "  'f',\n",
              "  '(',\n",
              "  't',\n",
              "  ',',\n",
              "  'd)',\n",
              "  '+k',\n",
              "  '(',\n",
              "  '1-b+b*doclen/avg',\n",
              "  'doclen)',\n",
              "  ')',\n",
              "  'n',\n",
              "  ' ',\n",
              "  'Normalization',\n",
              "  ' of',\n",
              "  ' T',\n",
              "  'F',\n",
              "  ' is',\n",
              "  ' very',\n",
              "  ' important',\n",
              "  '!',\n",
              "  '\\x0c',\n",
              "  'Normalization',\n",
              "  ' techniques',\n",
              "  'n',\n",
              "  ' Cosine',\n",
              "  ' normalization',\n",
              "  'n',\n",
              "  ' ',\n",
              "  'Maximum',\n",
              "  ' tf',\n",
              "  ' normalization',\n",
              "  'q ',\n",
              "  'The',\n",
              "  ' ',\n",
              "  'S',\n",
              "  'mart',\n",
              "  ' system',\n",
              "  '’s',\n",
              "  ' augmented',\n",
              "  ' tf',\n",
              "  ' factor',\n",
              "  'n ',\n",
              "  'Byte',\n",
              "  ' leng',\n",
              "  'th',\n",
              "  ' normalization',\n",
              "  'q ',\n",
              "  'Okapi',\n",
              "  ' system',\n",
              "  '\\x0cTF ',\n",
              "  'Normalization',\n",
              "  'n Why',\n",
              "  '?',\n",
              "  ' ',\n",
              "  'q ',\n",
              "  'Document',\n",
              "  ' leng',\n",
              "  'th',\n",
              "  ' variation',\n",
              "  'q',\n",
              "  ' “',\n",
              "  'Repeated',\n",
              "  ' occurrences',\n",
              "  '” are',\n",
              "  ' less',\n",
              "  ' informative',\n",
              "  ' than',\n",
              "  ' ',\n",
              "  'the',\n",
              "  ' ',\n",
              "  '“first',\n",
              "  ' occurrence',\n",
              "  '”',\n",
              "  'n',\n",
              "  ' ',\n",
              "  'Two',\n",
              "  ' views',\n",
              "  ' ',\n",
              "  'of',\n",
              "  ' document',\n",
              "  ' length',\n",
              "  'q A',\n",
              "  ' doc',\n",
              "  ' is',\n",
              "  ' long',\n",
              "  ' because',\n",
              "  ' it',\n",
              "  ' uses',\n",
              "  ' more',\n",
              "  ' words',\n",
              "  'q A',\n",
              "  ' doc',\n",
              "  ' is',\n",
              "  ' long',\n",
              "  ' because',\n",
              "  ' it',\n",
              "  ' has',\n",
              "  ' more',\n",
              "  ' contents',\n",
              "  'n',\n",
              "  ' ',\n",
              "  'Generally',\n",
              "  ' penalize',\n",
              "  ' long',\n",
              "  ' doc',\n",
              "  ',',\n",
              "  ' but',\n",
              "  ' avoid',\n",
              "  ' over',\n",
              "  '-',\n",
              "  'penalizing',\n",
              "  ' ',\n",
              "  '(',\n",
              "  'pivoted',\n",
              "  ' normalization)',\n",
              "  '\\x0c',\n",
              "  'Pivoted',\n",
              "  ' Normalization',\n",
              "  'P',\n",
              "  '(',\n",
              "  'retrieval)',\n",
              "  '>',\n",
              "  'P',\n",
              "  '(',\n",
              "  'relevance)',\n",
              "  ' is',\n",
              "  ' increased',\n",
              "  'P',\n",
              "  '(',\n",
              "  'retrieval)',\n",
              "  '<',\n",
              "  'P',\n",
              "  '(',\n",
              "  'relevance)',\n",
              "  ' is',\n",
              "  ' decreased',\n",
              "  '\\x0c',\n",
              "  'Pivoted',\n",
              "  ' Normalization',\n",
              "  ' ',\n",
              "  '(',\n",
              "  'cont.)',\n",
              "  'n',\n",
              "  ' ',\n",
              "  'Pivoted',\n",
              "  ' normalization',\n",
              "  ' ',\n",
              "  '= ',\n",
              "  '(',\n",
              "  '1.0-slope)',\n",
              "  'Îpivot',\n",
              "  ' + slope',\n",
              "  'Îold',\n",
              "  ' normalization',\n",
              "  '(',\n",
              "  'Snghal',\n",
              "  ',',\n",
              "  ' et al',\n",
              "  '.',\n",
              "  ',',\n",
              "  ' 1995',\n",
              "  ';',\n",
              "  ' ',\n",
              "  '1996',\n",
              "  ')',\n",
              "  '\\x0cIDF ',\n",
              "  'Weighting',\n",
              "  'n Idea',\n",
              "  ':',\n",
              "  ' A',\n",
              "  ' term',\n",
              "  ' is',\n",
              "  ' more',\n",
              "  ' discriminative',\n",
              "  ' if',\n",
              "  ' it',\n",
              "  ' occurs',\n",
              "  ' ',\n",
              "  'only',\n",
              "  ' in',\n",
              "  ' fewer',\n",
              "  ' documents',\n",
              "  'n ',\n",
              "  'Formula',\n",
              "  ':',\n",
              "  'IDF',\n",
              "  '(',\n",
              "  't) = 1+ log',\n",
              "  '(',\n",
              "  'N/n)',\n",
              "  'N',\n",
              "  ':',\n",
              "  ' total',\n",
              "  ' number',\n",
              "  ' of',\n",
              "  ' docs',\n",
              "  'n',\n",
              "  ':',\n",
              "  ' # docs',\n",
              "  ' with term',\n",
              "  ' t',\n",
              "  ' ',\n",
              "  '(',\n",
              "  'doc',\n",
              "  ' freq)',\n",
              "  '\\x0cTF-IDF Weighting',\n",
              "  'n TF-IDF weighting',\n",
              "  ' ',\n",
              "  ':',\n",
              "  ' ',\n",
              "  'weight',\n",
              "  '(',\n",
              "  't',\n",
              "  ',',\n",
              "  'd)',\n",
              "  '=TF',\n",
              "  '(',\n",
              "  't',\n",
              "  ',',\n",
              "  'd',\n",
              "  ')',\n",
              "  '*IDF',\n",
              "  '(',\n",
              "  't',\n",
              "  ')',\n",
              "  'q ',\n",
              "  'Freq',\n",
              "  ' in',\n",
              "  ' doc',\n",
              "  ' à',\n",
              "  ' high',\n",
              "  ' tf',\n",
              "  ' à',\n",
              "  ' high',\n",
              "  ' weight',\n",
              "  'q ',\n",
              "  'Rare',\n",
              "  ' in',\n",
              "  ' collection',\n",
              "  'à',\n",
              "  ' high',\n",
              "  ' idfà',\n",
              "  ' high',\n",
              "  ' ',\n",
              "  'weight',\n",
              "  'n',\n",
              "  ' ',\n",
              "  'I',\n",
              "  'magine',\n",
              "  ' a word',\n",
              "  ' count',\n",
              "  ' profile',\n",
              "  ',',\n",
              "  ' what',\n",
              "  ' kind',\n",
              "  ' of',\n",
              "  ' ',\n",
              "  'terms',\n",
              "  ' would',\n",
              "  ' have',\n",
              "  ' high',\n",
              "  ' weights',\n",
              "  '?',\n",
              "  ' ',\n",
              "  'n ',\n",
              "  'How',\n",
              "  ' is',\n",
              "  ' this related',\n",
              "  ' to ',\n",
              "  'Luhn',\n",
              "  '’s',\n",
              "  ' study',\n",
              "  ' ',\n",
              "  'of',\n",
              "  ' term',\n",
              "  ' ',\n",
              "  'frequen',\n",
              "  'cy',\n",
              "  ' and',\n",
              "  ' ',\n",
              "  'Zipf',\n",
              "  '’s',\n",
              "  ' law',\n",
              "  '?',\n",
              "  '\\x0c',\n",
              "  'Luhn',\n",
              "  '’s study',\n",
              "  ' of',\n",
              "  ' ',\n",
              "  ' term',\n",
              "  ' frequency',\n",
              "  '\\x0cThree',\n",
              "  ' ',\n",
              "  'Term-weighting',\n",
              "  ' ',\n",
              "  'Components',\n",
              "  'Type ',\n",
              "  'I',\n",
              "  'Term',\n",
              "  ' ',\n",
              "  'frequency',\n",
              "  ' ',\n",
              "  'component',\n",
              "  'Type',\n",
              "  ' II',\n",
              "  'Document',\n",
              "  ' ',\n",
              "  'frequency',\n",
              "  ' ',\n",
              "  'component',\n",
              "  'b',\n",
              "  ',',\n",
              "  ' 1.0',\n",
              "  't',\n",
              "  ',',\n",
              "  ' tf',\n",
              "  'Binary',\n",
              "  ' weight equals',\n",
              "  ' to',\n",
              "  ' 1',\n",
              "  ' for terms',\n",
              "  ' present',\n",
              "  ' in',\n",
              "  ' a',\n",
              "  ' vector',\n",
              "  ' ',\n",
              "  '(',\n",
              "  'term',\n",
              "  ' ',\n",
              "  'frequen',\n",
              "  'cy',\n",
              "  ' is',\n",
              "  ' ignored',\n",
              "  ')',\n",
              "  'Term',\n",
              "  ' frequency',\n",
              "  ' ',\n",
              "  '(',\n",
              "  'number',\n",
              "  ' of',\n",
              "  ' times',\n",
              "  ' a',\n",
              "  ' term',\n",
              "  ' occurs',\n",
              "  ' in',\n",
              "  ' a',\n",
              "  ' document',\n",
              "  ' or ',\n",
              "  'query',\n",
              "  ' text)',\n",
              "  '.',\n",
              "  '.',\n",
              "  'n',\n",
              "  ',',\n",
              "  ' 0.5+0.5',\n",
              "  '(',\n",
              "  'tf',\n",
              "  ' /max',\n",
              "  ' tf',\n",
              "  ')',\n",
              "  'Augmented',\n",
              "  ' normalized',\n",
              "  ' term',\n",
              "  ' frequency',\n",
              "  ' ',\n",
              "  '(',\n",
              "  'tf)',\n",
              "  ' factor',\n",
              "  ' normalized',\n",
              "  ' by',\n",
              "  ' ',\n",
              "  'maximum',\n",
              "  ' ',\n",
              "  'tf',\n",
              "  ' in',\n",
              "  ' the',\n",
              "  ' vector',\n",
              "  ',',\n",
              "  ' and',\n",
              "  ' further',\n",
              "  ' normalized',\n",
              "  ' to',\n",
              "  ' lie',\n",
              "  ' between',\n",
              "  ' ',\n",
              "  '0.5',\n",
              "  ' and',\n",
              "  ' 1.0.',\n",
              "  'x',\n",
              "  ',',\n",
              "  ' 1.0',\n",
              "  'No',\n",
              "  ' change',\n",
              "  ' in',\n",
              "  ' weights',\n",
              "  ';',\n",
              "  ' use',\n",
              "  ' original',\n",
              "  ' term',\n",
              "  ' frequency',\n",
              "  ' component',\n",
              "  '.',\n",
              "  'f',\n",
              "  ',',\n",
              "  ' ',\n",
              "  '(',\n",
              "  'log ',\n",
              "  '(',\n",
              "  'N/n)',\n",
              "  ')',\n",
              "  'Multiply',\n",
              "  ' original',\n",
              "  ' tf',\n",
              "  ' factor',\n",
              "  ' by',\n",
              "  ' an',\n",
              "  ' inverse',\n",
              "  ' document',\n",
              "  ' frequen',\n",
              "  'cy',\n",
              "  ' factor',\n",
              "  ' ',\n",
              "  '(',\n",
              "  'idf)',\n",
              "  ' ',\n",
              "  '(',\n",
              "  'N is',\n",
              "  ' total',\n",
              "  ' number of',\n",
              "  ' documents',\n",
              "  ...],\n",
              " 'Week3-From Textual Information to Numerical Vector': ['From',\n",
              "  ' ',\n",
              "  'Textual',\n",
              "  ' ',\n",
              "  'Information',\n",
              "  ' to',\n",
              "  ' ',\n",
              "  'Numerical',\n",
              "  ' ',\n",
              "  'Vector',\n",
              "  'with ',\n",
              "  'Py',\n",
              "  'thon',\n",
              "  'Fu-ren',\n",
              "  ' Lin',\n",
              "  ',',\n",
              "  ' Gild',\n",
              "  ' Shen',\n",
              "  ',',\n",
              "  ' Viet-Cuong Trieu',\n",
              "  ' ',\n",
              "  '(',\n",
              "  'Daniel)',\n",
              "  ' ',\n",
              "  'National',\n",
              "  ' Tsing',\n",
              "  ' ',\n",
              "  'Hua',\n",
              "  ' ',\n",
              "  'University',\n",
              "  '\\x0cOutlines',\n",
              "  '1. Tokenization',\n",
              "  ' ',\n",
              "  '(',\n",
              "  'word',\n",
              "  ',',\n",
              "  ' multi-word',\n",
              "  ',',\n",
              "  ' sentence',\n",
              "  ')',\n",
              "  '2. Stemming',\n",
              "  ' vs',\n",
              "  ' ',\n",
              "  'Lemmatization',\n",
              "  '3. Text',\n",
              "  ' ',\n",
              "  'Normalization',\n",
              "  '4. Stopwords',\n",
              "  ' & Build',\n",
              "  ' stopwords',\n",
              "  '5. ',\n",
              "  'Term',\n",
              "  ' frequency',\n",
              "  ' ',\n",
              "  '(',\n",
              "  'TF)',\n",
              "  ' vectorizer',\n",
              "  '\\x0c',\n",
              "  'If',\n",
              "  ' y',\n",
              "  'ou',\n",
              "  ' have',\n",
              "  ' any',\n",
              "  ' question',\n",
              "  ',',\n",
              "  ' please',\n",
              "  ' ask me',\n",
              "  ' ',\n",
              "  'in',\n",
              "  ' the',\n",
              "  ' slido',\n",
              "  ' at slido.com',\n",
              "  ' with',\n",
              "  ' ',\n",
              "  '#809100',\n",
              "  '\\x0c',\n",
              "  '• Working',\n",
              "  ' with',\n",
              "  ' “Natural',\n",
              "  ' ',\n",
              "  'Language',\n",
              "  ' ',\n",
              "  'Toolkit',\n",
              "  '” https',\n",
              "  ':',\n",
              "  '//www.nltk.org',\n",
              "  '/',\n",
              "  '•',\n",
              "  ' Online',\n",
              "  ' book',\n",
              "  ':',\n",
              "  ' http',\n",
              "  ':',\n",
              "  '//www.nltk.org/book',\n",
              "  '/',\n",
              "  '1. Tokenization',\n",
              "  '2. Stemming',\n",
              "  '3. Lemmatization',\n",
              "  'Bird',\n",
              "  ',',\n",
              "  ' Steven',\n",
              "  ',',\n",
              "  ' Edward ',\n",
              "  'Loper',\n",
              "  ' and',\n",
              "  ' Ewan',\n",
              "  ' ',\n",
              "  'Klein',\n",
              "  ' ',\n",
              "  '(',\n",
              "  '2009)',\n",
              "  ',',\n",
              "  ' Natural',\n",
              "  ' Language',\n",
              "  ' ',\n",
              "  'Processing',\n",
              "  ' with',\n",
              "  ' ',\n",
              "  'Python. O’',\n",
              "  'Reilly',\n",
              "  ' ',\n",
              "  'Media',\n",
              "  ' ',\n",
              "  'Inc',\n",
              "  '.',\n",
              "  '\\x0cUsing',\n",
              "  ' ',\n",
              "  'Google',\n",
              "  ' ',\n",
              "  'Colab',\n",
              "  'Click',\n",
              "  ' on',\n",
              "  ' this',\n",
              "  ' link to',\n",
              "  ' open',\n",
              "  ' TA',\n",
              "  ' sharing',\n",
              "  ':',\n",
              "  'https',\n",
              "  ':',\n",
              "  '//bit.ly/3vwjDGi',\n",
              "  'Then',\n",
              "  ' click',\n",
              "  ' on “',\n",
              "  'Copy',\n",
              "  ' do ',\n",
              "  'Drive',\n",
              "  '” to',\n",
              "  ' save',\n",
              "  ' to',\n",
              "  ' your',\n",
              "  ' ',\n",
              "  'google',\n",
              "  ' drive',\n",
              "  '\\x0cWord',\n",
              "  ' ',\n",
              "  'Tokenization',\n",
              "  'Word',\n",
              "  ' tokenization',\n",
              "  ' is',\n",
              "  ' the',\n",
              "  ' process',\n",
              "  ' of',\n",
              "  ' splitting',\n",
              "  ' a',\n",
              "  ' large',\n",
              "  ' sample',\n",
              "  ' of',\n",
              "  ' text',\n",
              "  ' into',\n",
              "  ' words. ',\n",
              "  'This',\n",
              "  ' is',\n",
              "  ' a',\n",
              "  ' requirement',\n",
              "  ' in',\n",
              "  ' natural',\n",
              "  ' language',\n",
              "  ' processing',\n",
              "  ' tasks',\n",
              "  ' where',\n",
              "  ' ',\n",
              "  'ea',\n",
              "  'ch',\n",
              "  ' word',\n",
              "  ' needs',\n",
              "  ' to',\n",
              "  ' be',\n",
              "  ' captured',\n",
              "  ' and',\n",
              "  ' subjected',\n",
              "  ' ',\n",
              "  'to',\n",
              "  ' further',\n",
              "  ' analysis',\n",
              "  ' like',\n",
              "  ' classifying',\n",
              "  ' ',\n",
              "  'and',\n",
              "  ' counting',\n",
              "  ' them',\n",
              "  ' ',\n",
              "  'for a',\n",
              "  ' particular',\n",
              "  ' sentiment',\n",
              "  ' etc',\n",
              "  'Text',\n",
              "  \"Let's\",\n",
              "  ' go',\n",
              "  ' ahead',\n",
              "  ' and',\n",
              "  ' start',\n",
              "  ' now. We',\n",
              "  ' can',\n",
              "  \"'t\",\n",
              "  ' wait any',\n",
              "  ' longer',\n",
              "  'Tokenization',\n",
              "  '(',\n",
              "  'Splitting)',\n",
              "  ' ',\n",
              "  '“Blank',\n",
              "  ' ',\n",
              "  'Space',\n",
              "  '”',\n",
              "  'splitting',\n",
              "  'We',\n",
              "  'can',\n",
              "  '’t',\n",
              "  'wait',\n",
              "  '“',\n",
              "  'Punctuation',\n",
              "  ' ',\n",
              "  'Marks',\n",
              "  '”',\n",
              "  'splitting',\n",
              "  '“Dictionary',\n",
              "  ' ',\n",
              "  'base',\n",
              "  '”',\n",
              "  'splitting',\n",
              "  'We',\n",
              "  'can',\n",
              "  '’',\n",
              "  't',\n",
              "  'wait',\n",
              "  'We',\n",
              "  'ca',\n",
              "  'n’t',\n",
              "  'wait',\n",
              "  '\\x0c2. Word',\n",
              "  ' ',\n",
              "  'Tokenization',\n",
              "  '“Blank',\n",
              "  ' ',\n",
              "  'Space',\n",
              "  '”',\n",
              "  'splitting',\n",
              "  '“',\n",
              "  'Punctuation',\n",
              "  ' ',\n",
              "  'Marks',\n",
              "  '”',\n",
              "  'splitting',\n",
              "  '“Dictionary',\n",
              "  ' ',\n",
              "  'base',\n",
              "  '”',\n",
              "  'splitting',\n",
              "  '\\x0cword_tokenize',\n",
              "  ' function',\n",
              "  'word',\n",
              "  'punct_tokenize',\n",
              "  ' separate',\n",
              "  ' word',\n",
              "  ' based',\n",
              "  ' ',\n",
              "  'on',\n",
              "  ' punctuation',\n",
              "  ' ',\n",
              "  'Marks',\n",
              "  ':',\n",
              "  \"'\",\n",
              "  '!',\n",
              "  '\"#$%&\\\\\\'',\n",
              "  '(',\n",
              "  ')*',\n",
              "  '+',\n",
              "  ',',\n",
              "  '-',\n",
              "  './:',\n",
              "  ';',\n",
              "  '<',\n",
              "  '=>',\n",
              "  '?',\n",
              "  '@',\n",
              "  '[',\n",
              "  '\\\\\\\\',\n",
              "  ']^_`{|}~',\n",
              "  \"'\",\n",
              "  '1. ',\n",
              "  'Word_tokenize',\n",
              "  ' require',\n",
              "  ' punkt',\n",
              "  ' resource',\n",
              "  '.',\n",
              "  '2. punkt',\n",
              "  ' resource',\n",
              "  ' is',\n",
              "  ' different',\n",
              "  ' for',\n",
              "  ' each',\n",
              "  ' ',\n",
              "  'language',\n",
              "  '\\x0c',\n",
              "  'Tokenization',\n",
              "  ' of',\n",
              "  ' ',\n",
              "  'Sentences',\n",
              "  '\\x0cExercise',\n",
              "  ' 1',\n",
              "  ':',\n",
              "  ' Tokenization',\n",
              "  '•',\n",
              "  ' Open',\n",
              "  ' a',\n",
              "  ' new',\n",
              "  ' code',\n",
              "  ' cell',\n",
              "  '.',\n",
              "  '• ',\n",
              "  'Find',\n",
              "  ' some',\n",
              "  ' English',\n",
              "  ' text',\n",
              "  ' and',\n",
              "  ' ',\n",
              "  'copy',\n",
              "  ' to',\n",
              "  ' your',\n",
              "  ' py',\n",
              "  'thon',\n",
              "  ' ',\n",
              "  'notebook',\n",
              "  '.',\n",
              "  '•',\n",
              "  ' Tokenize',\n",
              "  ' this',\n",
              "  ' text',\n",
              "  'Any',\n",
              "  ' ',\n",
              "  'Question',\n",
              "  '?',\n",
              "  '\\x0c',\n",
              "  'Multi-word',\n",
              "  ' ',\n",
              "  'Tokenization',\n",
              "  'Text',\n",
              "  \"Let's\",\n",
              "  ' go',\n",
              "  ' ahead',\n",
              "  ' and',\n",
              "  ' start',\n",
              "  ' now',\n",
              "  '.',\n",
              "  'We',\n",
              "  ' can',\n",
              "  \"'t\",\n",
              "  ' wait',\n",
              "  ' any',\n",
              "  ' longer',\n",
              "  'go',\n",
              "  'ahea',\n",
              "  'd',\n",
              "  'service',\n",
              "  'science',\n",
              "  'Multi-Word Tokenize',\n",
              "  'Token',\n",
              "  '“unique',\n",
              "  ' ',\n",
              "  'meaning',\n",
              "  '”',\n",
              "  'go_ahea',\n",
              "  'd',\n",
              "  'service_science',\n",
              "  '\\x0cMulti-Word Tokenization',\n",
              "  '\\x0c',\n",
              "  'How to',\n",
              "  ' find',\n",
              "  ' out',\n",
              "  ' multi-word',\n",
              "  '?',\n",
              "  '•',\n",
              "  ' Method',\n",
              "  ' ',\n",
              "  '1',\n",
              "  ':',\n",
              "  ' Using',\n",
              "  ' domain',\n",
              "  ' knowledge',\n",
              "  '.',\n",
              "  '•',\n",
              "  ' Method',\n",
              "  ' ',\n",
              "  '2',\n",
              "  ':',\n",
              "  ' Using',\n",
              "  ' ngrams',\n",
              "  ' and',\n",
              "  ' check',\n",
              "  ' the',\n",
              "  ' frequency',\n",
              "  'ngrams',\n",
              "  ',',\n",
              "  ' n=2 ',\n",
              "  '→ bigram',\n",
              "  'Text',\n",
              "  ' = ',\n",
              "  'Service',\n",
              "  ' Science',\n",
              "  ' is',\n",
              "  ' truly',\n",
              "  ' an',\n",
              "  ' interdisciplinary',\n",
              "  ' field',\n",
              "  '.',\n",
              "  '→',\n",
              "  ' Service',\n",
              "  ' Science',\n",
              "  ',',\n",
              "  ' Science',\n",
              "  ' is',\n",
              "  ',',\n",
              "  ' is truly',\n",
              "  ',',\n",
              "  ' truly',\n",
              "  ' an',\n",
              "  ',',\n",
              "  ' an',\n",
              "  ' an',\n",
              "  ' interdisciplinary',\n",
              "  ',',\n",
              "  '…',\n",
              "  '\\x0c2. Stemming',\n",
              "  ' vs',\n",
              "  ' ',\n",
              "  'Lemmatization',\n",
              "  'Text',\n",
              "  'Token',\n",
              "  '“unique',\n",
              "  ' ',\n",
              "  'meaning',\n",
              "  '”',\n",
              "  'jumping',\n",
              "  'jumps',\n",
              "  'Stemming',\n",
              "  'jump',\n",
              "  \"'lying\",\n",
              "  \"'\",\n",
              "  ',',\n",
              "  ' \"v',\n",
              "  '\"',\n",
              "  \"'lying\",\n",
              "  '’',\n",
              "  ',',\n",
              "  ' ”n',\n",
              "  '\"',\n",
              "  'Lemmatization',\n",
              "  'lie',\n",
              "  'lying',\n",
              "  '\\x0c2.1',\n",
              "  ' Stemming',\n",
              "  'Q',\n",
              "  ':',\n",
              "  ' ',\n",
              "  'What',\n",
              "  ' is',\n",
              "  ' ',\n",
              "  'the',\n",
              "  ' difference',\n",
              "  ' between',\n",
              "  ' ',\n",
              "  'The',\n",
              "  ' ',\n",
              "  'Porter',\n",
              "  ' vs',\n",
              "  ' ',\n",
              "  'Lancaster',\n",
              "  ' stemming',\n",
              "  ' ',\n",
              "  'algorithm',\n",
              "  '?',\n",
              "  '\\x0c2.2',\n",
              "  ' ',\n",
              "  'Lemmatization',\n",
              "  '•',\n",
              "  ' Q',\n",
              "  ':',\n",
              "  ' ',\n",
              "  'What',\n",
              "  ' is',\n",
              "  ' ',\n",
              "  'the',\n",
              "  ' difference',\n",
              "  ' between',\n",
              "  ' Stemming',\n",
              "  ' and',\n",
              "  ' ',\n",
              "  ' ',\n",
              "  'Lemmatization',\n",
              "  '?',\n",
              "  '\\x0c3. ',\n",
              "  'Text',\n",
              "  ' ',\n",
              "  'Normalization',\n",
              "  ' with',\n",
              "  ' re',\n",
              "  ' ',\n",
              "  'module',\n",
              "  '•',\n",
              "  ' Document',\n",
              "  ':',\n",
              "  ' https',\n",
              "  ':',\n",
              "  '//docs.py',\n",
              "  'thon.org/3/library/re.html#module-re',\n",
              "  'Replace',\n",
              "  ' the',\n",
              "  ' ',\n",
              "  'word',\n",
              "  ' “went',\n",
              "  '” by',\n",
              "  ' ',\n",
              "  '“go”',\n",
              "  ' ',\n",
              "  'Remove',\n",
              "  ' all',\n",
              "  ' except',\n",
              "  ' alphanumeric',\n",
              "  ' character',\n",
              "  '.',\n",
              "  're.sub',\n",
              "  '(',\n",
              "  ')',\n",
              "  ' ',\n",
              "  ':',\n",
              "  ' replace',\n",
              "  ' string',\n",
              "  '.',\n",
              "  '[',\n",
              "  ' ]',\n",
              "  ':',\n",
              "  ' indicate',\n",
              "  ' a',\n",
              "  ' set',\n",
              "  ' of',\n",
              "  ' character',\n",
              "  '^',\n",
              "  ':',\n",
              "  ' For',\n",
              "  ' example',\n",
              "  ',',\n",
              "  ' [',\n",
              "  '^5',\n",
              "  ']',\n",
              "  ' will',\n",
              "  ' match',\n",
              "  ' any',\n",
              "  ' character',\n",
              "  ' except',\n",
              "  \" '5'\",\n",
              "  ',',\n",
              "  ' ',\n",
              "  '\\\\',\n",
              "  'w',\n",
              "  ':',\n",
              "  ' matches',\n",
              "  ' any',\n",
              "  ' al',\n",
              "  'phanumeric',\n",
              "  ' character',\n",
              "  ';',\n",
              "  ' this',\n",
              "  ' is',\n",
              "  ' equivalent to the',\n",
              "  ' set',\n",
              "  ' [a-zA-Z0-9_',\n",
              "  ']',\n",
              "  '\\x0c3.Text',\n",
              "  ' normalization',\n",
              "  ' and',\n",
              "  ' text',\n",
              "  ' manipulation',\n",
              "  ' ',\n",
              "  '\\x0c4. ',\n",
              "  'Remove',\n",
              "  ' Stopwords',\n",
              "  '•',\n",
              "  ' Stopwor',\n",
              "  'ds',\n",
              "  ' ',\n",
              "  ',',\n",
              "  ' sometimes',\n",
              "  ' written',\n",
              "  ' stop',\n",
              "  ' words',\n",
              "  ' ',\n",
              "  ',',\n",
              "  ' are',\n",
              "  ' words',\n",
              "  ' that',\n",
              "  ' ',\n",
              "  'have',\n",
              "  ' litt',\n",
              "  'le',\n",
              "  ' or no',\n",
              "  ' significance',\n",
              "  '.',\n",
              "  '• ',\n",
              "  'They',\n",
              "  ' are',\n",
              "  ' usually',\n",
              "  ' removed',\n",
              "  ' from',\n",
              "  ' text',\n",
              "  ' during',\n",
              "  ' processing',\n",
              "  ' so as',\n",
              "  ' to',\n",
              "  ' ',\n",
              "  'retain',\n",
              "  ' words',\n",
              "  ' having',\n",
              "  ' maximum',\n",
              "  ' significan',\n",
              "  'ce',\n",
              "  ' and',\n",
              "  ' context',\n",
              "  '.',\n",
              "  '• ',\n",
              "  'There',\n",
              "  ' is',\n",
              "  ' no',\n",
              "  ' universal',\n",
              "  ' or',\n",
              "  ' exhaustive',\n",
              "  ' list ',\n",
              "  'of',\n",
              "  ' stopwords. Each',\n",
              "  ' ',\n",
              "  'domain',\n",
              "  ' or',\n",
              "  ' language',\n",
              "  ' may',\n",
              "  ' have',\n",
              "  ' its',\n",
              "  ' own',\n",
              "  ' set',\n",
              "  ' ',\n",
              "  'of',\n",
              "  ' stop',\n",
              "  'words',\n",
              "  '.',\n",
              "  '\\x0c',\n",
              "  'Define',\n",
              "  ' user',\n",
              "  ' Stop',\n",
              "  'words',\n",
              "  '•',\n",
              "  ' We',\n",
              "  ' define',\n",
              "  ' user',\n",
              "  ' stopwords',\n",
              "  ':',\n",
              "  ' “’s”.',\n",
              "  ',',\n",
              "  ' “ca',\n",
              "  '”',\n",
              "  '•',\n",
              "  ' ',\n",
              "  'Then',\n",
              "  ' remove',\n",
              "  ' the',\n",
              "  ' user',\n",
              "  ' stop',\n",
              "  'words',\n",
              "  '.',\n",
              "  '\\x0c3rd',\n",
              "  ' ',\n",
              "  'Week',\n",
              "  ' Assignment',\n",
              "  '•',\n",
              "  ' Use',\n",
              "  ' the',\n",
              "  ' example',\n",
              "  ' from',\n",
              "  ' ',\n",
              "  'the',\n",
              "  ' class',\n",
              "  ',',\n",
              "  ' try',\n",
              "  ' a',\n",
              "  ' web',\n",
              "  ' site',\n",
              "  ' or',\n",
              "  ' pdf',\n",
              "  ' file',\n",
              "  ' which',\n",
              "  ' ',\n",
              "  'you',\n",
              "  ' have',\n",
              "  ' scraped',\n",
              "  ' it. And',\n",
              "  ' do',\n",
              "  ' ',\n",
              "  'the',\n",
              "  ' same',\n",
              "  ' ',\n",
              "  'analysis',\n",
              "  ' as',\n",
              "  ' it. ',\n",
              "  '(',\n",
              "  'if',\n",
              "  ' yours',\n",
              "  ' ',\n",
              "  'are',\n",
              "  ' in',\n",
              "  ' Mandarin',\n",
              "  ',',\n",
              "  ' you',\n",
              "  ' may',\n",
              "  ' use',\n",
              "  ' exam',\n",
              "  'ple',\n",
              "  ' in',\n",
              "  ' ',\n",
              "  'the',\n",
              "  ' class',\n",
              "  ' but',\n",
              "  ' in',\n",
              "  ' ',\n",
              "  'different',\n",
              "  ' article',\n",
              "  ',',\n",
              "  ' surely',\n",
              "  ',',\n",
              "  ' you',\n",
              "  ' are',\n",
              "  ' welcome',\n",
              "  ' to',\n",
              "  ' try',\n",
              "  ' others)',\n",
              "  '• ',\n",
              "  'W',\n",
              "  'hat',\n",
              "  ' do',\n",
              "  ' you',\n",
              "  ' find',\n",
              "  ' from',\n",
              "  ' ',\n",
              "  'the',\n",
              "  ' result',\n",
              "  '.',\n",
              "  '•',\n",
              "  ' Your',\n",
              "  ' out',\n",
              "  'come',\n",
              "  ':',\n",
              "  '• ',\n",
              "  'Hand',\n",
              "  ' out',\n",
              "  ' the',\n",
              "  ' result',\n",
              "  ' in',\n",
              "  ' doc',\n",
              "  ' or',\n",
              "  ' pdf',\n",
              "  ' format',\n",
              "  ' and',\n",
              "  ' attach',\n",
              "  ' your',\n",
              "  ' program',\n",
              "  ' in',\n",
              "  ' ',\n",
              "  '(',\n",
              "  'ipynb)',\n",
              "  ' format to',\n",
              "  ' ',\n",
              "  '[',\n",
              "  'eeclass]',\n",
              "  ' platform',\n",
              "  '.',\n",
              "  '•',\n",
              "  ' Write',\n",
              "  ' a',\n",
              "  ' report within',\n",
              "  ' one',\n",
              "  ' page',\n",
              "  ',',\n",
              "  ' describing',\n",
              "  ' what',\n",
              "  ' you',\n",
              "  ' find',\n",
              "  '.',\n",
              "  '•',\n",
              "  ' Due',\n",
              "  ' date',\n",
              "  ':',\n",
              "  ' before',\n",
              "  ' the',\n",
              "  ' next',\n",
              "  ' class. ',\n",
              "  '\\x0c'],\n",
              " 'Week5 _ Regular expression - NER ': ['Week',\n",
              "  ' 5',\n",
              "  'Regular',\n",
              "  ' ',\n",
              "  'Expression',\n",
              "  ',',\n",
              "  ' NER',\n",
              "  'Gild Shen',\n",
              "  ',',\n",
              "  ' Viet-Cuong Trieu',\n",
              "  ' ',\n",
              "  '(',\n",
              "  'Daniel)',\n",
              "  ',',\n",
              "  ' Fu-ren',\n",
              "  ' Lin',\n",
              "  'National',\n",
              "  ' Tsing',\n",
              "  ' ',\n",
              "  'Hua',\n",
              "  ' ',\n",
              "  'University',\n",
              "  '\\x0cPython',\n",
              "  ' practice',\n",
              "  ' week',\n",
              "  ' 5',\n",
              "  'https',\n",
              "  ':',\n",
              "  '//bit.ly/3ud',\n",
              "  'KqFl',\n",
              "  '\\x0c1. Regular ',\n",
              "  'Expression',\n",
              "  '“re',\n",
              "  '”',\n",
              "  ' module',\n",
              "  ' ',\n",
              "  '(',\n",
              "  'https',\n",
              "  ':',\n",
              "  '//docs.py',\n",
              "  'thon.org/3/library/re.html#m',\n",
              "  'odule-re)',\n",
              "  '•',\n",
              "  ' re.findall',\n",
              "  '(',\n",
              "  ')',\n",
              "  '•',\n",
              "  ' re.search',\n",
              "  '(',\n",
              "  ')',\n",
              "  '•',\n",
              "  ' re.match',\n",
              "  '(',\n",
              "  ')',\n",
              "  '•',\n",
              "  ' re.sub',\n",
              "  '(',\n",
              "  ')',\n",
              "  '[',\n",
              "  ' ]',\n",
              "  ':',\n",
              "  ' indicate',\n",
              "  ' a',\n",
              "  ' set',\n",
              "  ' of',\n",
              "  ' character',\n",
              "  '^ = start',\n",
              "  ',',\n",
              "  ' $ = end',\n",
              "  ' -- match',\n",
              "  ' the',\n",
              "  ' start',\n",
              "  ' or',\n",
              "  ' end',\n",
              "  ' of',\n",
              "  ' the',\n",
              "  ' string',\n",
              "  '\\\\d -- decimal digit',\n",
              "  ' [0-9',\n",
              "  ']',\n",
              "  '.',\n",
              "  ' ',\n",
              "  ':',\n",
              "  ' any',\n",
              "  ' sing',\n",
              "  'le',\n",
              "  ' character',\n",
              "  '…',\n",
              "  '…',\n",
              "  'Note',\n",
              "  ':',\n",
              "  '•',\n",
              "  ' re.match',\n",
              "  '(',\n",
              "  ')',\n",
              "  ',',\n",
              "  ' re.search',\n",
              "  '(',\n",
              "  ')',\n",
              "  ' only',\n",
              "  ' matches',\n",
              "  ' the',\n",
              "  ' first',\n",
              "  ' occurrence.',\n",
              "  ' ',\n",
              "  '•',\n",
              "  ' re.match',\n",
              "  '(',\n",
              "  ')',\n",
              "  ',',\n",
              "  ' give',\n",
              "  ' the',\n",
              "  ' ',\n",
              "  ' sear',\n",
              "  'ches',\n",
              "  ' only',\n",
              "  ' from',\n",
              "  ' ',\n",
              "  'the',\n",
              "  ' beginning',\n",
              "  ' of',\n",
              "  ' the',\n",
              "  ' string. ',\n",
              "  '•',\n",
              "  ' re.search',\n",
              "  '(',\n",
              "  ')',\n",
              "  ' search',\n",
              "  ' any',\n",
              "  ' location',\n",
              "  '.',\n",
              "  'Goal',\n",
              "  ':',\n",
              "  ' find',\n",
              "  '\"',\n",
              "  ' or',\n",
              "  ' \"find',\n",
              "  ' and',\n",
              "  ' replace',\n",
              "  '\"',\n",
              "  '\\x0c1. Regular ',\n",
              "  'Expression',\n",
              "  'Example',\n",
              "  '1',\n",
              "  ':',\n",
              "  '  ',\n",
              "  'Find',\n",
              "  ' string “grey',\n",
              "  '”',\n",
              "  'Example',\n",
              "  ' 2',\n",
              "  ':',\n",
              "  ' Boole',\n",
              "  'an',\n",
              "  ' \"or',\n",
              "  '”',\n",
              "  ':',\n",
              "  ' A',\n",
              "  ' vertical',\n",
              "  ' bar separates',\n",
              "  ' alternatives. For',\n",
              "  ' ',\n",
              "  'example',\n",
              "  ',',\n",
              "  ' gray',\n",
              "  '|grey',\n",
              "  ' means',\n",
              "  ' \"',\n",
              "  'gray',\n",
              "  '\"',\n",
              "  ' or',\n",
              "  ' \"',\n",
              "  'grey',\n",
              "  '\"',\n",
              "  '.',\n",
              "  '\\x0c1. Regular ',\n",
              "  'Expression',\n",
              "  'Example',\n",
              "  '3',\n",
              "  ':',\n",
              "  '  ',\n",
              "  'The',\n",
              "  ' question',\n",
              "  ' mark',\n",
              "  ' indicates',\n",
              "  ' zero',\n",
              "  ' or',\n",
              "  ' one',\n",
              "  ' occurrences',\n",
              "  ' of',\n",
              "  ' ',\n",
              "  'the',\n",
              "  ' preceding',\n",
              "  ' element. For',\n",
              "  ' example',\n",
              "  ',',\n",
              "  ' colou',\n",
              "  '?',\n",
              "  'r matches',\n",
              "  ' both',\n",
              "  ' \"color',\n",
              "  '\"',\n",
              "  ' ',\n",
              "  'and',\n",
              "  ' \"',\n",
              "  'colour',\n",
              "  '\" ',\n",
              "  '.',\n",
              "  '\\x0c1. Regular ',\n",
              "  'Expression',\n",
              "  'Example',\n",
              "  '4',\n",
              "  ':',\n",
              "  '  ',\n",
              "  'The',\n",
              "  ' asterisk',\n",
              "  ' indicates',\n",
              "  ' zero',\n",
              "  ' or more',\n",
              "  ' occurrences',\n",
              "  ' of',\n",
              "  ' the',\n",
              "  ' ',\n",
              "  'preceding',\n",
              "  ' element. ',\n",
              "  'For',\n",
              "  ' example',\n",
              "  ',',\n",
              "  ' ab*c',\n",
              "  ' matches',\n",
              "  ' \"ac',\n",
              "  '\"',\n",
              "  ',',\n",
              "  ' \"abc',\n",
              "  '\"',\n",
              "  ',',\n",
              "  ' \"abbc',\n",
              "  '\"',\n",
              "  ',',\n",
              "  ' ',\n",
              "  '\"abbbc',\n",
              "  '\"',\n",
              "  ',',\n",
              "  ' and',\n",
              "  ' so',\n",
              "  ' on',\n",
              "  '\"',\n",
              "  'Example',\n",
              "  ' 5',\n",
              "  ':',\n",
              "  'The',\n",
              "  ' plus',\n",
              "  ' ',\n",
              "  '(',\n",
              "  '+) sign',\n",
              "  ' indicates',\n",
              "  ' one',\n",
              "  ' or',\n",
              "  ' more',\n",
              "  ' occurrences',\n",
              "  ' of',\n",
              "  ' the',\n",
              "  ' preceding',\n",
              "  ' ',\n",
              "  'element. ',\n",
              "  'For',\n",
              "  ' example',\n",
              "  ',',\n",
              "  ' ab+c matches',\n",
              "  ' \"abc',\n",
              "  '\"',\n",
              "  ',',\n",
              "  ' \"abbc',\n",
              "  '\"',\n",
              "  ',',\n",
              "  ' \"abbbc',\n",
              "  '\"',\n",
              "  ',',\n",
              "  ' and so',\n",
              "  ' on',\n",
              "  ',',\n",
              "  ' but',\n",
              "  ' not',\n",
              "  ' ',\n",
              "  '\"ac\"',\n",
              "  '.',\n",
              "  '\\x0c1. Regular ',\n",
              "  'Expression',\n",
              "  'Example',\n",
              "  '6',\n",
              "  ':',\n",
              "  '  ',\n",
              "  'The',\n",
              "  ' [a-z]+ means',\n",
              "  ' the',\n",
              "  ' pattern',\n",
              "  ' is',\n",
              "  ' combined',\n",
              "  ' in repeated',\n",
              "  ' ',\n",
              "  'characters',\n",
              "  ' from',\n",
              "  ' “a',\n",
              "  '” to ',\n",
              "  '“z',\n",
              "  '”',\n",
              "  ' more',\n",
              "  ' than',\n",
              "  ' 1',\n",
              "  ' times',\n",
              "  ' but',\n",
              "  ' exclude',\n",
              "  ' of',\n",
              "  ' Arabic',\n",
              "  ' ',\n",
              "  'numerals',\n",
              "  ' ',\n",
              "  '(',\n",
              "  '0',\n",
              "  ',',\n",
              "  '1',\n",
              "  ',',\n",
              "  '…9)',\n",
              "  'Example',\n",
              "  '7',\n",
              "  ':',\n",
              "  '  ',\n",
              "  'Find',\n",
              "  ' any',\n",
              "  ' string',\n",
              "  ' by',\n",
              "  ' the',\n",
              "  ' patte',\n",
              "  'rn',\n",
              "  ' that',\n",
              "  ' begins',\n",
              "  ' with ',\n",
              "  '“dis',\n",
              "  '” and',\n",
              "  ' ',\n",
              "  'ends',\n",
              "  ' in',\n",
              "  ' “tion',\n",
              "  '”',\n",
              "  '.',\n",
              "  '\\x0c1. Regular ',\n",
              "  'Expression',\n",
              "  'Example',\n",
              "  '7',\n",
              "  ':',\n",
              "  '  ',\n",
              "  'Find',\n",
              "  ' any',\n",
              "  ' string',\n",
              "  ' by',\n",
              "  ' the',\n",
              "  ' patte',\n",
              "  'rn',\n",
              "  ' that',\n",
              "  ' begins',\n",
              "  ' with ',\n",
              "  '“go”',\n",
              "  ' and',\n",
              "  ' ends',\n",
              "  ' in',\n",
              "  ' ',\n",
              "  '“d”',\n",
              "  ',',\n",
              "  ' additionally',\n",
              "  ',',\n",
              "  ' with',\n",
              "  ' at',\n",
              "  ' least two',\n",
              "  ' ',\n",
              "  '“o',\n",
              "  '”',\n",
              "  ' and',\n",
              "  ' 5',\n",
              "  ' maximum',\n",
              "  ' ',\n",
              "  '“o',\n",
              "  '”',\n",
              "  ' in',\n",
              "  ' it',\n",
              "  '.',\n",
              "  '\\x0c',\n",
              "  'What',\n",
              "  ' is',\n",
              "  ' this',\n",
              "  ' used',\n",
              "  ' for',\n",
              "  '?',\n",
              "  '\\x0c2. Named-entity',\n",
              "  ' recognition',\n",
              "  ' ',\n",
              "  '(',\n",
              "  'NER)',\n",
              "  'Spacy',\n",
              "  ' Entity',\n",
              "  '\\x0cNER',\n",
              "  'Person',\n",
              "  ' name',\n",
              "  ' Organizations',\n",
              "  ' ',\n",
              "  'Locations',\n",
              "  'Medical',\n",
              "  'codes',\n",
              "  ' ',\n",
              "  'Time',\n",
              "  ' ',\n",
              "  'expressions',\n",
              "  'Quantities',\n",
              "  'Monetary',\n",
              "  'values',\n",
              "  'Percentages',\n",
              "  'Etc',\n",
              "  '.',\n",
              "  '\\x0cNER – English',\n",
              "  ',',\n",
              "  ' spacy',\n",
              "  ' package',\n",
              "  '\\x0cNER – English',\n",
              "  ',',\n",
              "  ' spacy',\n",
              "  ' package',\n",
              "  '\\x0cNER- Chinese- using',\n",
              "  ' spacy',\n",
              "  'You',\n",
              "  ' need',\n",
              "  ' to down',\n",
              "  'load',\n",
              "  ' zh_core_w',\n",
              "  'eb_s',\n",
              "  'm',\n",
              "  ' for spacy',\n",
              "  ' ',\n",
              "  '\\x0cNER- Chinese- Using',\n",
              "  ' Jieba',\n",
              "  '\\x0cNER- Chinese- using',\n",
              "  ' Jieba',\n",
              "  'url',\n",
              "  \" = 'https\",\n",
              "  ':',\n",
              "  '//github.com/fxsjy/jieba/raw/master/extra_dict/dict.txt.big',\n",
              "  \"'\",\n",
              "  'r ',\n",
              "  '= request',\n",
              "  's.get',\n",
              "  '(url',\n",
              "  ',',\n",
              "  ' allow_redirects=',\n",
              "  'True)',\n",
              "  'open',\n",
              "  '(',\n",
              "  \"'dict.txt.big'\",\n",
              "  ',',\n",
              "  \" 'wb')\",\n",
              "  '.write',\n",
              "  '(',\n",
              "  'r',\n",
              "  '.content)',\n",
              "  'jieba.set_dictionary',\n",
              "  '(',\n",
              "  \"'dict.txt.big')\",\n",
              "  'kw1',\n",
              "  '=jieba.analyse.extract_tags',\n",
              "  '(',\n",
              "  'chinese_text',\n",
              "  ',',\n",
              "  'top',\n",
              "  'K=20',\n",
              "  ',',\n",
              "  'with',\n",
              "  'Weight',\n",
              "  '=True',\n",
              "  ',',\n",
              "  'allo',\n",
              "  'wPOS',\n",
              "  '=',\n",
              "  '(',\n",
              "  \"'n'\",\n",
              "  ',',\n",
              "  \"'ns')\",\n",
              "  ')',\n",
              "  'for',\n",
              "  ' item',\n",
              "  ' in',\n",
              "  ' kw1',\n",
              "  ':',\n",
              "  'print',\n",
              "  '(',\n",
              "  '\"extract_tags',\n",
              "  ':',\n",
              "  ' \"',\n",
              "  ',',\n",
              "  'item)',\n",
              "  '\\x0cNER- Chinese- Using CKIP',\n",
              "  'Trans',\n",
              "  'former',\n",
              "  '\\x0cCkip',\n",
              "  'Tagger',\n",
              "  '• This',\n",
              "  ' open-source',\n",
              "  ' library',\n",
              "  ' implements',\n",
              "  ' neural',\n",
              "  ' CKIP-style',\n",
              "  ' ',\n",
              "  'Chinese',\n",
              "  ' ',\n",
              "  'NLP tools',\n",
              "  '.',\n",
              "  '•',\n",
              "  ' ',\n",
              "  '(',\n",
              "  'WS)',\n",
              "  ' word segmentation',\n",
              "  '•',\n",
              "  ' ',\n",
              "  '(',\n",
              "  'POS)',\n",
              "  ' part-of-speech',\n",
              "  ' tagging',\n",
              "  '•',\n",
              "  ' ',\n",
              "  '(',\n",
              "  'NER)',\n",
              "  ' named',\n",
              "  ' entity',\n",
              "  ' recognition',\n",
              "  'https',\n",
              "  ':',\n",
              "  '//github.com/ckiplab/ckiptagger',\n",
              "  '/',\n",
              "  '\\x0cNER',\n",
              "  ' result',\n",
              "  '\\x0cCKIP ',\n",
              "  'Trans',\n",
              "  'formers',\n",
              "  '•',\n",
              "  ' BERT Based',\n",
              "  '• This',\n",
              "  ' project',\n",
              "  ' provides',\n",
              "  ' traditional',\n",
              "  ' ',\n",
              "  'Chinese',\n",
              "  ' transformers',\n",
              "  ' models',\n",
              "  ' ',\n",
              "  '(',\n",
              "  'including',\n",
              "  ' ALBERT',\n",
              "  ',',\n",
              "  ' BERT',\n",
              "  ',',\n",
              "  ' GPT2)',\n",
              "  ' and',\n",
              "  ' NLP tools',\n",
              "  ' ',\n",
              "  '(',\n",
              "  'including',\n",
              "  ' ',\n",
              "  'word',\n",
              "  ' segmentation',\n",
              "  ',',\n",
              "  ' part-of-spee',\n",
              "  'ch',\n",
              "  ' tagging',\n",
              "  ',',\n",
              "  ' named',\n",
              "  ' entity',\n",
              "  ' ',\n",
              "  'recognition)',\n",
              "  '.',\n",
              "  '•',\n",
              "  ' ',\n",
              "  '這',\n",
              "  '個',\n",
              "  '專案',\n",
              "  '提供',\n",
              "  '了',\n",
              "  '繁體',\n",
              "  '中文',\n",
              "  '的',\n",
              "  ' transformers',\n",
              "  ' ',\n",
              "  '模型',\n",
              "  '（',\n",
              "  '包含',\n",
              "  ' ALBERT',\n",
              "  '、',\n",
              "  'BERT',\n",
              "  '、',\n",
              "  'GPT2',\n",
              "  '）',\n",
              "  '及',\n",
              "  '自然',\n",
              "  '語言',\n",
              "  '處理',\n",
              "  '工具',\n",
              "  '（',\n",
              "  '包含',\n",
              "  '斷詞',\n",
              "  '、',\n",
              "  '詞性',\n",
              "  '標記',\n",
              "  '、',\n",
              "  '實體',\n",
              "  '辨識',\n",
              "  '）',\n",
              "  '。',\n",
              "  '\\x0cNER ',\n",
              "  'Result',\n",
              "  '\\x0chttps',\n",
              "  ':',\n",
              "  '//hugging',\n",
              "  'face.co',\n",
              "  '/',\n",
              "  '\\x0c',\n",
              "  'Feature',\n",
              "  ' ',\n",
              "  'Generation',\n",
              "  'What',\n",
              "  ' sorts',\n",
              "  ' of',\n",
              "  ' features',\n",
              "  ' would',\n",
              "  ' we',\n",
              "  ' generate',\n",
              "  '?',\n",
              "  ' ',\n",
              "  '•',\n",
              "  ' Tokens',\n",
              "  ' ',\n",
              "  '(',\n",
              "  'bag',\n",
              "  ' ',\n",
              "  'of',\n",
              "  ' words)',\n",
              "  '•',\n",
              "  ' Multiwords',\n",
              "  '•',\n",
              "  ' Features',\n",
              "  ' by',\n",
              "  ' POS',\n",
              "  ' tags',\n",
              "  '•',\n",
              "  ' Name',\n",
              "  ' entities',\n",
              "  ',',\n",
              "  ' such',\n",
              "  ' as',\n",
              "  ' doctor',\n",
              "  ',',\n",
              "  ' president',\n",
              "  ',',\n",
              "  ' professor',\n",
              "  ',',\n",
              "  ' ',\n",
              "  '…',\n",
              "  'Remove',\n",
              "  ' insignificant tokens',\n",
              "  '•',\n",
              "  ' Remove',\n",
              "  ' stopwords',\n",
              "  '• ',\n",
              "  'Remove',\n",
              "  ' tokens',\n",
              "  ' based',\n",
              "  ' on term',\n",
              "  ' ',\n",
              "  'frequency/document',\n",
              "  ' frequency',\n",
              "  '•',\n",
              "  ' ',\n",
              "  'Remove',\n",
              "  ' tokens',\n",
              "  ' based',\n",
              "  ' on',\n",
              "  ' domain',\n",
              "  ' knowledge',\n",
              "  '\\x0cHW5',\n",
              "  ':',\n",
              "  ' ',\n",
              "  '•',\n",
              "  ' Finish',\n",
              "  ' 15',\n",
              "  ' steps',\n",
              "  ' of',\n",
              "  ' website',\n",
              "  ' on',\n",
              "  ' https',\n",
              "  ':',\n",
              "  '//regexone.com',\n",
              "  '/',\n",
              "  '•',\n",
              "  ' And …',\n",
              "  '• ',\n",
              "  'Capture',\n",
              "  ' the',\n",
              "  ' last',\n",
              "  ' question',\n",
              "  ' with',\n",
              "  ' your',\n",
              "  ' answer',\n",
              "  ' and',\n",
              "  ' time',\n",
              "  ' in',\n",
              "  ' an',\n",
              "  ' image',\n",
              "  ' ',\n",
              "  '(',\n",
              "  'check',\n",
              "  ' the',\n",
              "  ' example)',\n",
              "  '•',\n",
              "  ' Write',\n",
              "  ' down',\n",
              "  ' every',\n",
              "  ' answer',\n",
              "  ' of',\n",
              "  ' ',\n",
              "  '15',\n",
              "  ' steps',\n",
              "  '.',\n",
              "  '•',\n",
              "  ' ',\n",
              "  'Put',\n",
              "  ' all',\n",
              "  ' these',\n",
              "  ' two',\n",
              "  ' results',\n",
              "  ' in',\n",
              "  ' your',\n",
              "  ' submission',\n",
              "  '.',\n",
              "  '•',\n",
              "  ' ',\n",
              "  'You',\n",
              "  ' may',\n",
              "  ' want to',\n",
              "  ' use',\n",
              "  ' https',\n",
              "  ':',\n",
              "  '//regex',\n",
              "  '101.com/ to',\n",
              "  ' rein',\n",
              "  'force',\n",
              "  ' your',\n",
              "  ' ',\n",
              "  'learning',\n",
              "  ' in',\n",
              "  ' ',\n",
              "  'Regular',\n",
              "  ' ',\n",
              "  'Expression',\n",
              "  '\\x0c\\x0cNote',\n",
              "  '• ',\n",
              "  'We',\n",
              "  ' will',\n",
              "  ' have',\n",
              "  ' a',\n",
              "  ' quiz',\n",
              "  ' of',\n",
              "  ' regular',\n",
              "  ' expression',\n",
              "  ' next',\n",
              "  ' week',\n",
              "  '.',\n",
              "  '• ',\n",
              "  'Don',\n",
              "  '’t',\n",
              "  ' forget',\n",
              "  ' to',\n",
              "  ' bring',\n",
              "  ' your',\n",
              "  ' laptop',\n",
              "  ' or',\n",
              "  ' pad',\n",
              "  ',',\n",
              "  ' it will',\n",
              "  ' be',\n",
              "  ' inconvenient',\n",
              "  ' if',\n",
              "  ' ',\n",
              "  'you',\n",
              "  ' do',\n",
              "  ' it',\n",
              "  ' on',\n",
              "  ' cellphone',\n",
              "  '.',\n",
              "  '\\x0c']}"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4zzB2oCb5H3a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "33b46c32-9fcc-44b6-b5a6-6fea45ee2766"
      },
      "source": [
        "#create bag words\n",
        "bag_words_2 =[] # declare bag_words is a list\n",
        "for doc in doc_all_2.keys():\n",
        "  bag_words_2 += doc_all_2[doc]\n",
        "bag_words_2\n",
        "\n",
        "print(bag_words_2)\n",
        "\n",
        "bag_words_2=set(bag_words_2)\n",
        "\n",
        "\n",
        "#calculate idf for every word in bag_words\n",
        "bag_words_idf_2={} # declare \"bag_words_idf\" data structure is dictionary \n",
        "for word in bag_words_2:\n",
        "  bag_words_idf_2[word]= idf(word,doc_all_2.values())\n",
        "\n",
        "##calculate tfidf with cosine normalization\n",
        "tfidf_2={} # declare tfidf dictionary to store tfidf value\n",
        "for doc in doc_all_2.keys():\n",
        "  tfidf_2[doc]= compute_tfidf(doc_all_2[doc],bag_words_idf_2)"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['From', ' ', 'Textual', ' ', 'Information', ' to', ' ', 'Numerical', ' ', 'Vector', 'with ', 'Py', 'thon', 'Fu-ren', ' Lin', ',', ' Gild', ' Shen', ',', ' Viet-Cuong Trieu', ' ', '(', 'Daniel)', ' ', 'National', ' Tsing', ' ', 'Hua', ' ', 'University', '\\x0cOutlines', '1. Tokenization', ' ', '(', 'word', ',', ' multi-word', ',', ' sentence', ')', '2. Stemming', ' vs', ' ', 'Lemmatization', '3. Text', ' ', 'Normalization', '4. Stopwords', ' & Build', ' stopwords', '5. ', 'Term', ' frequency', ' ', '(', 'TF)', ' vectorizer', '\\x0c', 'If', ' y', 'ou', ' have', ' any', ' question', ',', ' please', ' ask me', ' ', 'in', ' the', ' slido', ' at slido.com', ' with', ' ', '#809100', '\\x0c', '• Working', ' with', ' “Natural', ' ', 'Language', ' ', 'Toolkit', '” https', ':', '//www.nltk.org', '/', '•', ' Online', ' book', ':', ' http', ':', '//www.nltk.org/book', '/', '1. Tokenization', '2. Stemming', '3. Lemmatization', 'Bird', ',', ' Steven', ',', ' Edward ', 'Loper', ' and', ' Ewan', ' ', 'Klein', ' ', '(', '2009)', ',', ' Natural', ' Language', ' ', 'Processing', ' with', ' ', 'Python. O’', 'Reilly', ' ', 'Media', ' ', 'Inc', '.', '\\x0cUsing', ' ', 'Google', ' ', 'Colab', 'Click', ' on', ' this', ' link to', ' open', ' TA', ' sharing', ':', 'https', ':', '//bit.ly/3vwjDGi', 'Then', ' click', ' on “', 'Copy', ' do ', 'Drive', '” to', ' save', ' to', ' your', ' ', 'google', ' drive', '\\x0cWord', ' ', 'Tokenization', 'Word', ' tokenization', ' is', ' the', ' process', ' of', ' splitting', ' a', ' large', ' sample', ' of', ' text', ' into', ' words. ', 'This', ' is', ' a', ' requirement', ' in', ' natural', ' language', ' processing', ' tasks', ' where', ' ', 'ea', 'ch', ' word', ' needs', ' to', ' be', ' captured', ' and', ' subjected', ' ', 'to', ' further', ' analysis', ' like', ' classifying', ' ', 'and', ' counting', ' them', ' ', 'for a', ' particular', ' sentiment', ' etc', 'Text', \"Let's\", ' go', ' ahead', ' and', ' start', ' now. We', ' can', \"'t\", ' wait any', ' longer', 'Tokenization', '(', 'Splitting)', ' ', '“Blank', ' ', 'Space', '”', 'splitting', 'We', 'can', '’t', 'wait', '“', 'Punctuation', ' ', 'Marks', '”', 'splitting', '“Dictionary', ' ', 'base', '”', 'splitting', 'We', 'can', '’', 't', 'wait', 'We', 'ca', 'n’t', 'wait', '\\x0c2. Word', ' ', 'Tokenization', '“Blank', ' ', 'Space', '”', 'splitting', '“', 'Punctuation', ' ', 'Marks', '”', 'splitting', '“Dictionary', ' ', 'base', '”', 'splitting', '\\x0cword_tokenize', ' function', 'word', 'punct_tokenize', ' separate', ' word', ' based', ' ', 'on', ' punctuation', ' ', 'Marks', ':', \"'\", '!', '\"#$%&\\\\\\'', '(', ')*', '+', ',', '-', './:', ';', '<', '=>', '?', '@', '[', '\\\\\\\\', ']^_`{|}~', \"'\", '1. ', 'Word_tokenize', ' require', ' punkt', ' resource', '.', '2. punkt', ' resource', ' is', ' different', ' for', ' each', ' ', 'language', '\\x0c', 'Tokenization', ' of', ' ', 'Sentences', '\\x0cExercise', ' 1', ':', ' Tokenization', '•', ' Open', ' a', ' new', ' code', ' cell', '.', '• ', 'Find', ' some', ' English', ' text', ' and', ' ', 'copy', ' to', ' your', ' py', 'thon', ' ', 'notebook', '.', '•', ' Tokenize', ' this', ' text', 'Any', ' ', 'Question', '?', '\\x0c', 'Multi-word', ' ', 'Tokenization', 'Text', \"Let's\", ' go', ' ahead', ' and', ' start', ' now', '.', 'We', ' can', \"'t\", ' wait', ' any', ' longer', 'go', 'ahea', 'd', 'service', 'science', 'Multi-Word Tokenize', 'Token', '“unique', ' ', 'meaning', '”', 'go_ahea', 'd', 'service_science', '\\x0cMulti-Word Tokenization', '\\x0c', 'How to', ' find', ' out', ' multi-word', '?', '•', ' Method', ' ', '1', ':', ' Using', ' domain', ' knowledge', '.', '•', ' Method', ' ', '2', ':', ' Using', ' ngrams', ' and', ' check', ' the', ' frequency', 'ngrams', ',', ' n=2 ', '→ bigram', 'Text', ' = ', 'Service', ' Science', ' is', ' truly', ' an', ' interdisciplinary', ' field', '.', '→', ' Service', ' Science', ',', ' Science', ' is', ',', ' is truly', ',', ' truly', ' an', ',', ' an', ' an', ' interdisciplinary', ',', '…', '\\x0c2. Stemming', ' vs', ' ', 'Lemmatization', 'Text', 'Token', '“unique', ' ', 'meaning', '”', 'jumping', 'jumps', 'Stemming', 'jump', \"'lying\", \"'\", ',', ' \"v', '\"', \"'lying\", '’', ',', ' ”n', '\"', 'Lemmatization', 'lie', 'lying', '\\x0c2.1', ' Stemming', 'Q', ':', ' ', 'What', ' is', ' ', 'the', ' difference', ' between', ' ', 'The', ' ', 'Porter', ' vs', ' ', 'Lancaster', ' stemming', ' ', 'algorithm', '?', '\\x0c2.2', ' ', 'Lemmatization', '•', ' Q', ':', ' ', 'What', ' is', ' ', 'the', ' difference', ' between', ' Stemming', ' and', ' ', ' ', 'Lemmatization', '?', '\\x0c3. ', 'Text', ' ', 'Normalization', ' with', ' re', ' ', 'module', '•', ' Document', ':', ' https', ':', '//docs.py', 'thon.org/3/library/re.html#module-re', 'Replace', ' the', ' ', 'word', ' “went', '” by', ' ', '“go”', ' ', 'Remove', ' all', ' except', ' alphanumeric', ' character', '.', 're.sub', '(', ')', ' ', ':', ' replace', ' string', '.', '[', ' ]', ':', ' indicate', ' a', ' set', ' of', ' character', '^', ':', ' For', ' example', ',', ' [', '^5', ']', ' will', ' match', ' any', ' character', ' except', \" '5'\", ',', ' ', '\\\\', 'w', ':', ' matches', ' any', ' al', 'phanumeric', ' character', ';', ' this', ' is', ' equivalent to the', ' set', ' [a-zA-Z0-9_', ']', '\\x0c3.Text', ' normalization', ' and', ' text', ' manipulation', ' ', '\\x0c4. ', 'Remove', ' Stopwords', '•', ' Stopwor', 'ds', ' ', ',', ' sometimes', ' written', ' stop', ' words', ' ', ',', ' are', ' words', ' that', ' ', 'have', ' litt', 'le', ' or no', ' significance', '.', '• ', 'They', ' are', ' usually', ' removed', ' from', ' text', ' during', ' processing', ' so as', ' to', ' ', 'retain', ' words', ' having', ' maximum', ' significan', 'ce', ' and', ' context', '.', '• ', 'There', ' is', ' no', ' universal', ' or', ' exhaustive', ' list ', 'of', ' stopwords. Each', ' ', 'domain', ' or', ' language', ' may', ' have', ' its', ' own', ' set', ' ', 'of', ' stop', 'words', '.', '\\x0c', 'Define', ' user', ' Stop', 'words', '•', ' We', ' define', ' user', ' stopwords', ':', ' “’s”.', ',', ' “ca', '”', '•', ' ', 'Then', ' remove', ' the', ' user', ' stop', 'words', '.', '\\x0c3rd', ' ', 'Week', ' Assignment', '•', ' Use', ' the', ' example', ' from', ' ', 'the', ' class', ',', ' try', ' a', ' web', ' site', ' or', ' pdf', ' file', ' which', ' ', 'you', ' have', ' scraped', ' it. And', ' do', ' ', 'the', ' same', ' ', 'analysis', ' as', ' it. ', '(', 'if', ' yours', ' ', 'are', ' in', ' Mandarin', ',', ' you', ' may', ' use', ' exam', 'ple', ' in', ' ', 'the', ' class', ' but', ' in', ' ', 'different', ' article', ',', ' surely', ',', ' you', ' are', ' welcome', ' to', ' try', ' others)', '• ', 'W', 'hat', ' do', ' you', ' find', ' from', ' ', 'the', ' result', '.', '•', ' Your', ' out', 'come', ':', '• ', 'Hand', ' out', ' the', ' result', ' in', ' doc', ' or', ' pdf', ' format', ' and', ' attach', ' your', ' program', ' in', ' ', '(', 'ipynb)', ' format to', ' ', '[', 'eeclass]', ' platform', '.', '•', ' Write', ' a', ' report within', ' one', ' page', ',', ' describing', ' what', ' you', ' find', '.', '•', ' Due', ' date', ':', ' before', ' the', ' next', ' class. ', '\\x0c', 'Retrieval', ' ', 'Model', ':', ' Vector', ' ', 'Space', 'Fu-ren', ' Lin', 'Institute', ' ', 'of', ' ', 'Service', ' ', 'Science', 'National', ' Tsing', ' ', 'Hua', ' ', 'University', '\\x0c', 'The', ' Basic', ' ', 'Question', 'n ', 'Given', ' a', ' query', ',', ' how', ' do', ' we', ' know', ' if', ' document', ' ', 'A is', ' more', ' relevant', ' than', ' B', '?', 'n', ' A possible', ' answer', ':', 'q', ' ', 'If', ' document', ' A', ' uses', ' more', ' query', ' words', ' than', ' ', 'document', ' B ', '(', 'Word', ' usage', ' in', ' document', ' A', ' is', ' more', ' ', 'similar to', ' that', ' in', ' query', ')', '\\x0c', 'Relevance', ' = Similarity', 'n Assumptions', 'q ', 'Query', ' and document', ' are', ' represented', ' similarly', 'q A', ' query', ' can', ' be', ' regarded', ' as', ' a ', '“do', 'cument', '”', 'q ', 'Rele', 'vance', '(', 'd', ',', 'q)', ' µ similarity', '(', 'd', ',', 'q', ')', 'n R', '(', 'q)', ' = {dÎC|f', '(', 'd', ',', 'q)', '>', 'q}', ',', ' f', '(', 'q', ',', 'd)', '=D', '(', 'Rep', '(', 'q)', ',', ' Rep', '(', 'd)', ')', 'n ', 'Key', ' issues', 'q ', 'How', ' to', ' represent', ' query/document', '?', 'q', ' ', 'How', ' to', ' define', ' the', ' similarity', ' measure', ' D', '?', '\\x0c', 'Word', ' ', 'count', ' profile', 'n A', ' ', 'do', 'cument', ' word', ' profile', ' tells', ' us', ' a', ' lot', ' about', ' ', 'what', ' ', 'the', ' document', ' might', ' be', ' about', '.', ' ', 'n A', ' query', ' word', ' profile', ' tells', ' us', ' a', ' lot', ' about', ' what', ' ', 'information', ' the', ' user', ' might', ' want', 'n So', ',', ' perhaps', ' we', ' could', ' represent', ' a ', 'do', 'cument/query', ' by', ' a', ' word', ' count', ' vector', '?', '\\x0c', 'Vector', ' ', 'Space', ' Model', 'n ', 'Represent a doc/query', ' by', ' a term', ' vector', 'q Term', ':', ' basic', ' concept', ',', ' e.g.', ',', ' word', ' or', ' phrase', 'q ', 'Each term', ' defines', ' one', ' dimension', 'q N terms', ' define', ' a', ' high-dimensional', ' space', 'q ', 'Element', ' of', ' vector', ' corres', 'ponds', ' to', ' term', ' weight', 'q e.g', '.', ',', ' d', '=(x1', ',', '…', ',', 'xN)', ',', ' xi', ' is', ' ', '“importance', '”', ' of', ' term', ' i', 'n', ' ', 'Measure', ' relevance', ' by', ' the', ' distance', ' between', ' ', 'the', ' query', ' vector', ' and', ' document', ' vector', ' in', ' ', 'the', ' ', 'vector', ' space', '5', '\\x0cVS ', 'Model', ':', ' illustration', '?', ' ', '?', 'D3', 'D10', 'D7', 'D8', 'Micros', 'oft', 'Starbucks', 'D9', 'D11', 'D2 ', '?', ' ', '?', 'D5', 'D4', 'D6', 'Query', 'D1', '?', ' ', '?', 'Java', '\\x0cWhat', ' the', ' VS ', 'model', ' doesn', '’t', ' say', 'n', ' ', 'How to', ' define/select the', ' “basic', ' concept', '”', 'q ', 'Concepts', ' are', ' assumed', ' to', ' be', ' or', 'thogonal', 'n ', 'How', ' to', ' assign', ' weights', 'q ', 'Weight', ' in', ' query', ' indicates', ' importance', ' of', ' term', 'q ', 'Weight', ' in doc', ' indicates', ' how well', ' ', 'the', ' term', ' ', 'characterizes', ' the', ' doc', 'n', ' ', 'How to', ' define', ' the', ' similarity/distance', ' measure', '?', '\\x0cWhat', '’s', ' a', ' good', ' ', '“basic', ' concept', '”', '?', 'n Orthogonal', 'q ', 'Linearly', ' independent', ' basis', ' vectors', 'q “', 'Non-over', 'lapping” in ', 'meaning', 'n No', ' ambiguity', 'n', ' ', 'Weights', ' can', ' be', ' assigned', ' automatically', ' and', ' ', 'hope', 'fully', ' accurately', 'n', ' ', 'Many', ' possibilities', ':', ' Words', ',', ' stemmed', ' words', ',', ' ', 'phrases', ',', ' “latent', ' concept', '”', ',', ' ', '…', '\\x0c', 'How to Assign ', 'Weights', '?', 'n ', 'Very', ' very', ' important', '!', 'n ', 'Why', ' weighting', 'q ', 'Query', ' side', ':', ' ', 'Not', ' all', ' terms', ' are', ' equally', ' important', 'q ', 'Doc side', ':', ' Some', ' terms', ' carry', ' more', ' contents', 'n How', '?', ' ', 'q ', 'Two', ' basic', ' heuristics', 'n TF ', '(', 'Term Frequency)', ' = Within-doc-frequency', 'n', ' IDF ', '(', 'Inverse', ' ', 'Document', ' ', 'Frequency', ')', 'q ', 'Document', ' leng', 'th', ' normalization', ' ', '\\x0cTF ', 'Weighting', 'n Idea', ':', ' A', ' term', ' is', ' more', ' important', ' if', ' it', ' occurs', ' ', 'more', ' frequent', 'ly', ' in', ' a', ' document', 'n ', 'Formulas', ':', ' Let', ' f', '(', 't', ',', 'd)', ' be the', ' frequency', ' count', ' of', ' ', 'term', ' t', ' in', ' doc', ' d', 'q Raw', ' TF', ':', '  TF', '(', 't', ',', 'd)', ' = f', '(', 't', ',', 'd', ')', 'q ', 'Maximum', ' frequen', 'cy', ' normalization', ':', ' ', 'TF', '(', 't', ',', 'd)', ' = 0.5 +0.5*f', '(', 't', ',', 'd)/Max', 'Freq', '(', 'd)', 'q “Okapi/BM25 TF”', ':', ' ', 'TF', '(', 't', ',', 'd)', '=kf', '(', 't', ',', 'd)', '/', '(', 'f', '(', 't', ',', 'd)', '+k', '(', '1-b+b*doclen/avg', 'doclen)', ')', 'n', ' ', 'Normalization', ' of', ' T', 'F', ' is', ' very', ' important', '!', '\\x0c', 'Normalization', ' techniques', 'n', ' Cosine', ' normalization', 'n', ' ', 'Maximum', ' tf', ' normalization', 'q ', 'The', ' ', 'S', 'mart', ' system', '’s', ' augmented', ' tf', ' factor', 'n ', 'Byte', ' leng', 'th', ' normalization', 'q ', 'Okapi', ' system', '\\x0cTF ', 'Normalization', 'n Why', '?', ' ', 'q ', 'Document', ' leng', 'th', ' variation', 'q', ' “', 'Repeated', ' occurrences', '” are', ' less', ' informative', ' than', ' ', 'the', ' ', '“first', ' occurrence', '”', 'n', ' ', 'Two', ' views', ' ', 'of', ' document', ' length', 'q A', ' doc', ' is', ' long', ' because', ' it', ' uses', ' more', ' words', 'q A', ' doc', ' is', ' long', ' because', ' it', ' has', ' more', ' contents', 'n', ' ', 'Generally', ' penalize', ' long', ' doc', ',', ' but', ' avoid', ' over', '-', 'penalizing', ' ', '(', 'pivoted', ' normalization)', '\\x0c', 'Pivoted', ' Normalization', 'P', '(', 'retrieval)', '>', 'P', '(', 'relevance)', ' is', ' increased', 'P', '(', 'retrieval)', '<', 'P', '(', 'relevance)', ' is', ' decreased', '\\x0c', 'Pivoted', ' Normalization', ' ', '(', 'cont.)', 'n', ' ', 'Pivoted', ' normalization', ' ', '= ', '(', '1.0-slope)', 'Îpivot', ' + slope', 'Îold', ' normalization', '(', 'Snghal', ',', ' et al', '.', ',', ' 1995', ';', ' ', '1996', ')', '\\x0cIDF ', 'Weighting', 'n Idea', ':', ' A', ' term', ' is', ' more', ' discriminative', ' if', ' it', ' occurs', ' ', 'only', ' in', ' fewer', ' documents', 'n ', 'Formula', ':', 'IDF', '(', 't) = 1+ log', '(', 'N/n)', 'N', ':', ' total', ' number', ' of', ' docs', 'n', ':', ' # docs', ' with term', ' t', ' ', '(', 'doc', ' freq)', '\\x0cTF-IDF Weighting', 'n TF-IDF weighting', ' ', ':', ' ', 'weight', '(', 't', ',', 'd)', '=TF', '(', 't', ',', 'd', ')', '*IDF', '(', 't', ')', 'q ', 'Freq', ' in', ' doc', ' à', ' high', ' tf', ' à', ' high', ' weight', 'q ', 'Rare', ' in', ' collection', 'à', ' high', ' idfà', ' high', ' ', 'weight', 'n', ' ', 'I', 'magine', ' a word', ' count', ' profile', ',', ' what', ' kind', ' of', ' ', 'terms', ' would', ' have', ' high', ' weights', '?', ' ', 'n ', 'How', ' is', ' this related', ' to ', 'Luhn', '’s', ' study', ' ', 'of', ' term', ' ', 'frequen', 'cy', ' and', ' ', 'Zipf', '’s', ' law', '?', '\\x0c', 'Luhn', '’s study', ' of', ' ', ' term', ' frequency', '\\x0cThree', ' ', 'Term-weighting', ' ', 'Components', 'Type ', 'I', 'Term', ' ', 'frequency', ' ', 'component', 'Type', ' II', 'Document', ' ', 'frequency', ' ', 'component', 'b', ',', ' 1.0', 't', ',', ' tf', 'Binary', ' weight equals', ' to', ' 1', ' for terms', ' present', ' in', ' a', ' vector', ' ', '(', 'term', ' ', 'frequen', 'cy', ' is', ' ignored', ')', 'Term', ' frequency', ' ', '(', 'number', ' of', ' times', ' a', ' term', ' occurs', ' in', ' a', ' document', ' or ', 'query', ' text)', '.', '.', 'n', ',', ' 0.5+0.5', '(', 'tf', ' /max', ' tf', ')', 'Augmented', ' normalized', ' term', ' frequency', ' ', '(', 'tf)', ' factor', ' normalized', ' by', ' ', 'maximum', ' ', 'tf', ' in', ' the', ' vector', ',', ' and', ' further', ' normalized', ' to', ' lie', ' between', ' ', '0.5', ' and', ' 1.0.', 'x', ',', ' 1.0', 'No', ' change', ' in', ' weights', ';', ' use', ' original', ' term', ' frequency', ' component', '.', 'f', ',', ' ', '(', 'log ', '(', 'N/n)', ')', 'Multiply', ' original', ' tf', ' factor', ' by', ' an', ' inverse', ' document', ' frequen', 'cy', ' factor', ' ', '(', 'idf)', ' ', '(', 'N is', ' total', ' number of', ' documents', ' in', ' collection', ',', ' and', ' n is', ' ', 'number', ' ', 'of', ' documents', ' to which', ' a', ' term', ' is', ' assigned)', '.', 'Type III', 'Normalization', ' ', 'component', 'p', ',', '  log', ' ', '[', '(', 'N-n)/n', ']', 'Multiply', ' tf', ' factor', ' by', ' a', ' probabilistic', ' inverse', ' collection', ' frequency', ' ', 'factor', '.', 'x', ',', ' 1.0', 'No', ' change', ';', ' use', ' factors', ' derived', ' from', ' term', ' frequen', 'cy', ' and', ' document', ' ', 'frequen', 'cy', ' ', '(', 'idf)', ' only ', '(', 'no', ' normalization)', '.', 'c', ',', ' 1/ ', '(', 'åwi', '2)', ' 1/2', 'Use', ' cosine', ' normalization', ' where', ' each', ' term', ' weight', ' w', ' is', ' divided', ' by', ' a ', 'factor', ' representing ', 'Euclidian', ' vector', ' leng', 'th', '.', '\\x0cTerm-Weight Systems', ' ', 'Weighting', ' System', 'Document', ' ', 'Term', ' ', 'Weight', 'Best', ' fully', ' ', 'Weighted', ' system', 'tfc', 't', 'f', '*', '(', 'log', 'N', 'n', ')', 'Best', ' weighted', ' probabilistic', ' weight', 'nxx', 'Classical', ' idf', ' weight', 'bf', 'x', 'Binary', ' term', ' independence', 'bxx', 'Standard', ' tf', ' weight', 'txc', 'Coordination', ' level', 'bxx', 'å *', '[', 't', 'f', 'i', 'vector', '2)', ']', '(', 'log', 'N', 'n', 'i', '5', '.0', 't', 'f', 'max', 't', 'f', '5.0 ', '+', 'lo', 'g', 'N', 'n', '1', 't', 'f', '(', ')', 'å', 'it', 'f', '2', 'vector', '1', '\\x0cTerm-Weight Systems', ' ', '(', 'cont.)', 'Weighting', ' System', 'Query', ' ', 'Term', ' ', 'Weight', 'Best', ' fully', ' ', 'Weighted', ' system', 'nfx', 'Best', ' weighted', ' probabilistic', ' weight', 'bpx', 'Classical', ' idf', ' weight', 'bf', 'x', 'Binary', ' term', ' independence', 'bpx', 'Standard', ' tf', ' weight', 'txx', 'Coordination', ' level', 'bxx', '5.0', '(', '+', '5', '.0', 't', 'f', 'max', 't', 'f', ')', '•', 'lo', 'g', 'N', 'n', 'lo', 'g', 'nN ', '-', 'n', 'lo', 'g', 'N', 'n', 'lo', 'g', 'nN ', '-', 'n', 't', 'f', '1', '\\x0c', 'How to ', 'Measure', ' ', 'Similarity', '?', '(', '=', '!', 'D', 'i', '!', 'w', 'Q', '=', '(', 'w', 'i', 'q', '1', ',', '..', '.', ',', '1', ',', '..', '.', ',', 'w', 'i', 'N', ')', 'w', 'q', 'N', ')', 'w', '=', 'term', ' a', ' if', ' 0', ' is', ' ', 'absent', ' ', 'Dot', ' ', 'product', ' ', 'similarity', '!', '!', ',', 'DQsim', '(', 'i', ')', '=', ':', 'Cosine', ':', '!', '!', ',', 'DQsim', '(', 'i', ')', '=', '(', '=', 'normalize', 'd', 'dot', ' ', 'product', ')', 'N', 'å', 'j', '=', '1', 'w', 'qj', '*', 'w', 'ij', 'N', 'å', 'j', '=', '1', 'w', 'qj', '*', 'w', 'ij', 'N', 'å', 'j', '=', '1', '(', 'w', 'qj', '2', ')', '*', 'N', 'å', 'j', '=', '1', '(', 'w', 'ij', '2', ')', ' ', '    ', ' ', ' ', '         ', ' ', '         ', ' ', '         ', ' ', ' ', '     ', '     ', '         ', ' ', '\\x0cVS ', 'Example', ':', ' Raw TF & Dot', ' P', 'roduct', ' ', 'doc1', 'information', ' ', 'retrie', 'val', 'search', 'engine', 'information', 'travel', 'information', ' ', 'Sim', '(', 'q', ',', 'doc1', ')', '=4.8*2.4+4.5*4.5', 'query', '=“information', ' retrieval', '”', 'Sim', '(', 'q', ',', 'doc2', ')', '=', '2.4*2.4', 'doc', '2', 'map', 'travel', 'Sim', '(', 'q', ',', 'doc3)', '=0', 'doc3', 'government', ' ', 'president', 'congress', '…', '…', 'info     retrieval', '    travel', ' ', ' map', '   search ', ' ', ' engine', ' ', ' govern', ' ', ' president', '   congress', 'IDF', '(', 'faked)', ' ', '2.4', '         4.5', '2.8        3.3', '     2.1', '         5.4', '         2.2        3.2           ', '4.3', 'doc1', 'doc', '2', 'doc3', '2', '(', '4.8)', '        1', '(', '4.5', ')', '1', '(', '2.4 )                     2 ', '(', '5.6)', '   1', '(', '3.3)', '         ', '1', '(', '2.1', ')', '      1', '(', '5.4', ')', '1 ', '(', '2.2)   1', '(', '3.2)', '      1', '(', '4.3', ')', 'query', '1', '(', '2.4', ')', '1', '(', '4.5', ')', '\\x0cWhat', ' ', 'Works', ' the', ' Best', '?', '•', 'Use single', ' words', ' ', '•', 'Use', ' stat. phrases', '•', 'Remove', ' stop', ' words', '•', 'Stemming', '•', 'Others', '(', '?', ')', '(', 'Singhal', ' 2001', ')', '\\x0c', 'Relevance', ' ', 'Feedback', 'Query', 'Updated', 'query', 'Retrie', 'val', 'Engine', 'Document', 'collection', 'Feedback', 'Results', ':', 'd1 3.5', 'd2 2.4', '…', 'dk 0.5', '.', '.', '.', 'Judgments', ':', 'd1 ', '+', 'd2 ', '-', 'd3 ', '+', '…', 'dk ', '-', '.', '.', '.', 'User', '\\x0c', 'Relevance', ' ', 'Feedback', 'Query', ' ', 'String', 'Revise', 'd', 'Query', 'Document', 'corpus', 'Rankings', 'I', 'R', 'System', 'Re', 'Ranked', 'Document', 's', 'Query', 'Reformulation', 'Ranked', 'Document', 's', 'Feedback', '1. Doc1', '  ', 'ß', '2. Doc2  Ý', '3. Doc3  ', 'ß', '.', '.', '1. Doc2', ' ', '2. Doc4 ', '3. Doc5', ' ', '.', '.', '1. Doc1', ' ', '2. Doc2', ' ', '3. Doc3', ' ', '.', '.', '\\x0cPseudo', ' ', 'Feedback', 'n', ' ', 'Use', ' relevance', ' feedback', ' methods', ' without', ' ', 'explicit', ' user', ' input', '.', 'n', ' ', 'Just', ' assume', ' the', ' top m', ' retrieved', ' documents', ' ', 'are', ' relevant', ',', ' and', ' use', ' them', ' to', ' reformulate', ' the', ' ', 'query', '.', 'n ', 'Allows', ' for', ' query', ' expansion', ' ', 'that', ' includes', ' ', 'terms', ' that are', ' correlated', ' with', ' ', 'the', ' query', ' terms', '.', '\\x0cPseudo/Blind/Automatic', ' ', 'Feedback', 'Query', 'Updated', 'query', 'Retrie', 'val', 'Engine', 'Document', 'collection', 'Feedback', 'Results', ':', 'd1 3.5', 'd2 2.4', '…', 'dk 0.5', '.', '.', '.', 'Judgments', ':', 'd1 ', '+', 'd2 ', '+', 'd3 ', '+', '…', 'dk ', '-', '.', '.', '.', 'top', ' 10', '\\x0cPseudo', ' feed', 'back', '\\x0c', 'Relevance', ' ', 'Feedback', ' in', ' V', 'S', 'n Basic', ' setting', ':', ' ', 'Learn', ' from', ' examples', 'q ', 'Positive', ' examples', ':', ' docs', ' known', ' to', ' be', ' relevant', 'q ', 'Negative', ' examples', ':', ' docs', ' known', ' to', ' be', ' non-relevant', 'q ', 'How', ' do', ' you learn', ' from', ' ', 'this', ' to', ' improve', ' per', 'form', 'ance', '?', 'n', ' ', 'General', ' method', ':', ' ', 'Query', ' modification', 'q ', 'Adding', ' new', ' ', '(', 'weighted)', ' terms', 'q ', 'Adjusting', ' weights', ' of', ' old', ' terms', 'q ', 'Doing', ' both', 'n', ' ', 'The', ' most', ' well-known', ' and', ' effective', ' approach', ' is', ' ', 'Rocchio', '\\x0c', 'Rocchio', ' ', 'Feedback', ':', ' Illustration', '+', 'q+ +', '-', '+', '+', '---', '+', '+', '+', '+', '+', '-', '-', '++', '+', '-', '+ ', '+', '+ + +', '-', '---', '- --', '-', '-', '---', 'q', '+', '---', '-- --', '-', '\\x0c', 'Rocchio', ' ', 'Feedback', ':', ' Formula', 'New query', 'Parameters', 'Original', ' query', 'Rel docs', 'Non-rel docs', '\\x0c', 'Relevance', ' feed', 'back', '\\x0c', 'Learning', ' classification', ' algorithm', '\\x0c', 'Rocchio', ' in Practice', 'n', ' ', 'Negative', ' ', '(', 'non-relevant)', ' examples', ' are', ' not', ' very', ' ', 'important', ' ', '(', 'why', '?', ')', 'n', ' ', 'Often', ' project', ' the', ' vector', ' onto', ' a', ' lower', ' dimension', ' ', '(', 'i.e.', ',', ' ', 'consider', ' only', ' a small', ' number', ' ', 'of', ' words', ' that have', ' ', 'high weights in the', ' centroid', ' vector)', ' ', '(', 'efficiency', ' ', 'concern', ')', 'n', ' ', 'Avoid', ' “training', ' bias', '”', ' ', '(keep', ' relatively', ' high', ' weight', ' on', ' ', 'the', ' original', ' query', ' weights)', ' ', '(', 'why', '?', ')', 'n', ' ', 'Can', ' be', ' used', ' for', ' relevance', ' feed', 'back', ' and', ' pseudo', ' ', 'feedback', 'n ', 'Usually', ' robust', ' and', ' effective', '\\x0c', '(', 'cid', ':', '1)', 'Extension', '” ', 'of', '  VS', ' ', 'Model', 'n ', 'Alternative', ' similarity', ' measures', ' ', 'q ', 'Many', ' other', ' choices', ' ', '(', 'tend', ' not to', ' be', ' very', ' effective', ')', 'q', ' p-norm', ' ', '(', 'Extended', ' Boolean)', 'n ', 'Alternative', ' representation', 'q ', 'Many', ' choices', ' ', '(', 'performance', ' varies', ' a lot', ')', 'q ', 'Latent', ' ', 'Semantic', ' ', 'Indexing', ' ', '(', 'LSI)', ' ', '[TREC ', 'performance', ' tends', ' to', ' be', ' average', ']', 'n', ' ', 'Generalized', ' vector', ' space', ' model', 'q ', 'Theoretically', ' interesting', ',', ' not', ' serious', 'ly', ' evaluated', '\\x0cAdvantages', ' of', '  VS ', 'Model', 'n ', 'Empirica', 'l', 'ly', ' effective', '!', ' ', '(', 'Top', ' TREC', ' per', 'formance)', 'n Intuitive', 'n', ' Easy', ' to', ' implement', 'n ', 'Well-studied/Most', ' evaluated', 'n ', 'The', ' ', 'S', 'mart', ' system', 'q ', 'Deve', 'loped', ' at ', 'Cornell', ':', ' 1960-1999', 'q Still', ' widely', ' used', ' ', 'n', ' ', 'Warning', ':', ' Many', ' many', ' variants', ' of', ' TF-IDF', '!', '\\x0cDisadvantages', ' of', '  VS ', 'Model', 'n', ' Assume', ' term', ' independence', 'n', ' Assume', ' query', ' and', ' document', ' can', ' ', ' be', ' the', ' ', 'same', 'n ', 'Lack', ' ', 'of', ' “predictive', ' adequacy', '”', 'q ', 'Arbitrary', ' term', ' weighting', 'q ', 'Arbitrary', ' similarity', ' mea', 'sure', 'n ', 'Lots', ' ', 'of', ' parameter', ' tuning', '!', '\\x0cSummary', 'n ', 'Vector Space', ' ', 'Model', ' is', ' a FAMILY', ' of', ' heuristic', ' ', 'retrieval models', 'n TF-IDF+Pivoted', ' ', 'Length ', 'Norm tends', ' to', ' give', ' ', 'very', ' good', ' per', 'formance', ' ', '(', 'Underst', 'and', ' the', ' ', 'details', ' of', ' the', ' formula)', 'n', ' ', 'Rocchio', ' feed', 'back', ' is', ' robust', ' and', ' effective', ' for', ' ', 'TF-IDF weighting', 'n ', 'The', ' VS ', 'model', ' has', ' been', ' used', ' in', ' many', ' ', 'applications', '\\x0c', 'IR Basics', 'Fu-ren', ' Lin', 'Institute', ' ', 'of', ' ', 'Service', ' ', 'Science', 'National', ' Tsing', ' ', 'Hua', ' ', 'University', '\\x0cOutline', 'n ', 'Problem', ' formulation', 'n ', 'Retrieval', ' models', 'n ', 'Evaluat', 'ion', 'n', ' ', 'Common', ' IR ', 'components', '2019/3/11', '2', '\\x0cP', 'roblem', ' Formulation', ':', ' Infor', 'mal', 'n', ' ', 'User', ' has', ' an', ' information', ' need', 'q ', 'Short-term', ' ', '(', '“ad', ' hoc”)', ',', ' e.g.', ',', ' ', '“price', ' of', ' iPhone', '(', 'cid', ':', '1', ')', 'q ', 'Long-term', ',', ' e.g.', ',', ' “technology', ' roadmap', '(', 'cid', ':', '1', ')', 'n ', 'There', ' exists an', ' information', ' source', 'q ', 'Relative', 'ly', ' static', ',', ' e.g.', ',', ' library', ' system', 'q ', 'Inherent', 'ly dynamic', ',', ' e.g.', ',', ' news', ' articles', 'n', ' ', 'Goal', ' is', ' to', ' find', ' information', ' items', ' ', 'that', ' can', ' ', 'satis', 'fy', ' a', ' user', '’s', ' information', ' need', '2019/3/11', '3', '\\x0cUser task', 'n ', 'Retrie', 'val', 'q A', ' query', ' expression', ' is', ' used', ' to', ' convey', ' the', ' ', 'constraints', ' that', ' must', ' be', ' satisfied', ' by', ' objects', ' in', ' the', ' ', 'answer set', '.', 'q purpose', 'ful', 'n Browsing', 'q ', 'Glancing', ' around', 'q', ' ', 'Hypertext', ' systems', ' are', ' usually', ' tuned', ' for providing', ' ', 'quick', ' browsing', '.', '2019/3/11', '4', '\\x0c', 'Logical', ' view', ' of', '  ', 'the', ' documents', 'Document', 's', 'Accents', 'spacing', 'Stopwords', 'Noun', 'groups', 'stemming', 'Automatic', 'or', ' manual', ' ', 'indexing', 'Structure', 'recognition', 'structure', 'Full', '  text', 'Index', ' terms', '2019/3/11', '5', '\\x0cText', 'The', ' ', 'Retrieval', ' Process', 'User', 'Inter', 'face', '4', ',', ' 10', 'user', ' need', 'Text   ', 'Operations', 'logical', ' vie', 'w', 'logical', ' vie', 'w', 'Indexing', 'inverted', ' file', 'Index', 'user', ' feedback', 'Query', ' ', 'Operations', 'query', 'Searching', '5', '8', 'retrieved', ' docs', 'ranked', ' docs', ' ', 'Ranking', '2', '2019/3/11', 'Text', '6', ',', ' 7', '8', 'DB', ' manager', ' ', 'module', 'Text', ' ', 'database', '6', '\\x0cAd', ' hoc', ' ', 'Retrieval', ' vs', '.', ' ', 'Information', ' ', 'Filtering', 'n Ad hoc', ' retrieval', 'q ', 'Short-term', ' information', ' need', ' + static', ' source', ' ', 'q ', 'User', ' “pulls', '”', ' information', ',', ' e.g.', ',', ' web search', 'n', ' ', 'Information', ' filtering', '(', 'routing)', ' ', 'q ', 'Persistent', ' information', ' need', ' + dynamic', ' source', 'q ', 'System', ' “pushes', '”', ' information', ' to', ' user', ',', ' e.g.', ',', ' news', ' ', 'filter', 'q', ' ', 'Traditional', 'ly', ' callled', ' “', 'Selective', ' Dissemination', ' of', ' ', 'Information', '”', ',', ' or', ' SDI', '2019/3/11', '7', '\\x0cAd hoc', ' retrieval', 'Q1', 'Q2', 'Q3', 'Collection', '“Fixed', ' Size', '”', 'Q4', 'Q5', '2019/3/11', '8', '\\x0cFiltering', 'User  2', 'Profile', 'User', '  1', 'Profile', 'Docs Filtered', 'for', ' User', ' 2', 'Docs', ' for', 'User', ' 1', '2019/3/11', '9', 'Documents', '  Stream', '\\x0cImportance', ' ', 'of', '  ', 'Ad', ' hoc', ' ', 'Retrieval', 'n', ' ', 'Directly', ' manages', ' any', ' existing', ' large', ' collection', ' ', 'of', ' information', 'n', ' ', 'There', ' are', ' many', ' many', ' ', '“ad', ' hoc', '”', ' information', ' ', 'needs.', ' ', 'n', ' A', ' long-term', ' information', ' need', ' can', ' be', ' satisfied', ' ', 'through', ' frequent', ' ad', ' hoc', ' retrieval', 'n', ' ', 'Basic', ' techniques', ' of', ' ad', ' hoc', ' retrieval', ' can', ' be', ' ', 'used', ' for filtering', ' and', ' other “non-retrieval', '”', 'tasks', ',', ' such', ' as', ' automatic', ' summarization', '.', ' ', '2019/3/11', '10', '\\x0c', 'The', ' ', 'Ad', ' hoc', ' ', 'Retrieval', ' ', 'Problem', 'n ', 'Query', ':', ' Description of', ' information', ' need', ',', ' e.g.', ',', 'q ', 'Boole', 'an', ':', ' “iPhone', '(', 'cid', ':', '1)', ' AND ', '(', '“cheap', '” OR “sale”)', 'q ', 'English', ':', ' “cheap', ' iPhone', ' on', ' sale', '”', 'n ', 'Document', ':', ' ', 'Information', ' item', ',', ' e.g.', ',', ' ', 'q ', 'Textual', ' ', '(', 'possibly', ' with', ' structural', ' information', ')', 'q ', 'Multi-media', 'n Database/Collection/Corpus', ':', ' ', 'q a set', ' of', ' documents', 'n ', 'Retrie', 'val', ' task', ':', ' ', 'q', ' find', ' documents', ' relevant', ' to', ' a', ' query', ' in', ' one', ' or', ' more', ' collections', '2019/3/11', '11', '\\x0c', 'Information retrieval', 'Index', ' terms', 'doc', 'Information', ' ', 'Need', 'Match', 'Ranking', 'query', '2019/3/11', '12', '\\x0c', 'Formal', ' ', 'Formulation', ' of', '  IR', 'n', ' ', 'Vocabulary', ' V', '={w1', ',', ' w', '2', ',', ' …', ',', ' wN} ', 'of', ' language', 'n ', 'Query', ' q = q1', ',', '…', ',', 'qm', ',', ' where', ' qi', ' Î ', 'V', 'n', ' ', 'Document', ' di = di1', ',', '…', ',', 'dimi', ',', ' where', ' dij', ' Î ', 'V', 'n', ' Collection', ' C', '= {d1', ',', ' …', ',', ' dk}', 'n', ' ', 'Set', ' ', 'of', ' relevant', ' documents', ' R', '(', 'q) Í C', 'q ', 'Generally', ' unknown', ' and', ' user-dependent', 'q ', 'Query', ' is', ' a “hint”', ' on', ' which document', ' is', ' in R', '(', 'q', ')', 'n', ' Task', ' =  com', 'pute', ' R’', '(', 'q)', ',', ' an', ' approximate', ' R', '(', 'q', ')', '2019/3/11', '13', '\\x0cComputing', ' R', '(', 'q)', 'n Strategy', ' 1', ':', ' ', 'Document', ' selection', 'q R', '(', 'q)', '={dÎC|f', '(', 'd', ',', 'q)', '=', '1}', ',', ' where', ' f', '(', 'd', ',', 'q)', ' ', 'Î{0', ',', '1} is', ' an', ' ', 'indicator', ' function', ' or classifier', 'q ', 'System', ' must decide', ' if', ' a', ' doc', ' is', ' relevant', ' or not', ' ', '(', '“absolute', ' relevance', '”)', 'n Strategy', ' 2', ':', ' ', 'Document', ' ranking', 'q R', '(', 'q)', ' = {dÎC|f', '(', 'd', ',', 'q)', '>', 'q}', ',', ' where', ' f', '(', 'd', ',', 'q)', ' ÎÂ is', ' a ', 'relevance', ' measure', ' function', ';', ' q is', ' a', ' cut', 'of', 'f', 'q', ' ', 'System', ' must decide', ' if', ' one', ' doc', ' is', ' more', ' likely', ' to', ' be', ' ', 'relevant', ' than', ' another', ' ', '(', '“relative', ' relevance', '”', ')', '2019/3/11', '14', '\\x0c', 'Document', ' ', 'Selection', ' vs. Ranking', 'True', ' R', '(', 'q)', '-', '-', '+ ', '+', '-', '+', '-', '-', '-', '+', '+', '-', '-', '-', '-', '-', '-', '-', '-', '- -', 'Doc ', 'Selection', 'f', '(', 'd', ',', 'q)', '=', '?', '1', '0', 'Doc', ' Ranking', 'f', '(', 'd', ',', 'q)', '=', '?', '2019/3/11', '-', '-', '+', '+', '+', '+', '+', 'R’', '(', 'q)', '-', '-', '-', '-', '+', '-', '-', '-', '-', '-', '0.98', ' d1 ', '+', '0.95', ' d2 ', '+', '0.83', ' d3 -', '0.80', ' d4 ', '+', '0.76', ' d5 ', '-', '0.56', ' d6 ', '-', '0.34', ' d7 -', '0.21', ' d8', ' ', '+', '0.21', ' d9 -', 'R’', '(', 'q)', '15', '\\x0c', 'Problems', ' ', 'of', '  ', 'Doc', ' ', 'Selection', 'n ', 'The', ' classifier', ' is', ' unlikely', ' accurate', 'q “', 'Over-constrained', '” query', ' ', '(', 'terms', ' are', ' too', ' specific)', ':', ' ', 'no', ' relevant documents', ' found', 'q “', 'Under-constrained', '” query', ' ', '(', 'terms', ' are', ' too', ' general)', ':', ' ', 'over', ' delivery', 'q', ' ', 'It', ' is', ' extremely', ' hard', ' to', ' find', ' the', ' right', ' position', ' ', 'between', ' these', ' two', ' extremes', 'n ', 'Even', ' if', ' it', ' is', ' accurate', ',', '  all', ' relevant documents', ' ', 'are', ' not', ' equally', ' relevant', 'n', ' ', 'Relevance', ' is a', ' matter', ' of', ' degree', '!', '2019/3/11', '16', '\\x0cRanking', 'n', ' ', 'A', ' ranking', ' is an', ' ordering', ' of', ' ', 'the', ' documents', ' retrieved', ' ', 'that', ' ', '(', 'hopefully)', ' reflects', ' the', ' relevance', ' of', ' ', 'the', ' ', 'documents', ' to', ' the', ' ', ' user', ' query', ' ', 'n A', ' ranking', ' is', ' based', ' on', ' fundamental', ' premisses', ' ', 'regarding', ' the', ' notion', ' of', ' relevance', ',', ' such', ' as', ':', 'q', ' common', ' sets', ' of', ' index', ' terms', 'q sharing', ' of', ' weighted', ' terms', 'q', ' likelihood', ' of', ' relevance', 'n', ' ', 'Each', ' set', ' ', 'of', ' premisses', ' leads', ' to', ' a', ' distinct', ' IR', ' model', '2019/3/11', '17', '\\x0c', 'Ranking', ' is often', ' preferred', 'n', ' ', 'Relevance', ' is a', ' matter', ' of', ' degree', 'n A', ' user', ' can', ' stop', ' browsing', ' any', 'where', ',', ' which', ' ', 'means', ' that', ' the', ' ', 'boundary', ' is', ' controlled', ' by', ' ', 'the', ' ', 'user', 'q ', 'High', ' recall', ' users', ' would', ' view', ' more', ' items', 'q ', 'High', ' precision', ' users', ' would', ' view', ' only', ' a', ' few', 'n ', 'Theoretical', ' justification', ':', ' Probability', ' Ranking', ' ', 'Principle', ' [Robertson', ' 77', ']', '2019/3/11', '18', '\\x0cProbability Ranking', ' Principle', '(', 'PRP)', ' [', 'Robertson', ' 77', ']', 'n As', ' stated', ' by', ' ', 'Cooper', '(', 'cid', ':', '1)', 'If a', ' reference', ' retrieval', ' system', '’s', ' res', 'ponse', ' to', ' each', ' request', ' is', ' a ranking', ' of', ' ', 'the', ' documents', ' in', ' the', ' collections', ' in', ' order', ' ', 'of', ' decre', 'asing', ' probability', ' of', ' ', 'usefulness', ' to', ' the', ' user', ' who', ' submitted', ' the', ' ', 'request', ',', ' where', ' ', 'the', ' probabilities', ' are', ' ', 'estimated', ' as accurately', ' as', ' possible', ' on', ' ', 'the', ' basis', ' of', ' whatever', ' data', ' made', ' ', 'available', ' to the', ' system', ' for', ' this', ' purpose', ',', ' then', ' the', ' overall', ' effectiveness', ' of', ' the', ' ', 'system', ' to', ' its', ' users', ' ', ' will', ' be', ' ', 'the', ' best', ' ', 'that', ' is', ' obtainable', ' on', ' the', ' basis', ' of', ' that', ' ', 'data.', '”', 'n', ' ', 'Robertson', ' provides', ' two', ' formal', ' justifications', 'n Assumptions', ':', ' ', 'Independent relevance', ' and', ' ', 'se', 'quential', ' browsing', '2019/3/11', '19', '\\x0c', 'Relevance', ' measure', ' function', ' f', 'n', ' ', 'According', ' to', ' the', ' PRP', ',', ' all', ' we', ' need', ' is ', ' ', 'A', ' relevance', ' measure', ' function', ' f', 'which', ' satis', 'fies', 'for', ' all', ' q', ',', ' d', '1', ',', ' d2', ',', ' ', ' ', 'f', '(', 'q', ',', 'd1)', ' >', ' f', '(', 'q', ',', 'd', '2)', ' iff  p', '(', 'Rel|q', ',', 'd1)', ' >', 'p', '(', 'Rel|q', ',', 'd', '2', ')', '2019/3/11', '20', '\\x0cMap', ' of', '  IR ', 'Models', 'Relevance', 'D', '(', 'Rep', '(', 'q)', ',', ' Rep', '(', 'd))', '    ', 'Similarity', 'P', '(', 'r', '=1|q', ',', 'd)', '   r Î{0', ',', '1}', 'Probability', ' of', ' ', 'Relevance', 'P', '(', 'd ®q)', ' or P', '(', 'q ®d', ')', 'Probabilistic', ' inference', 'Different', ' ', 'rep', ' &  similarity', '…', 'Regression', 'Model', '(', 'Fox', ' 83', ')', 'Generative', 'Model', 'Doc', 'generation', 'Query', 'generation', 'Vector', ' space', 'model', '(', 'Salton', ' et', ' al', '.', ',', ' 75)', 'Prob. distr', '.', 'model', '(', 'Wong & Yao', ',', ' 89', ')', 'Classical', 'prob. Model', '(', 'Robert', 'son', ' & ', 'Sparck', ' ', 'Jones', ',', ' 76', ')', 'L', 'M', 'approach', '(', 'Ponte & Croft', ',', ' 98)', '(', 'Lafferty', ' & Zhai', ',', ' 01a)', 'Different', 'inference', ' system', 'Prob. concept', 'space', ' model', '(', 'Wong & Yao', ',', ' 95)', 'Inference', ' ', 'network', 'model', '(', 'Turtle & Croft', ',', ' 91)', '2019/3/11', '21', '\\x0cIR  ', 'Models', 'U', 's', 'e', 'r', ' ', 'T', 'a', 's', 'k', 'Retrie', 'val', ':', ' ', 'Adhoc', 'Filtering', 'Browsing', 'Classic', ' ', 'Models', 'boolean', 'vector', 'probabilistic', 'Set', ' ', 'Theoretic', 'Fuzzy', 'Extended', ' ', 'Boolean', 'Algebraic', 'Generalized', ' ', 'Vector', 'Lat. ', 'Semantic', ' ', 'Index', 'Neural ', 'Networks', 'Structured', ' ', 'Models', 'Probabilistic', 'Non-Overlapping Lists', 'Proximal', ' ', 'Nodes', 'Inference', ' ', 'Network', ' ', 'Belief', ' Network', 'Browsing', 'Flat', 'Structure', ' ', 'Guided', 'Hypertext', '2019/3/11', '22', '\\x0cIR  ', 'Models', 'LOGICAL   VIEW   OF   DOCUMENTS', 'Index', '  ', 'Terms', 'Full ', 'Text', 'Retrie', 'val', 'Classic', 'Set', ' ', 'Theoretic', 'Algebraic', 'Probabilistic', 'Classic', 'Set', ' ', 'Theoretic', 'Algebraic', 'Probabilistic', 'Full ', 'Text', ' ', '+', 'Structure', 'Structured', 'Browsing', 'Flat', 'Flat', 'Hypertext', 'Structure', ' ', 'Guided', 'Hypertext', 'U', 'S', 'E', 'R', 'T', 'A', 'S', 'K', '2019/3/11', '23', '\\x0cIR System', ' Evaluation', 'n System-based', ' evaluation', ':', 'q', ' how', ' good', ' are', ' document', ' rankings', '?', 'n', ' User-based', ' evaluation', ':', 'q', ' how', ' satisfied', ' is', ' the', ' user', '?', '2019/3/11', '24', '\\x0c', 'Evaluation', ' Criteria', 'n', ' ', 'Effectiveness', 'q ', 'Precision', ',', ' Recall', 'n', ' ', 'Efficiency', 'q ', 'Space', ' and', ' time', ' complexity', ' ', 'n Usability', 'q ', 'How', ' use', 'ful', ' for', ' real', ' user', ' tasks', '?', '2019/3/11', '25', '\\x0c', 'Cranfield', ' Tradition', 'n', ' ', 'Laboratory', ' testing', ' of', ' retrieval', ' systems', ' first', ' ', 'done', ' in Cranfield', ' II', ' experiment', ' ', '(', '1963', ')', 'q', ' fixed', ' document', ' and', ' query', ' sets', 'q', ' evaluation', ' based', ' on', ' relevance', ' judgments', 'q', ' relevance', ' abstracted', ' to topical', ' similarity', 'n', ' ', 'Laboratory', ' tests', ' less', ' expensive', 'n', ' ', 'Laboratory', ' tests', ' more', ' diagnostic', 'n ', 'Laboratory', ' tests', ' are', ' effective', '2019/3/11', '26', '\\x0cMethodology', ':', ' ', 'Cranfield', ' Tradition', 'n', ' ', 'Laboratory', ' testing', ' of', ' system components', 'q ', 'Precision', ',', ' Recall', 'q ', 'Comparative', ' testing', 'n Test', ' collections', 'q ', 'Set', ' of', ' documents', 'q ', 'Set', ' of', ' questions', 'q ', 'Relevance', ' judgments', '2019/3/11', '27', '\\x0c', 'Cranfield', ' Tradition', ' Assumptions', 'n', ' ', 'Relevance', ' can', ' be', ' approximated', ' by', ' topical', ' ', 'similarity', 'q', ' relevance', ' of', ' one', ' doc', ' is', ' independent', ' ', 'of', ' others', 'q', ' all', ' relevant documents', ' equally', ' desirable', 'q', ' user', ' information', ' need', ' doesn', '’t', ' change', 'n', ' ', 'Single', ' set', ' of', ' judgments', ' is', ' representative', ' of', ' ', 'user', ' populat', 'ion', 'n', ' ', 'Complete', ' judgments', ' ', '(', 'i.e.', ',', ' recall', ' is', ' kno', 'wable)', '2019/3/11', '28', '\\x0cThe', ' Case', ' ', 'Against', ' the', ' ', 'Cranfield', ' ', 'Tradition', 'n', ' ', 'Relevance', ' judgments', ' ', 'q', ' vary', ' too', ' mu', 'ch to', ' be', ' ', 'the', ' basis', ' of', ' evaluation', 'q', ' topical', ' similarity', ' is', ' not', ' utility', 'q static', ' set', ' of', ' judgments', ' cannot', ' reflect', ' user', '’s', ' ', 'changing', ' information', ' need', 'n', ' ', 'Results', ' on', ' test', ' collections', ' are', ' not', ' ', 'representative', ' of', ' operational', ' retrieval', ' ', 'sy', 'stems', '2019/3/11', '29', '\\x0cResponse', ' to Criticism', 'n', ' ', 'Goal', ' in Cranfield', ' tradition', ' is', ' to', ' compare', ' systems', 'q', ' gives', ' relative', ' scores', ' of', ' evaluation', ' measures', ',', ' not', ' absolute', ' ', 'q', ' differences', ' in relevance', ' judgments', ' matter', ' only', ' if', ' relative', ' ', 'measures', ' based', ' on', ' those', ' judgments', ' change', 'n', ' ', 'Cranfield', ' tests', ' used', ' small', ' collections', ' and', ' assessed', ' ', 'relevance', ' for whole', ' collections', ' ', 'n TREC', ',', ' NTCIR', ' and', ' CLEF', ' have', ' very', ' big', ' collections', ' ', '-', 'thus', ' adopt', ' pooling', ' metho', 'dology', '2019/3/11', '30', '\\x0cUsing', ' ', 'Pooling', ' ', 'Methodology', ' to ', 'Create', ' ', 'Large', ' ', 'Test', ' Collections', '2019/3/11', '31', '\\x0cMain', ' IR ', 'Evaluation', ' Programs', 'n TREC', ':', ' ', 'q ', 'Text', ' REtrieval', ' ', 'Conference', ',', ' cosponsored', ' by', ' NIST', ' ', 'and', ' ARDA', 'n NTCIR', ':', ' ', 'q', ' ', 'Evaluation of', ' ', 'Information', ' Access', ' ', 'Technologies', ':', ' IR', ',', ' ', 'QA', ' and', ' C-L Information', ' Access', ',', ' NII', ',', ' Tokyo', 'n CLEF', ':', ' ', 'q ', 'Cross', ' Language', ' ', 'Evaluation ', 'Forum', ' - C-L ', 'evaluation', ' for multilingual', ' IR systems', ' operating', ' ', 'on', ' ', 'Europe', 'an', ' languages', ',', ' sponsored', ' by', ' DELO', 'S', '2019/3/11', '32', '\\x0c', 'The', ' ', 'Contingen', 'cy', ' ', 'Table', 'Action', 'Doc', 'Retrie', 've', 'd', 'Not', ' ', 'Retrie', 've', 'd', 'Relevant', 'Relevant', ' Retrieve', 'd', 'Relevant', ' ', 'Rejected', 'Not', ' relevant', 'Irrelevant', ' ', 'Retrieved', ' ', 'Irrelevant ', 'Rejected', 'Precision', '=', 'Recall', '=', 'Relevant', ' ', 'Retrie', 've', 'd', 'Retrie', 've', 'd', 'Relevant', ' ', 'Retrie', 've', 'd', 'Relevant', '2019/3/11', '33', '\\x0c', 'How to', ' measure', ' a', ' ranking', '?', 'n ', 'Compute', ' ', 'the', ' precision', ' at', ' every', ' recall', ' point', 'n ', 'Plot', ' a', ' precision-recall', ' ', '(', 'PR) curve', 'precision', 'x', 'x', 'precision', 'x', 'Which', ' is', ' better', '?', 'x', 'x', 'x', 're', 'call', 'x', 'x', 're', 'call', '2019/3/11', '34', '\\x0c', 'Summarize', ' a Ranking', 'n', ' ', 'Compute', ' ', 'the', ' precision', ' for retrieving', ' each', ' ', 'relevant', ' document', 'n', ' ', 'If a', ' relevant document', ' never', ' gets', ' retrieved', ',', ' ', 'precision', '=0', 'n ', 'Compute', ' the', ' average', ' over', ' all the', ' relevant', ' ', 'documents. ', 'n ', 'Average', ' precision', ' captures', ' both', ' precision', ' ', 'and', ' recall', '2019/3/11', '35', '\\x0cPR ', 'Curve', 'Recall', 'Avg. Precision', '2019/3/11', '36', '\\x0c', 'What', ' ', 'Query', ' ', 'Averaging', ' ', 'Hides', 'n', 'o', 'i', 's', 'i', 'c', 'e', 'r', 'P', '1', '0.9', '0.8', '0.7', '0.6', '0.5', '0.4', '0.3', '0.2', '0.1', '0', '0', '0.1', '0.2', '0.3', '0.4', '0.5', '0.6', '0.7', '0.8', '0.9', '1', 'Recall', '2019/3/11', '37', '\\x0cNext ', '(', '& Last)', ' ', 'Topic', 'n ', 'Problem', ' formulation', ' ', 'n ', 'Retrieval', ' models', 'n ', 'Evaluat', 'ion', 'n', ' ', 'Common', ' IR ', 'Components', '2019/3/11', '38', '\\x0c', 'Arabic', ' ', 'Retrie', 'val', 'Doc1', ':', 'ﯾَﺠِﺐُ ﻋَﻠَﻰ اﻹﻧْﺴَﺎ', 'نِ أ', 'ن ﯾَﻜُﻮ', 'نَ أﻣِﯿْﻨَﺎً وَﺻَﺎدِﻗَﺎً ﻣَﻊَ ﻧَﻔْﺴِﮫِ وَﻣَﻊَ أَھْﻠِﮫِ ', '…', ' ', 'وَﺟِﯿْﺮَاﻧِﮫِ وَأَ', 'نْ ﯾَﺒْﺬُلَ ﻛُﻞﱠ ﺟُﮭْﺪٍ ﻓِﻲ إِﻋْﻼءِ ﺷَﺄْ', 'نِ اﻟﻮَطَﻦِ ', 'وَأَنْ ﯾَﻌْﻤَﻞَ ﻋَﻠَﻰ ﻣَﺎ', '.', '.', '.', 'Doc', '2', ':', '...', ' ', 'ﯾَﺠْﻠِﺐُ اﻟﺴﱠﻌَﺎدَةَ ﻟِﻠﻨﱠﺎسِ . وﻟَﻦ ﯾَﺘِﻢﱠ ﻟَﮫُ ذﻟِﻚ إِﻻ ﺑِﺄَنْ ﯾُﻘَﺪﱢمَ اﻟﻤَﻨْﻔَﻌَﺔَ اﻟﻌَﺎﻣﱠﺔَ ', 'ﻋَﻠَﻰاﻟﻤَﻨْﻔَﻌَﺔ', 'ِ', 'اﻟﺨَﺎﺻﱠﺔِ ', 'وَھﺬَ', 'ا ﻣِﺜَﺎ', 'لٌ ﻟِﻠﺘﱠﻀْﺤِﯿَﺔِ', '.', '.', '.', 'Query', ':', 'اﻟﻮَطَﻦِ ', 'وَأَنْ ﯾَﻌْﻤَﻞَ ﻋَﻠَﻰ ﻣَﺎ', 'Which', ' document', ' is', ' more', ' likely', ' to', ' be', ' relevant', '?', '2019/3/11', '39', '\\x0cIR System', ' Architecture', 'docs', 'INDEXING', 'Doc', 'Rep', 'Query', 'Rep', 'Ranking', 'query', 'User', 'result', 's', 'Feedback', 'judgment', 's', '2019/3/11', '40', '\\x0c', 'Text ', 'Representation/Indexing', 'n', ' ', 'Making', ' it easier', ' to', ' match', ' a', ' query', ' with a', ' ', 'document', 'n', ' ', 'Query', ' and document', ' should', ' be', ' represented', ' ', 'using', ' the', ' same', ' units/terms', 'n', ' ', 'Controlled', ' vocabulary', ' vs. full', ' text', ' indexing', 'n', ' ', 'Full-text', ' indexing', ' is', ' more', ' practically', ' useful', ' ', 'and', ' has', ' proven', ' to', ' be', ' as', ' effective', ' as', ' manual', ' ', 'indexing', ' with', ' controlled', ' vocabulary', '2019/3/11', '41', '\\x0cWhat', ' is', ' a', ' good', ' indexing', ' term', '?', 'n', ' ', 'Specific', ' ', '(', 'phrases)', ' or general ', '(', 'single', ' word', ')', '?', 'n', ' ', 'Luhn', ' found', ' that', ' words', ' with middle', ' frequency', ' ', 'are', ' most', ' use', 'ful', 'q ', 'Not', ' too', ' specific', ' ', 'q ', 'Not', ' too', ' general', 'n ', 'All', ' words', ' or a ', '(', 'controlled)', ' subset', '?', 'n ', 'When', ' term', ' weighting', ' is', ' used', ',', ' it', ' is', ' a', ' matter', ' of', ' ', 'weighting', ' not selecting', ' of', ' indexing', ' terms', ' ', '(', 'more', ' later', ')', '2019/3/11', '42', '\\x0c', 'C', 'haracteristics', ' of', '  ', 'Text', 'n ', 'There', ' are', ' stable', ' language-independent', ' ', 'patterns', ' in how', ' people', ' use', ' natural', ' languages', 'q A', ' few', ' words', ' occur', ' very', ' frequently', ';', ' most', ' occur', ' ', 'rarely', 'n Top', ' 2 words', ':', ' 10~15%', ' word', ' occurrences', 'n Top', ' 50', ' words', ':', ' 50%', ' word', ' occurrences', 'q ', 'The', ' most', ' frequent', ' word', ' in', ' one', ' corpus', ' may', ' be', ' ', 'rare', ' in', ' another', ' ', '2019/3/11', '43', '\\x0cZipf’s', ' Law', 'n', ' rank', ' * frequency', ' »', ' constant', 'n', ' ', 'Tells', ' us', ' what', ' the', ' frequency', ' counts', ' in', ' a', ' ', 'document', ' might be', ' like', 'Most', ' use', 'ful', ' words', ' ', '(', 'Luhn 57', ')', 'Word', 'Freq', '.', 'Biggest', 'data structure', '(', 'stop', ' words', ')', '2019/3/11', '44', 'Word Rank', ' ', '(', 'by', ' ', 'Freq', ')', '\\x0cZipf’s', ' Law', 'n', ' ', 'The', ' \"law\" was', ' publicized', ' by ', 'Harvard', ' linguist', ' ', 'George', ' ', 'Kingsley', ' ', 'Zipf. ', 'n', ' ', \"Zipf's\", ' law', ' is', ' thus', ' an', ' empirical', ' law', ',', ' not', ' a theoretical', 'one. ', 'Zipfian', ' distributions', ' are', ' commonly', ' observed', ',', ' in', ' ', 'many', ' kinds', ' of', ' phenomena. ', 'The', ' causes', ' of', ' ', 'Zipfian', ' ', 'distributions', ' in', ' real', ' life', ' are', ' a', ' matter', ' ', 'of', ' some', ' ', 'cont', 'roversy', ',', ' however', '. ', 'n ', 'The', ' fact', ' that ', 'Zipfian', ' distributions', ' arise', ' in randomly', '-', 'generated', ' texts', ' with', ' no', ' linguistic', ' structure', ' suggests', ' ', 'that the', ' law as', ' applied', ' to', ' languages', ' may', ' in', ' part', ' be', ' ', 'a statistical', ' artifact', ' ', '2019/3/11', '45', '\\x0cZipf’s', ' Law', 'n', ' ', 'Empirical research', ' has', ' found', ' that', ' in', ' ', 'English', ',', ' ', 'the', ' frequencies', ' of', ' the', ' approximately', ' 1000', ' ', 'most-frequently-used', ' words', ' are', ' ', 'approximately', ' proportional', ' to', ' ', '1/ns', ' where', ' s', ' is', ' ', 'just', ' slightly', ' more', ' than', ' one', '.', 'n', ' As', ' long', ' as the', ' exponent', ' s', ' exceeds', ' 1', ',', ' it is', ' ', 'possible', ' for such', ' a', ' law to', ' hold', ' with', ' infinitely', ' ', 'many', ' words', ',', ' since', ' if', ' s', ' >', ' 1', ' then', 'q', 'n', ' where', ' ζ', ' is', ' ', \"Riemann's\", ' zeta', ' ', 'function', '.', '2019/3/11', '46', '\\x0cA plot', ' of', ' word', ' frequency', ' in Wikipedia', ' ', '(', 'November', ' 27', ',', ' 2006', '). The', ' plot', ' is', ' in', ' log', '-', 'log', ' coordinates. x', ' is', ' rank', ' of', ' a', ' word', ' in', ' ', 'the', ' frequency', ' table', ';', ' y is', ' the', ' total', ' ', 'number', ' ', 'of', ' the', ' word', '’s', ' occurences. ', 'Most', ' popular words', ' are', ' ', '“the', '”', ',', ' “of', '” and', ' ', '“and', '”', ',', ' as', \" expected. Zipf's\", ' law', ' corresponds', ' to ', 'the', ' upper', ' linear', ' portion', ' of', ' the', ' ', 'curve', ',', ' roughly', ' following', ' the', ' green', ' ', '(', '1/x)', ' line', '.', '2019/3/11', '47', '\\x0cStemming', 'n ', 'Words', ' with', ' similar', ' meanings', ' should', ' be', ' ', 'mapped', ' to the', ' same', ' indexing', ' term', 'n Stemming', ':', ' Mapping', ' all', ' inflectional', ' forms', ' of', ' ', 'words', ' to', ' the', ' same', ' root', ' form', ',', ' e.g', '.', 'q', ' computer', ' ->', ' compute', 'q', ' computation', ' ->', ' compute', 'q', ' computing', ' ->', ' compute', ' ', 'n', ' ', 'Porter', '’s Stemmer', ' is', ' ', 'the', ' most', ' popular', ' for', ' ', 'English', '2019/3/11', '48', '\\x0cTwo ', 'Statistic ', 'Profile', ' ', 'Examples', 'Doc', ' 1', 'the', ' 0.3', 'a 0.15', '.', '.', '.', 'text', '  0.2', 'mining', ' 0.1', 'assocation', ' 0.02', 'clustering', ' 0.01', '…', 'food', ' 0.00001', '…', 'Doc 2', 'the', ' 0.29', 'a 0.16', 'of', ' 0.10', '…', 'food', ' 0.25', 'nutrition', ' 0.1', 'heal', 'thy', ' 0.05', 'diet', ' 0.02', '…', 'Word', ' counts', ' tell', ' us', ' about', ' the', ' content/topic', ' of', ' ', 'the', ' document', ',', ' ', 'making', ' it possible', ' to by-pass', ' sophisticated', ' natural', ' language', ' analysis', '!', '2019/3/11', '49', '\\x0c', 'Relevance', ' feed', 'back', ' ', 'n', ' ', 'Relevance', ' feed', 'back', ' is', ' a', ' feature', ' of', ' some', ' ', 'information retrieval', ' systems. ', 'The', ' idea', ' ', 'behind', ' relevance', ' feedback', ' is', ' to', ' take', ' the', ' ', 'results', ' that are', ' initially', ' returned', ' from', ' some', ' ', 'query', ' and', ' to', ' use', ' in', 'formation', ' about', ' whe', 'ther', ' ', 'or', ' not', ' those', ' results', ' are', ' relevant to', ' per', 'form', ' a', ' ', 'new', ' query', '. ', '2019/3/11', '50', '\\x0c', 'Relevance', ' ', 'Feedback', 'n ', 'Explicit', ' feed', 'back', ' ', 'q ', 'Obtained', ' by having', ' the', ' user', ' mark', ' specific', ' documents', ' as', ' ', 'relevant', ' or', ' irrelevant', ' ', 'n', ' Implicit', ' feed', 'back', 'q inferred', ' from', ' user', ' behavior', ',', ' such', ' as', ' noting', ' which', ' ', 'documents', ' they', ' do', ' and', ' do', ' not', ' select', ' for viewing', ',', ' and/or ', 'how', ' long', ' they', ' view', ' those', ' documents', ' ', 'n ', 'Blind', ' feed', 'back', ' ', 'q ', 'Blind', ' or', ' \"pseudo\"', ' relevance', ' feedback', ' is', ' obtained', ' by', ' ', 'assuming', ' that the', ' top', ' n', ' documents', ' in', ' the', ' result', ' set', ' actually', ' ', 'are', ' relevant', '.', '2019/3/11', '51', '\\x0cExplicit', ' ', 'Feedback', 'Query', 'Updated', 'query', 'Retrie', 'val', 'Engine', 'Document', 'collection', 'Feedback', 'Results', ':', 'd1 3.5', 'd2 2.4', '…', 'dk 0.5', '.', '.', '.', 'Judgments', ':', 'd1 ', '+', 'd2 ', '-', 'd3 ', '+', '…', 'dk ', '-', '.', '.', '.', 'User', '2019/3/11', '52', '\\x0cPseudo/Blind/Automatic', ' ', 'Feedback', 'Query', 'Updated', 'query', 'Retrie', 'val', 'Engine', 'Document', 'collection', 'Feedback', 'Results', ':', 'd1 3.5', 'd2 2.4', '…', 'dk 0.5', '.', '.', '.', 'Judgments', ':', 'd1 ', '+', 'd2 ', '+', 'd3 ', '+', '…', 'dk ', '-', '.', '.', '.', 'top', ' 10', '2019/3/11', '53', '\\x0cSummary', 'n', ' Ad', ' hoc retrieval', ' and', ' filtering', ' are', ' two', ' basic', ' ', 'forms', ' ', 'of', ' information', ' retrieval', 'n', ' ', 'Document', ' selecting', ' and', ' ranking', ' are', ' two', ' basic', ' ', 'ad', ' hoc', ' retrieval', ' strategies', ' ', 'n', ' ', 'Ranking', ' is generally', ' preferred', ' due', ' to ', 'the', ' ', 'inherent', ' uncertainty', ' and', ' vagueness', ' of', ' ', 'relevance', ' ', '(', 'justified', ' by', ' PRP', ')', 'n', ' A', ' retrieval', ' model', ' defines', ' a', ' way', ' to', ' measure', ' ', 'relevance', ',', ' and', ' there', ' are', ' three', ' general', ' types', ' of', ' ', 'models', '2019/3/11', '54', '\\x0c', 'Summary', ' ', '(', 'cont.)', 'n', ' ', 'How to', ' interpret', ' and', ' compute', ' the', ' precision', ',', ' ', 'recall', ',', ' PR', ' ', 'curve', ',', ' and', ' average', ' precision', 'n', ' ', 'W', 'hat', ' is', ' ', 'Zipf', '’s', ' law', ',', ' and', ' what', ' it', ' tells', ' us', ' about', ' ', 'the', ' word', ' usage', ' in', ' a', ' document', ' or', ' collection', ' ', 'of', ' ', 'document', 's', 'n ', 'Why', ' stemming might', ' help', ' improve', ' retrieval', ' ', 'performance', 'n', ' ', 'What', ' is', ' relevance', ' feedback and', ' pseudo', ' ', 'feedback', '2019/3/11', '55', '\\x0c', 'Week', ' 5', 'Regular', ' ', 'Expression', ',', ' NER', 'Gild Shen', ',', ' Viet-Cuong Trieu', ' ', '(', 'Daniel)', ',', ' Fu-ren', ' Lin', 'National', ' Tsing', ' ', 'Hua', ' ', 'University', '\\x0cPython', ' practice', ' week', ' 5', 'https', ':', '//bit.ly/3ud', 'KqFl', '\\x0c1. Regular ', 'Expression', '“re', '”', ' module', ' ', '(', 'https', ':', '//docs.py', 'thon.org/3/library/re.html#m', 'odule-re)', '•', ' re.findall', '(', ')', '•', ' re.search', '(', ')', '•', ' re.match', '(', ')', '•', ' re.sub', '(', ')', '[', ' ]', ':', ' indicate', ' a', ' set', ' of', ' character', '^ = start', ',', ' $ = end', ' -- match', ' the', ' start', ' or', ' end', ' of', ' the', ' string', '\\\\d -- decimal digit', ' [0-9', ']', '.', ' ', ':', ' any', ' sing', 'le', ' character', '…', '…', 'Note', ':', '•', ' re.match', '(', ')', ',', ' re.search', '(', ')', ' only', ' matches', ' the', ' first', ' occurrence.', ' ', '•', ' re.match', '(', ')', ',', ' give', ' the', ' ', ' sear', 'ches', ' only', ' from', ' ', 'the', ' beginning', ' of', ' the', ' string. ', '•', ' re.search', '(', ')', ' search', ' any', ' location', '.', 'Goal', ':', ' find', '\"', ' or', ' \"find', ' and', ' replace', '\"', '\\x0c1. Regular ', 'Expression', 'Example', '1', ':', '  ', 'Find', ' string “grey', '”', 'Example', ' 2', ':', ' Boole', 'an', ' \"or', '”', ':', ' A', ' vertical', ' bar separates', ' alternatives. For', ' ', 'example', ',', ' gray', '|grey', ' means', ' \"', 'gray', '\"', ' or', ' \"', 'grey', '\"', '.', '\\x0c1. Regular ', 'Expression', 'Example', '3', ':', '  ', 'The', ' question', ' mark', ' indicates', ' zero', ' or', ' one', ' occurrences', ' of', ' ', 'the', ' preceding', ' element. For', ' example', ',', ' colou', '?', 'r matches', ' both', ' \"color', '\"', ' ', 'and', ' \"', 'colour', '\" ', '.', '\\x0c1. Regular ', 'Expression', 'Example', '4', ':', '  ', 'The', ' asterisk', ' indicates', ' zero', ' or more', ' occurrences', ' of', ' the', ' ', 'preceding', ' element. ', 'For', ' example', ',', ' ab*c', ' matches', ' \"ac', '\"', ',', ' \"abc', '\"', ',', ' \"abbc', '\"', ',', ' ', '\"abbbc', '\"', ',', ' and', ' so', ' on', '\"', 'Example', ' 5', ':', 'The', ' plus', ' ', '(', '+) sign', ' indicates', ' one', ' or', ' more', ' occurrences', ' of', ' the', ' preceding', ' ', 'element. ', 'For', ' example', ',', ' ab+c matches', ' \"abc', '\"', ',', ' \"abbc', '\"', ',', ' \"abbbc', '\"', ',', ' and so', ' on', ',', ' but', ' not', ' ', '\"ac\"', '.', '\\x0c1. Regular ', 'Expression', 'Example', '6', ':', '  ', 'The', ' [a-z]+ means', ' the', ' pattern', ' is', ' combined', ' in repeated', ' ', 'characters', ' from', ' “a', '” to ', '“z', '”', ' more', ' than', ' 1', ' times', ' but', ' exclude', ' of', ' Arabic', ' ', 'numerals', ' ', '(', '0', ',', '1', ',', '…9)', 'Example', '7', ':', '  ', 'Find', ' any', ' string', ' by', ' the', ' patte', 'rn', ' that', ' begins', ' with ', '“dis', '” and', ' ', 'ends', ' in', ' “tion', '”', '.', '\\x0c1. Regular ', 'Expression', 'Example', '7', ':', '  ', 'Find', ' any', ' string', ' by', ' the', ' patte', 'rn', ' that', ' begins', ' with ', '“go”', ' and', ' ends', ' in', ' ', '“d”', ',', ' additionally', ',', ' with', ' at', ' least two', ' ', '“o', '”', ' and', ' 5', ' maximum', ' ', '“o', '”', ' in', ' it', '.', '\\x0c', 'What', ' is', ' this', ' used', ' for', '?', '\\x0c2. Named-entity', ' recognition', ' ', '(', 'NER)', 'Spacy', ' Entity', '\\x0cNER', 'Person', ' name', ' Organizations', ' ', 'Locations', 'Medical', 'codes', ' ', 'Time', ' ', 'expressions', 'Quantities', 'Monetary', 'values', 'Percentages', 'Etc', '.', '\\x0cNER – English', ',', ' spacy', ' package', '\\x0cNER – English', ',', ' spacy', ' package', '\\x0cNER- Chinese- using', ' spacy', 'You', ' need', ' to down', 'load', ' zh_core_w', 'eb_s', 'm', ' for spacy', ' ', '\\x0cNER- Chinese- Using', ' Jieba', '\\x0cNER- Chinese- using', ' Jieba', 'url', \" = 'https\", ':', '//github.com/fxsjy/jieba/raw/master/extra_dict/dict.txt.big', \"'\", 'r ', '= request', 's.get', '(url', ',', ' allow_redirects=', 'True)', 'open', '(', \"'dict.txt.big'\", ',', \" 'wb')\", '.write', '(', 'r', '.content)', 'jieba.set_dictionary', '(', \"'dict.txt.big')\", 'kw1', '=jieba.analyse.extract_tags', '(', 'chinese_text', ',', 'top', 'K=20', ',', 'with', 'Weight', '=True', ',', 'allo', 'wPOS', '=', '(', \"'n'\", ',', \"'ns')\", ')', 'for', ' item', ' in', ' kw1', ':', 'print', '(', '\"extract_tags', ':', ' \"', ',', 'item)', '\\x0cNER- Chinese- Using CKIP', 'Trans', 'former', '\\x0cCkip', 'Tagger', '• This', ' open-source', ' library', ' implements', ' neural', ' CKIP-style', ' ', 'Chinese', ' ', 'NLP tools', '.', '•', ' ', '(', 'WS)', ' word segmentation', '•', ' ', '(', 'POS)', ' part-of-speech', ' tagging', '•', ' ', '(', 'NER)', ' named', ' entity', ' recognition', 'https', ':', '//github.com/ckiplab/ckiptagger', '/', '\\x0cNER', ' result', '\\x0cCKIP ', 'Trans', 'formers', '•', ' BERT Based', '• This', ' project', ' provides', ' traditional', ' ', 'Chinese', ' transformers', ' models', ' ', '(', 'including', ' ALBERT', ',', ' BERT', ',', ' GPT2)', ' and', ' NLP tools', ' ', '(', 'including', ' ', 'word', ' segmentation', ',', ' part-of-spee', 'ch', ' tagging', ',', ' named', ' entity', ' ', 'recognition)', '.', '•', ' ', '這', '個', '專案', '提供', '了', '繁體', '中文', '的', ' transformers', ' ', '模型', '（', '包含', ' ALBERT', '、', 'BERT', '、', 'GPT2', '）', '及', '自然', '語言', '處理', '工具', '（', '包含', '斷詞', '、', '詞性', '標記', '、', '實體', '辨識', '）', '。', '\\x0cNER ', 'Result', '\\x0chttps', ':', '//hugging', 'face.co', '/', '\\x0c', 'Feature', ' ', 'Generation', 'What', ' sorts', ' of', ' features', ' would', ' we', ' generate', '?', ' ', '•', ' Tokens', ' ', '(', 'bag', ' ', 'of', ' words)', '•', ' Multiwords', '•', ' Features', ' by', ' POS', ' tags', '•', ' Name', ' entities', ',', ' such', ' as', ' doctor', ',', ' president', ',', ' professor', ',', ' ', '…', 'Remove', ' insignificant tokens', '•', ' Remove', ' stopwords', '• ', 'Remove', ' tokens', ' based', ' on term', ' ', 'frequency/document', ' frequency', '•', ' ', 'Remove', ' tokens', ' based', ' on', ' domain', ' knowledge', '\\x0cHW5', ':', ' ', '•', ' Finish', ' 15', ' steps', ' of', ' website', ' on', ' https', ':', '//regexone.com', '/', '•', ' And …', '• ', 'Capture', ' the', ' last', ' question', ' with', ' your', ' answer', ' and', ' time', ' in', ' an', ' image', ' ', '(', 'check', ' the', ' example)', '•', ' Write', ' down', ' every', ' answer', ' of', ' ', '15', ' steps', '.', '•', ' ', 'Put', ' all', ' these', ' two', ' results', ' in', ' your', ' submission', '.', '•', ' ', 'You', ' may', ' want to', ' use', ' https', ':', '//regex', '101.com/ to', ' rein', 'force', ' your', ' ', 'learning', ' in', ' ', 'Regular', ' ', 'Expression', '\\x0c\\x0cNote', '• ', 'We', ' will', ' have', ' a', ' quiz', ' of', ' regular', ' expression', ' next', ' week', '.', '• ', 'Don', '’t', ' forget', ' to', ' bring', ' your', ' laptop', ' or', ' pad', ',', ' it will', ' be', ' inconvenient', ' if', ' ', 'you', ' do', ' it', ' on', ' cellphone', '.', '\\x0c', '2', 'From', ' ', 'Textual', ' ', 'Information', ' to', 'Numerical', ' ', 'Vectors', 'To mine', ' text', ',', ' we', ' ', 'ﬁrst', ' need', ' to', ' process', ' it', ' into', ' a', ' form', ' ', 'that', ' data-mining', 'procedures', ' can', ' use. As', ' mentioned', ' in', ' the', ' previous', ' chapter', ',', ' this', ' typi', '-', 'cally', ' involves', ' generating', ' features', ' in a', ' spreadsheet', ' format. ', 'Classical', 'data', ' mining', ' looks', ' at', ' highly', ' structured', ' data. ', 'Our', ' spre', 'ad', 'sheet', ' model', 'is', ' the', ' embodiment', ' of', ' a', ' representation', ' that', ' is', ' supportive', ' of', ' predictive', 'modeling. In', ' some', ' ways', ',', ' predictive', ' text mining', ' is', ' simpler', ' and', ' more', 'restrictive', ' than', ' open-ended', ' data', ' mining. ', 'Because', ' predictive', ' mining', 'methods', ' are so highly', ' deve', 'loped', ',', ' most', ' time', ' spent', ' on', ' data-mining', 'projects', ' is', ' for', ' data', ' preparation. We', ' say', ' ', 'that', ' text', ' mining', ' is', ' unstruc', '-', 'tured', ' because', ' it', ' is', ' very', ' far', ' from', ' ', 'the', ' spreadsheet', ' model', ' that we', ' need', 'to', ' process', ' data', ' for', ' prediction. Yet', ',', ' the', ' transformation', ' of', ' data', ' from', ' text', 'to', ' the', ' spreadsheet', ' model', ' can', ' be', ' highly', ' methodical', ',', ' and', ' we', ' have', ' a care', '-', 'fully', ' organized', ' procedure', ' to ', 'ﬁll', ' in', ' the', ' cells of', ' the', ' spread', 'sheet. ', 'First', ',', 'of', ' course', ',', ' we', ' have', ' to determine', ' the', ' nature', ' of', ' the', ' columns', ' ', '(', 'i', '.e', '.', ',', ' the', 'features)', ' ', 'of', ' the', ' spread', 'sheet. ', 'Some', ' use', 'ful', ' features', ' are', ' easy', ' to', ' obtain', '(', 'e.g.', ',', ' a', ' word', ' as', ' it occurs', ' in', ' text)', ' and', ' some', ' are ', 'much more', ' dif', 'ﬁcult', '(', 'e.g.', ',', ' the', ' grammatical function', ' of', ' a', ' word', ' in', ' a', ' sentence', ' such', ' as', ' subject', ',', 'ob', 'ject', ',', ' etc.). In this', ' chapter', ',', ' we', ' will', ' dis', 'cuss', ' how', ' to', ' obtain', ' ', 'the', ' kinds', ' of', 'features', ' commonly', ' generated', ' from', ' text', '.', '2.1', ' Collecting', ' ', 'Documents', 'Clearly', ',', ' the', ' ﬁrst step', ' in', ' text', ' mining', ' is', ' to', ' collect', ' the', ' data', ' ', '(', 'i.e.', ',', ' the', 'relevant documents). In', ' many', ' text-mining', ' scenarios', ',', ' the', ' relevant doc', '-', '\\x0c16', '2. From', ' ', 'Textual', ' ', 'Information', ' to', ' ', 'Numerical', ' ', 'Vectors', 'uments', ' may', ' already', ' be', ' given', ' or ', 'they', ' may', ' be', ' part', ' of', ' the', ' problem', 'description. ', 'For', ' example', ',', ' a', ' ', 'Web', ' page', ' retrieval', ' application', ' for an', ' in', '-', 'tranet', ' implicitly speci', 'ﬁes', ' the', ' rele', 'vant', ' documents', ' to', ' be', ' the', ' ', 'Web', ' pages', 'on', ' the', ' intranet. If', ' the', ' documents', ' are', ' readily', ' identi', 'ﬁed', ',', ' then', ' they', ' can', 'be', ' obtained', ',', ' and', ' the', ' main', ' issue', ' is', ' to', ' cleanse', ' ', 'the', ' samples', ' and', ' ensure', 'that', ' they', ' are', ' ', 'of', ' high', ' quality. As', ' with', ' nontextual', ' data', ',', ' human', ' interven', '-', 'tion', ' can', ' compromise', ' the', ' integrity', ' of', ' the', ' document', ' collection', ' process', ',', 'and', ' hence', ' extreme', ' care', ' must', ' be', ' exercised. Sometimes', ',', ' the', ' documents', 'may', ' be', ' obtained', ' from', ' document', ' ware', 'h', 'ouses', ' or databases. In', ' these', 'scenarios', ',', ' it', ' is reasona', 'ble', ' to', ' expect', ' ', 'that', ' data', ' cle', 'ansing', ' was', ' done', ' before', 'deposit', ' and', ' we', ' can', ' be', ' con', 'ﬁdent', ' in', ' ', 'the', ' quality', ' ', 'of', ' the', ' documents', '.', 'In some', ' applications', ',', ' one', ' may', ' need', ' to', ' have', ' a', ' data', ' collection', ' pro', '-', 'cess. ', 'For', ' instance', ',', ' for a ', 'Web', ' application', ' comprising', ' a', ' number', ' of', 'autonom', 'ous', ' ', 'Web', ' sites', ',', ' one', ' may', ' deploy', ' a', ' software', ' tool', ' ', 'such', ' as', ' a', ' ', 'Web', 'crawler', ' that collects', ' ', 'the', ' documents. In', ' other', ' applications', ',', ' one', ' may', 'have', ' a', ' logging', ' process', ' attached', ' to', ' an', ' input data', ' stream', ' for', ' a', ' leng', 'th', 'of', ' time. For', ' example', ',', ' an', ' e-mail', ' audit', ' application', ' may', ' log', ' all', ' incoming', 'and', ' outgoing', ' messages', ' at', ' a', ' mail', ' server', ' for', ' a', ' period', ' of', ' time', '.', 'Sometimes', ' the', ' set', ' of', ' documents', ' can', ' be', ' extremely', ' large', ' and', ' data', '-', 'sam', 'pling', ' techniques', ' can', ' be', ' used', ' to', ' select', ' a', ' manage', 'able', ' set', ' of', ' relevant', 'document', 's. These', ' sam', 'pling', ' techniques', ' will depend', ' on the', ' application', '.', 'For instance', ',', ' documents', ' may', ' have', ' a', ' time', ' stamp', ',', ' and', ' more', ' recent', ' doc', '-', 'uments', ' may have', ' a', ' higher', ' relevance. ', 'Depending', ' on', ' our', ' resources', ',', ' w', 'e', 'may', ' limit', ' our', ' sam', 'ple', ' to', ' documents', ' ', 'that', ' are', ' ', 'more', ' use', 'ful', '.', 'For', ' research and', ' development', ' of', ' text-mining', ' techniques', ',', ' more', 'generic', ' data', ' may', ' be', ' necessary. This', ' is', ' usually', ' called', ' a', ' corpus. For', 'the', ' accompanying', ' sof', 'tware', ',', ' we', ' mainly', ' used', ' the', ' collection', ' ', 'of', ' ', 'Reuters', 'news stories', ',', ' referred', ' to', ' as', ' ', 'Reuters', ' corpus', ' RCV1', ',', ' obtainable', ' from', ' the', 'Reuters', ' ', 'Corporation', ' ', 'Web site. ', 'However', ',', ' there', ' are', ' many', ' other', ' corpora', 'available', ' ', 'that', ' may', ' be', ' more', ' appropriate', ' for some', ' studies', '.', 'In', ' the', ' early', ' days', ' of', ' text', ' processing', ' ', '(', '1950s', ' and', ' 1960s)', ',', ' one', ' mill', 'ion', 'words', ' was', ' considered', ' a', ' very', ' large', ' collection. ', 'This', ' was', ' the', ' size', ' of', ' one', 'of', ' the', ' ﬁrst widely', ' available', ' collections', ',', ' the', ' ', 'Brown', ' corpus', ',', ' consisting', 'of', ' 500', ' samples', ' of', ' about', ' 2000', ' words', ' each', ' ', 'of', ' American', ' ', 'English', ' texts', ' of', 'varying', ' genres. A ', 'European', ' corpus', ',', ' the', ' Lancaster-Oslo-Bergen', ' corpus', '(', 'LOB)', ',', ' was', ' modeled', ' on the', ' ', 'Brown', ' corpus', ' but', ' was', ' ', 'for British ', 'English', '.', 'Both', ' these', ' are', ' still', ' available', ' and', ' still', ' used. In', ' the', ' 1970s', ' and', ' 1980s', ',', 'many', ' more', ' resources', ' became', ' available', ',', ' some', ' from academic', ' initiatives', 'and', ' others', ' as a result', ' of', ' government-s', 'ponsored', ' research. Some', ' widely', 'used', ' corpora', ' are', ' the', ' ', 'Penn Tree', ' ', 'Bank', ',', ' a', ' collection of', ' manually', ' parsed', '\\x0c2.1', ' ', 'Collecting', ' ', 'Documents', '17', 'sentences', ' from', ' the', ' ', 'Wall', ' ', 'Street', ' ', 'Journal', ';', ' the', ' TREC ', '(', 'Text', ' ', 'Retrieval', 'and', ' ', 'Evaluation', ' ', 'Conferences)', ' collec', 'tions', ',', ' consisting', ' of', ' selections', ' from', 'the', ' Wall', ' Street', ' ', 'Journal', ',', ' the', ' ', 'New York ', 'Times', ',', ' ', 'Ziff-Davis', ' Publications', ',', 'the', ' ', 'Federal', ' ', 'Register', ',', ' and', ' others', ';', ' the', ' proceedings', ' of', ' the', ' ', 'Canadian', 'Parliament', ' in', ' parallel', ' ', 'English', '–', 'French', ' ', 'translations', ',', ' widely', ' used', ' in', 'statistical', ' machine', ' translation', ' research', ';', ' and', ' the', ' ', 'Gutenberg', ' ', 'Project', ',', 'a', ' very', ' large', ' collection', ' of', ' literary', ' and', ' other', ' texts', ' put', ' into machine', '-', 'readable', ' form', ' as', ' the', ' material', ' comes', ' out', ' ', 'of', ' copyright. A', ' collec', 'tion', ' of', 'Reuters', ' news', ' stories', ' called', ' ', 'Reuters-21578', ' ', 'Distribution', ' ', '1.0', ' has', ' been', 'widely', ' used', ' in', ' studying', ' methods', ' for text', ' categorization', '.', 'As', ' the', ' importan', 'ce', ' of large', ' text', ' corpora', ' became', ' evident', ',', ' a number', 'of', ' organizations', ' and', ' initiatives', ' arose', ' to', ' coordinate', ' activity and', ' pro', '-', 'vide', ' a', ' distribution', ' mechanism', ' for', ' corpora. Two', ' of', ' the', ' main', ' ones', ' are', 'the', ' ', 'Linguistic', ' Data', ' ', 'Consortium', ' ', '(', 'LDC)', ' housed', ' at the', ' ', 'University', ' of', 'Pennsylvania', ' and', ' the', ' ', 'International', ' ', 'Computer', ' ', 'Archive', ' of ', 'Modern', ' and', 'Medieval', ' English', ' ', '(', 'ICAME)', ',', ' which', ' resides', ' in ', 'Bergen', ',', ' Norway. Many', 'other', ' centers', ' of', ' varying', ' size', ' exist', ' in', ' academic', ' institution', 's. ', 'The', ' ', 'Text', 'Encoding', ' Initiative', ' ', '(', 'TEI) is', ' a standard', ' for', ' text', ' collections', ' sponsored', ' by', 'a', ' number', ' ', 'of', ' professional', ' societies', ' concerned', ' with', ' language', ' processing', '.', 'There', ' a number', ' of', ' ', 'Web', ' sites', ' devoted', ' to corpus', ' linguistics', ',', ' most', ' having', 'links', ' to', ' collections', ',', ' courses', ',', ' software', ',', ' etc', '.', 'Another', ' resource', ' to', ' consider', ' is', ' the', ' ', 'World', ' ', 'Wide', ' ', 'Web', ' itsel', 'f. Web', 'crawlers', ' can', ' build', ' collections', ' of', ' pages', ' from', ' a', ' particular', ' site', ',', ' such', ' as', 'Yahoo', ',', ' or', ' on', ' a', ' particular', ' topic. ', 'Given', ' the', ' size', ' ', 'of', ' the', ' ', 'Web', ',', ' collections', 'built', ' ', 'this', ' way', ' can', ' be', ' huge. The', ' main', ' problem', ' with', ' ', 'this', ' approa', 'ch to', 'document', ' collection', ' is', ' that', ' the', ' data', ' may', ' be', ' ', 'of', ' dubious', ' quality', ' and', 'require', ' extensive', ' cleansing', ' before', ' use. A', ' more', ' focused', ' corpus', ' can', ' be', 'built', ' from', ' the', ' archives', ' of', ' USENET', ' news', ' groups', ' and', ' accessible', ' from', 'many', ' ISPs', ' directly', ' or through ', 'Google', ' ', 'Groups. These', ' discussion', ' groups', 'cover', ' a sing', 'le', ' topic', ',', ' such', ' as', ' ﬂy', ' ﬁshing', ',', ' or', ' broader topics', ' such', ' as', ' the', 'cultures', ' of', ' particular countries. A similar set', ' of', ' discussions', ' is', ' available', 'from', ' LISTSERVs. ', 'These', ' are', ' almost', ' always', ' available', ' only', ' by', ' subscrib', '-', 'ing', ' to a', ' particular', ' group', ' but have', ' ', 'the', ' advantage', ' that', ' many', ' lists have', 'long-term', ' archives', '.', 'Finally', ',', ' institutions', ' such', ' as', ' gove', 'rnment', ' agencies', ' and', ' corporations', 'of', 'ten', ' have', ' large', ' document', ' collections. Corporate', ' collections', ' are', ' usu', '-', 'ally', ' not', ' availa', 'ble', ' outside', ' ', 'the', ' corporation', ',', ' but', ' gove', 'rnment', ' collections', 'of', 'ten', ' are. One', ' widely', ' studied', ' collection', ' is', ' the', ' MEDLINE', ' data', ' set', ' from', 'the', ' National', ' ', 'Institutes', ' of', ' ', 'Health', ',', ' which', ' contains', ' a', ' very', ' large', ' number', 'of', ' abstracts', ' on', ' medical', ' subjects. ', 'The', ' advantage', ' of', ' getting', ' documents', '\\x0c18', '2. From', ' ', 'Textual', ' ', 'Information', ' to', ' ', 'Numerical', ' ', 'Vectors', 'from', ' such', ' sources', ' is', ' that', ' one', ' can', ' be', ' reas', 'onably', ' sure', ' that', ' the', ' data', ' have', 'been', ' reviewed', ' and', ' are', ' of', ' good', ' quality', '.', '2.2', ' ', 'Document', ' Standardization', 'Once', ' ', 'the', ' documents', ' are', ' collected', ',', ' it', ' is', ' not', ' uncommon', ' to ', 'ﬁnd', ' them', ' in', 'a', ' variety', ' of', ' different', ' formats', ',', ' depending', ' on how', ' the', ' documents', ' were', 'generated. ', 'For', ' example', ',', ' some', ' documents', ' may', ' have', ' been', ' generated', ' by', 'a', ' word', ' process', 'or', ' with', ' its', ' own', ' proprie', 'tary', ' format', ';', ' others', ' may', ' have', 'been', ' generated', ' using', ' a simple', ' text', ' editor', ' and', ' saved', ' as ASCII text', ';', ' and', 'some', ' may have', ' been', ' scanned', ' and', ' stored', ' as', ' images. Clearly', ',', ' if', ' we', ' are', 'to', ' process', ' all', ' the', ' documents', ',', ' it', '’s', ' helpful', ' to', ' convert', ' them', ' to', ' a', ' standard', 'format', '.', 'in', 'cluding', ' most', ' of', 'The', ' com', 'puter', ' industry', ' as', ' a', ' whole', ',', 'the', 'text-processing', ' community', ',', ' has', ' adopted', ' XML ', '(', 'Extensib', 'le', ' ', 'Markup', 'Language)', ' as', ' its', ' standard', ' exchange', ' format', ',', ' and', ' this', ' is', ' the', ' standard', ' we', 'adopt', ' for', ' our document', ' collections', ' as', ' well. Brie', 'ﬂy', ',', ' XML', ' is', ' a standard', 'way', ' to', ' insert', ' tags', ' onto', ' a', ' text', ' to', ' identify', ' its', ' parts. ', 'Al', 'though', ' tags', ' can', ' be', 'nested', ' within other', ' tags', ' to', ' arbitrary', ' depth', ',', ' we', ' will', ' use', ' that', ' capability', 'only', ' sparingly here. We', ' assume', ' that', ' each document', ' is', ' marked', ' off', ' from', 'the', ' other', ' documents', ' in', ' ', 'the', ' corpus', ' by', ' having', ' a distinguishing', ' tag', ' at', 'the', ' beginning', ',', ' such', ' as', ' <DOC>. By', ' XML convention', ',', ' tags', ' come', ' in be', '-', 'ginning', ' and', ' ending', ' pairs. ', 'They', ' are', ' enclosed', ' in', ' angle', ' brackets', ',', ' and', ' the', 'ending', ' tag', ' has a', ' back', ' slash', ' immediately', ' following', ' the', ' opening', ' angle', 'bracket. Within', ' a', ' document', ',', ' there', ' can', ' be', ' many', ' other', ' tags', ' to', ' mark', 'off', ' sections', ' of', ' the', ' document. ', 'Common', ' sections', ' are', ' <DATE>', ',', ' <SUB', '-', 'JECT>', ',', ' <TOPIC>', ',', ' and', ' <TEXT>. We', ' will', ' focus', ' mainly', ' on', ' <SUBJECT>', ',', '<TOPIC>', ',', ' and', ' <TEXT>. The', ' names', ' are', ' arbit', 'rary. They', ' could', ' just', ' as', 'well', ' be', ' <HEADLINE>', ' and', ' <BODY>', '. ', 'An', ' exam', 'ple', ' ', 'of', ' an', ' XML', ' document', 'is', ' shown', ' in', ' Figure', ' 2.1', ',', ' where', ' ', 'the', ' document', ' has', ' a', ' distinguishing', ' tag', ' of', '<DOC>', '.', 'Many', ' currently available', ' corpora', ' are', ' already', ' in', ' this', ' format', ' ', '(', 'e.g.,', ' the', 'new', ' corpus', ' available', ' from', ' ', 'Reuters). The', ' main reason', ' for identifying', 'the', ' pieces', ' ', 'of', ' a', ' document', ' consisten', 'tly', ' is', ' to', ' allow', ' selection', ' of', ' those', ' parts', 'that', ' will', ' be', ' used', ' to', ' generate', ' features. We', ' will', ' al', 'most', ' always', ' want', 'to', ' use', ' the', ' part', ' delimited', ' as', ' <TEXT>', ' but', ' may', ' also', ' want', ' to', ' include', 'parts', ' marked', ' <SUBJECT>', ',', ' <HEADLINE>', ',', ' or the', ' like. Additionally', ',', 'for', ' text', ' classi', 'ﬁcation', ' or clustering', ',', ' one', ' wants', ' to', ' generate', ' features', 'from', ' a TOPIC section', ' if', ' there', ' is', ' one. ', 'Selected', ' document', ' parts', ' may', ' be', 'con', 'catenated', ' into', ' a', ' single', ' string', ' of', ' characters', ' or', ' may', ' be', ' kept', ' separate', '\\x0c2.2', ' ', 'Document', ' Standardization', '19', '<DOC>', '<TEXT>', '<TITLE>', 'Solving', ' ', 'Regress', 'ion', ' Problems', ' with', ' ', 'Rule-based', ' Classifiers', '</TITLE>', '<AUTHORS>', '<AUTHOR>', 'Nitin', ' Indurkhya', '</AUTHOR>', '<AUTHOR>', 'Sholom', ' M. ', 'Weiss', '</AUTHOR>', '</AUTHORS>', '<ABSTRACT>', 'We', ' describe', ' a lightweight', ' learning', ' method', ' ', 'that', ' induces', ' an', ' ensemble', 'of', ' decision-rule', ' solutions', ' for', ' regression', ' problems', '.', 'direct', ' prediction', ' of', ' a', ' continuous', ' output', ' variable', ',', ' the', ' method', 'discretizes', ' the', ' variable', ' by k-means', ' clustering', ' and', ' solves', ' the', 'resultant', ' classification', ' problem. Predictions', ' on', ' new', ' examples', ' are', 'made', ' by', ' averaging', ' the', ' mean', ' values', ' of', ' classes', ' with', ' votes', ' that', ' are', 'close', ' in', ' number', ' to', ' the', ' most', ' likely', ' class', '.', 'evidence', ' that this', ' indirect', ' approa', 'ch', ' can', ' ', 'of', 'ten', ' yield', ' strong', 'results', ' for', ' many', ' applications', ',', ' generally', ' outper', 'forming', ' direct', 'approaches', ' such', ' as', ' regression', ' trees', ' and', ' rivaling', ' bagged', ' regression', 'trees', '.', '</ABSTRACT>', '</TEXT>', '</DOC>', 'We', ' provide', ' experimental', 'Instead', ' of', 'Figure', ' ', '2.1. An XML ', 'Document', 'if', ' one', ' wants', ' to', ' distinguish the', ' features', ' generated', ' from', ' ', 'the', ' headline', ',', 'say', ',', ' from', ' those', ' generated', ' from', ' ', 'the', ' document', ' body', ',', ' and', ' perhaps', ' weight', 'them', ' differently', '.', 'Many', ' word', ' processors', ' these', ' days', ' allow', ' ', 'documents', ' to be', ' saved', ' in', 'XML', ' format', ',', ' and', ' stand-alone', ' ', 'ﬁlters', ' can', ' be', ' obtained', ' to convert', ' existing', 'documents', ' without', ' having', ' to', ' process', ' each', ' one', ' manually. Documents', 'encoded', ' as', ' images', ' are', ' harder to', ' deal', ' with', ' curren', 'tly. ', 'There', ' are', ' some', 'OCR ', '(', 'optical character', ' recognition)', ' systems', ' that', ' can', ' be', ' use', 'ful', ',', ' but', 'these', ' can', ' introduce', ' errors', ' in the', ' text', ' and', ' ', 'must', ' be', ' used', ' with', ' care', '.', 'Why', ' should', ' we', ' care', ' about', ' document', ' standardization', '?', ' The main', 'advantage', ' ', 'of', ' standardizing', ' ', 'the', ' data', ' is', ' ', 'that the', ' mining', ' tools', ' can', ' be', 'applied', ' without', ' having', ' to', ' consider', ' the', ' pedigree', ' of', ' the', ' document. For', 'harvesting', ' information', ' from', ' a', ' document', ',', ' it', ' is', ' irrelevant', ' what', ' editor', 'was', ' used', ' to', ' create', ' it', ' or', ' ', 'what the', ' original', ' format', ' was. ', 'The', ' sof', 'tware', '\\x0c20', '2. From', ' ', 'Textual', ' ', 'Information', ' to', ' ', 'Numerical', ' ', 'Vectors', 'tools', ' need', ' to read', ' data', ' just', ' in', ' one', ' format', ',', ' and', ' not', ' in', ' the', ' many', ' different', 'formats', ' they', ' came', ' in', ' originally', '.', '2.3', ' Tokenization', 'Assume', ' the', ' document', ' collection', ' is', ' in', ' ', 'XML', ' ', 'format', ' and', ' we', ' are', ' ready', 'to', ' examine', ' the', ' unstructured', ' text', ' to', ' identify', ' use', 'ful', ' features. ', 'The', ' ', 'ﬁrst', 'step', ' in', ' hand', 'ling', ' text', ' is', ' to', ' break', ' ', 'the', ' stream', ' ', 'of', ' characters', ' into', ' words', 'or', ',', ' more', ' precisely', ',', ' tokens. This', ' is', ' fundamental', ' to', ' further', ' analysis', '.', 'Without', ' identifying', ' the', ' tokens', ',', ' it', ' is', ' d', 'if', 'ﬁcult', ' to', ' imagine', ' extracting', 'higher-level', ' information', ' from', ' the', ' document. Each', ' token', ' is', ' an', ' instance', 'of', ' a type', ',', ' so', ' the', ' number', ' of', ' tokens', ' is', ' much', ' higher', ' ', 'than', ' ', 'the', ' number', ' of', 'types. As', ' an', ' example', ',', ' in', ' the', ' previous sentence', ' ', 'there', ' are', ' two', ' tokens', 'spelled', ' “', 'the.” These', ' are', ' both instances', ' ', 'of', ' a', ' type', ' ', '“the', ',', '” which', ' occurs', 'twice', ' in', ' the', ' sentence. Properly', ' speaking', ',', ' one', ' should', ' always', ' refer to', 'the', ' frequency', ' of', ' occurrence', ' of', ' a type', ',', ' but', ' loose', ' usage', ' also', ' talks', ' about', 'the', ' frequen', 'cy', ' of', ' a', ' token. ', 'Breaking', ' a', ' stream', ' of', ' characters', ' into', ' tokens', 'is', ' trivial', ' for a', ' person', ' familiar', ' with', ' ', 'the', ' language', ' structure. A com', 'puter', 'program', ',', ' though', ',', ' being', ' linguistical', 'ly', ' chal', 'lenged', ',', ' would', ' ', 'ﬁnd', ' the', ' task', 'more', ' complicated. ', 'The', ' reason', ' is', ' that', ' certain', ' characters', ' are', ' sometimes', 'token', ' delimiters', ' and', ' sometimes', ' not', ',', ' depending', ' on', ' the', ' application. ', 'The', 'characters', ' space', ',', ' tab', ',', ' and', ' newline', ' we', ' assume', ' are', ' always', ' delimiters', 'and', ' are', ' not counted', ' as', ' tokens. ', 'They', ' are', ' ', 'often', ' collectively', ' called', ' white', 'space. The', ' ', 'characters', ' ', '(', ' )', ' < >', ' ', '!', ' ', '?', ' \"', ' are', ' always', ' delimiters', ' and', ' may', ' also', ' be', 'tokens. The', ' characters', ' . ', ',', ' ', ':', ' - ’ may', ' or may', ' not', ' be', ' delimiters', ',', ' depending', 'on', ' ', 'their', ' environment', '.', 'A period', ',', ' comma', ',', ' or', ' colon', ' between', ' numbers', ' would', ' not', ' normally', ' be', 'considered', ' a delimiter', ' but', ' rather', ' part', ' ', 'of', ' the', ' number. ', 'Any', ' other comma', 'or', ' colon', ' is a delimiter', ' and', ' may', ' be', ' a', ' token. A period', ' can', ' be', ' part', ' of', ' an', 'abbreviation', ' ', '(', 'e.g.', ',', ' if', ' it', ' has', ' a', ' capital', ' letter', ' on', ' both', ' sides', ')', '. It', ' can', ' also', ' be', 'part', ' of', ' an abbreviation', ' when', ' followed', ' by a', ' space', ' ', '(', 'e.g.', ',', ' Dr', '.). However', ',', 'some', ' of', ' these', ' are', ' really', ' ends', ' of', ' sentences. The', ' problem', ' of', ' detecting', 'when', ' a', ' period', ' is an', ' end', ' of', ' sentence', ' and', ' when', ' it', ' is', ' not', ' will', ' be', ' discussed', 'later. ', 'For the', ' purposes', ' of', ' tokenization', ',', ' it', ' is', ' probably', ' best', ' to', ' treat', ' any', 'ambiguous', ' period', ' as', ' a', ' word', ' delimiter', ' and', ' also', ' as', ' a', ' token', '.', 'The', ' apostrophe', ' also', ' has', ' a', ' number', ' of', ' uses. When', ' preceded', ' and', 'followed', ' by', ' nondelimiters', ',', ' it', ' should', ' be', ' treated', ' as', ' part', ' of', ' ', 'the', ' cur', '-', 'rent', ' token', ' ', '(', 'e.g.', ',', ' isn', '’t', ' or D’angelo)', '. When', ' followed', ' by', ' an', ' unambiguous', 'terminator', ',', ' it', ' might', ' be', ' a closing', ' internal', ' quote', ' or might', ' indicate', ' a', 'possessive', ' ', '(', 'e.g.', ',', ' Tess', '’). An apostrophe', ' preceded', ' by', ' a', ' termina', 'tor', ' is', '\\x0c2.4 ', 'Lemmatization', '21', 'unambiguously', ' the', ' beginning', ' of', ' an', ' internal', ' quote', ',', ' so', ' it', ' is', ' possible', 'to', ' distinguish', ' the', ' two', ' cases', ' by', ' kee', 'ping', ' track of', ' opening', ' and', ' closing', 'internal', ' quotes', '.', 'A', ' dash', ' is', ' a', ' terminator', ' and', ' a token', ' if', ' preceded', ' or', ' followed', ' by', ' another', 'dash. A', ' dash between', ' two', ' numbers', ' might', ' be', ' a', ' subtraction', ' symbol', ' or a', 'separator', ' ', '(', 'e.g.', ',', ' 555-1212', ' as a tele', 'phone', ' number). It', ' is', ' probably', ' best', ' to', 'treat', ' a', ' dash not', ' adjacent', ' to', ' another', ' dash', ' as', ' a', ' terminator', ' and', ' a token', ',', 'but', ' in', ' some', ' applications', ' it might', ' be', ' better', ' to', ' treat', ' the', ' dash', ',', ' except in', 'the', ' dou', 'ble', ' dash', ' case', ',', ' as', ' simply', ' a', ' character', '.', 'An', ' exam', 'ple', ' ', 'of', ' pseudocode', ' for tokenization', ' is', ' shown', ' in', ' ', 'Figure', ' 2.2. A', 'version', ' ', 'of', ' this', ' is', ' available', ' in', ' ', 'the', ' accompanying', ' software', '.', 'To', ' get', ' the', ' best', ' possible', ' features', ',', ' one', ' should', ' always', ' customize', ' the', 'tokenizer', ' for the', ' available', ' text', '—otherwise', ' extra', ' work', ' may', ' be', ' re', '-', 'quired', ' after', ' the', ' tokens', ' are', ' obtained. ', 'The', ' reader', ' should', ' note', ' that', 'the', ' tokenization', ' process', ' is', ' language-dependent. ', 'We', ',', ' of', ' course', ',', ' focus', 'on', ' documents', ' in ', 'English. For', ' other', ' languages', ',', ' although', ' ', 'the', ' general', 'principles', ' will', ' be', ' the', ' same', ',', ' the', ' details', ' will', ' differ', '.', '2.4 ', 'Lemmatization', 'Once', ' a', ' character', ' stream', ' has', ' been', ' segmented', ' into', ' a', ' sequence', ' of', ' tokens', ',', 'the', ' next', ' possible', ' step', ' is', ' to conve', 'rt', ' each', ' ', 'of', ' ', 'the', ' tokens', ' to', ' a', ' standard', 'form', ',', ' a', ' process', ' usually', ' referred', ' to', ' as', ' stemming', ' or lemmatization', '.', 'Whe', 'ther', ' or not this', ' step', ' is', ' necessary', ' is', ' application-dependent. For', 'the', ' purpose', ' ', 'of', ' document', ' classi', 'ﬁcation', ',', ' stemming', ' can', ' provide', ' a small', 'positive', ' bene', 'ﬁt', ' in', ' some', ' cases. Notice', ' that', ' one', ' ef', 'fect', ' of', ' stemming', ' is', ' to', 'reduce', ' the', ' number', ' of', ' distinct types', ' in', ' a', ' text', ' corpus', ' and to', ' increase', 'the', ' frequen', 'cy of', ' occurren', 'ce', ' of', ' some', ' individual', ' types. For', ' example', ',', ' in', 'the', ' previous', ' sentence', ',', ' the', ' two', ' inst', 'ances', ' of', ' “types', '” would', ' be', ' ', 'reduced', 'to', ' the', ' stem', ' “type', '”', ' and', ' would', ' be', ' ', 'counted', ' as', ' instances', ' of', ' that', ' type', ',', 'along', ' with', ' instances', ' of', ' the', ' tokens', ' “type', '”', ' and', ' “typed.” ', 'For', ' classi', 'ﬁcation', 'algorithms', ' that', ' take', ' frequency', ' into', ' account', ',', ' this', ' can', ' sometimes', ' make', 'a difference. In', ' other', ' scenarios', ',', ' the', ' extra', ' processing', ' may', ' not', ' provide', 'any', ' signi', 'ﬁcant', ' gains', '.', '2.4.1', ' In', 'ﬂectional', ' Stemming', 'In English', ',', ' as', ' in', ' many', ' other', ' languages', ',', ' words', ' occur', ' in', ' text', ' in', ' more', 'than', ' one form. Any', ' native', ' ', 'English', ' speaker', ' will', ' agree', ' that', ' the', ' nouns', '“book', '” and', ' “books', '”', ' are', ' two', ' forms', ' of', ' ', 'the', ' same', ' word. Often', ',', ' but', ' not', '\\x0c22', '2. From', ' ', 'Textual', ' ', 'Information', ' to', ' ', 'Numerical', ' ', 'Vectors', 'Initialize', ':', 'Set', ' Stream', ' to', ' the', ' input', ' text', ' string', 'Set', ' current', 'Position', ' to', ' 0 and', ' internal', 'Quote', 'Flag', ' to', ' false', 'Set', ' delimiter', 'Set', ' to', ' ', '’', ',', '.', ';', ':', '!', '?', '()', '<>', '+\"', '\\\\n\\\\t', ' space', 'Set white', 'Space', ' to ', '\\\\t', '\\\\n', ' space', 'Proce', 'dure', ' get', 'Next', 'Token', ':', 'L1', ':', ' cursor', ' ', ':', '= current', 'Position', ';', ' ch ', ':', '= charAt', '(', 'cursor)', ';', 'If', ' ch', ' = end', 'Of', 'Stream', ' then', ' return', ' null', ';', ' endif', 'L2', ':', ' while', ' ch is not', ' end', 'Of', 'Stream nor', ' instance', 'Of', '(', 'delimiter', 'Set)', ' do', 'increment', ' cursor', ' by', ' 1', ';', ' ch ', ':', '= charAt', '(', 'cursor)', ';', 'endwhile', 'If ch', ' = end', 'OfStream', ' then', 'If', ' cursor', ' = current', 'Position', ' then', ' return', ' null', ';', ' endif', 'endif', 'If ch', ' is', ' white', 'Space', ' then', 'If current', 'Position', ' = cursor', ' then', 'increment', ' current', 'Position', ' by', ' 1', ' and', ' goto', ' L1', ';', 'else', 'Token', ' ', ':', '= substring', '(', 'Stream', ',', 'current', 'Position', ',', 'cursor-1)', ';', 'current', 'Position', ' ', ':', '= cursor+1', ';', ' return', ' ', 'Token', ';', 'endif', 'endif', 'If ch', ' = ’ then', 'If char', 'At', '(', 'cursor-1)', ' = instance', 'Of', '(', 'delimiter', 'Set)', ' then', 'internal', 'Quote', 'Flag', ' ', ':', '= true', ';', ' increment', ' current', 'Position', ' by', ' 1', ';', ' goto L1', ';', 'endif', 'If char', 'At', '(', 'cursor', '+1)', ' ', '!', '= instanceof', '(', 'delimiter', 'Set)', ' then', 'increment', ' cursor', ' by', ' 1', ';', ' ch ', ':', '= charAt', '(', 'cursor)', ';', ' goto L2', ';', 'else', 'if', ' internal', 'Quote', 'Flag', ' = true', ' then', 'Token', ' ', ':', '= substring', '(', 'Stream', ',', 'current', 'Position', ',', 'cursor-1)', ';', 'internal', 'Quote', 'Flag', ' ', ':', '= false', ';', 'else', 'Token', ' ', ':', '= substring', '(', 'Stream', ',', 'current', 'Position', ',', 'cursor)', ';', 'endif', 'current', 'Position', ' ', ':', '= cursor+1', ';', ' return', ' ', 'Token', ';', 'endif', 'If cursor', ' = current', 'Position', ' then', 'Token', ' ', ':', '= ch', ';', ' current', 'Position', ' ', ':', '= cursor+1', ';', 'else', 'Token', ' ', ':', '= substring', '(', 'Stream', ',', 'current', 'Position', ',', 'cursor-1)', ';', 'current', 'Position', ' ', ':', '= cursor', ';', 'endif', 'return', ' ', 'Token', ';', 'endproce', 'dure', 'Figure', ' ', '2.2. Tokenization', ' ', 'Algorithm', '\\x0c2.4 ', 'Lemmatization', '23', 'al', 'ways', ',', ' it', ' is', ' advantage', 'ous', ' to', ' eliminate', ' ', 'this', ' kind', ' ', 'of', ' variation', ' before', 'further', ' processing', ' ', '(', 'i.e.', ',', ' to', ' normalize', ' both', ' words', ' to', ' ', 'the', ' single', ' form', '“book', '”). ', 'When', ' the', ' normalization', ' is', ' con', 'ﬁned', ' to regularizing', ' grammat-', 'ical', ' variants', ' such', ' as', ' singular/plural', ' and', ' present/past', ',', ' the', ' process', ' is', 'called', ' “in', 'ﬂectional', ' stemming.” In', ' linguistic', ' terminology', ',', ' this', ' is', ' called', '“morphological', ' analy', 'sis.” In', ' some', ' languages', ',', ' for', ' example', ' ', 'Spanish', ',', 'morphological', ' analy', 'sis', ' is', ' comparative', 'ly', ' simple. For', ' a', ' language', ' such as', 'English', ',', ' with', ' many', ' irregular word', ' forms', ' and', ' nonintuitive', ' spelling', ',', ' it', 'is', ' more', ' dif', 'ﬁcult. ', 'There', ' is', ' no', ' simple', ' rule', ',', ' for', ' example', ',', ' to', ' bring', ' toge', 'ther', '“seek', '” and', ' “sought.” ', 'Similarly', ',', ' the', ' stem', ' for “rebelled', '” is', ' “rebel', ',', '” but', ' the', 'stem', ' for', ' “belled', '” is', ' ', '“bell.” In', ' other', ' languages', ',', ' in', 'ﬂections', ' can', ' take', ' the', 'form', ' of', ' in', 'ﬁxing', ',', ' as', ',', ' in', ' the', ' ', 'German', ' “angeben', '” ', '(', 'declare)', ',', ' for which the', 'past', ' participle', ' is', ' ', '“ange', 'geben.”', 'Returning', ' to', ' ', 'English', ',', ' an', ' algorithm', ' for', ' in', 'ﬂectional', ' stemming', ' must', 'be', ' part rule-based', ' and', ' part dictionary-based. ', 'Any', ' stemming', ' algorithm', 'for', ' ', 'English', ' that', ' operates', ' only', ' on', ' tokens', ',', ' without', ' more', ' grammatical', 'information', ' such', ' as', ' part-of-speech', ',', ' will', ' make', ' some', ' mistakes', ' because', 'of', ' ambiguity. ', 'For', ' example', ',', ' is ', '“bored', '” the', ' adjective', ' as', ' in ', '“he', ' is', ' bored', '”', 'or', ' is', ' it', ' ', 'the', ' past tense', ' of', ' the', ' verb', ' ', '“bore', '”', '?', ' Furthermore', ',', ' is the', ' verb', '“bore', '”', ' an', ' instance', ' of', ' the', ' verb', ' ', '“bore', ' a', ' hole', ',', '”', ' or', ' is', ' it', ' the', ' past', ' tense', ' of', ' the', 'verb', ' “', 'bear', '”', '?', ' ', 'In', ' the', ' absence', ' of', ' some', ' often', ' complicated', ' disambiguation', 'process', ',', ' a', ' stemming', ' algorithm', ' should', ' probably', ' pick', ' ', 'the', ' most', ' frequent', 'choice. Pseudocode', ' for a', ' somewhat', ' simpli', 'ﬁed', ' in', 'ﬂectional', ' stemmer for', 'English', ' is', ' given', ' in', ' ', 'Figure', ' ', '2.3. ', 'Notice', ' how', ' ', 'the', ' algorithm', ' consists', ' of', 'rules', ' that', ' are', ' applied', ' in', ' sequence', ' until', ' one', ' of', ' them', ' is', ' satis', 'ﬁed. Also', 'notice', ' the', ' frequent', ' referrals', ' to', ' a', ' dictionary', ',', ' usually', ' referred', ' to', ' as', 'a stemming', ' dictionary. Although', ' ', 'the', ' in', 'ﬂectional', ' stemmer', ' is', ' not', ' ex', '-', 'pected', ' to', ' be', ' perfect', ',', ' it will', ' correctly', ' identify', ' quite', ' a signi', 'ﬁcant', ' number', 'of stems. An', ' in', 'ﬂectional', ' stemmer', ' is', ' available', ' with', ' ', 'the', ' accompanying', 'software', '.', '2.4.2', ' Stemming', ' to', ' a ', 'Root', 'Some', ' practitioners', ' have', ' felt that', ' normalization', ' more', ' aggressive', ' than', 'in', 'ﬂectional', ' stemming', ' is advantage', 'ous', ' for at least', ' some', ' text-processing', 'applications. The', ' intent', ' of', ' ', 'these', ' stemmers', ' is', ' to', ' reach', ' a', ' root', ' form', ' with', 'no', ' in', 'ﬂectional or', ' derivational', ' pre', 'ﬁxes', ' and', ' suf', 'ﬁxes. ', 'For', ' example', ',', ' “de', '-', 'normalization', '”', ' is', ' reduced', ' to', ' the', ' stem', ' ', '“norm.” ', 'The', ' end', ' re', 'sult', ' of', ' such', 'aggressive', ' stemming', ' is', ' to', ' reduce', ' the', ' number', ' of', ' types', ' in a', ' text', ' col', '-', 'lection', ' very', ' drastically', ',', ' there', 'by', ' making', ' distributional', ' statistics', ' more', 'reliable. Additional', 'ly', ',', ' words', ' with the', ' same', ' core', ' meaning', ' are', ' coalesced', ',', '\\x0c2', '4', '2. From', ' ', 'Textual', ' ', 'Information', ' to', ' ', 'Numerical', ' ', 'Vectors', 'Input', ':', ' a text', ' token', ' and', ' a', ' dictionary', 'Doubling', ' cons', 'onants', ':', ' b d g k m', ' n', ' p r l', ' t', 'Rules', ':', 'If token', ' leng', 'th', ' < 4 return', ' token', 'If token', ' is', ' number', ' return', ' token', 'If token', ' is', ' acronym', ' return', ' token', 'If token', ' in dictionary', ' return', ' the', ' stored', ' stem', 'If token', ' ends in', ' s', '’', 'strip', ' the', ' ’ and', ' return', ' stripped', ' token', 'If token', ' ends in', ' ’s', 'strip', ' the', ' ', '’s', ' and', ' return', ' stripped', ' token', 'If token', ' ends in', ' ', '“is', '”', ',', ' “us', '”', ',', ' or', ' “ss” return', ' token', 'If token', ' ends in', ' s', 'strip s', ',', ' che', 'ck', ' in', ' dictionary', ',', ' and', ' return', ' stripped', ' token', ' if', ' ', 'there', 'If token', ' ends', ' with es', 'strip', ' es', ',', ' che', 'ck', ' in', ' dictionary', ',', ' and', ' return', ' stripped', ' token', ' if', ' ', 'there', 'If token', ' ends in', ' ies', 'replace', ' ies', ' by y', ' and', ' return', ' changed', ' token', 'If token', ' ends in', ' s', 'strip', ' s', ' and', ' return', ' stripped', ' token', 'If token', ' doesn', '’t', ' end', ' with', ' ed', ' or', ' ing', ' return', ' token', 'If token', ' ends', ' with', ' ed', 'strip', ' ed', ',', ' check', ' in', ' dictionary', ' and', ' return', ' stripped', ' token', ' if', ' there', 'If token', ' ends in', ' ied', 'replace', ' ied', ' by y', ' and', ' return', ' changed', ' token', 'If token', ' ends', ' in', ' eed', 'remove', ' d and return', ' stripped', ' token', ' if', ' in', ' dictionary', 'If token', ' ends', ' with', ' ing', 'strip', ' ing', ' ', '(', 'if', ' leng', 'th >', ' 5)', ' and', ' return', ' stripped', ' token', ' if', ' in', ' dictionary', 'If token', ' ends with', ' ing', ' and', ' length', '// Now', ' we', ' have', ' SS', ',', ' the', ' stripped', ' stem', ',', ' without', ' ed', ' or', ' ing', ' and', ' it', '’s', '// not', ' in', ' ', 'the', ' dictionary', ' ', '(otherwise', ' algorithm', ' would', ' terminate', ')', 'If SS', ' ends', ' in', ' d', 'ou', 'bling', ' cons', 'onant', '5', ' return', ' token', '≤', 'strip ', 'ﬁnal', ' consonant', ' and', ' return', ' the', ' ', 'changed', ' SS if', ' in', ' dictionary', 'If', ' doubling', ' consonant', ' was', ' l', ' return', ' original', ' SS', 'If no', ' d', 'oubled', ' cons', 'onants', ' in', ' SS', 'add', ' e', ' and', ' return', ' changed', ' SS', ' ', 'if', ' in', ' dictionary', 'If SS', ' ends', ' in', ' c or z', ',', ' or', ' there', ' is', ' a g', ' or', ' l', ' before', ' ', 'the', ' ', 'ﬁnal', ' d', 'oubling', ' consonant', 'add', ' e', ' and', ' return', ' changed', ' SS', 'If SS', ' ends', ' in', ' any', ' consonant', ' that', ' is', ' preceded', ' by', ' a', ' single', ' vowel', 'add', ' e', ' and', ' return', ' changed', ' SS', 'return', ' SS', 'Figure', ' ', '2.3. In', 'ﬂectional', ' Stemming Algorithm', '\\x0c2.5 ', 'Vector ', 'Generation', ' for Prediction', '25', 'so', ' that', ' a', ' concept', ' such', ' as', ' ', '“apply', '”', ' has', ' only', ' one', ' stem', ',', ' although', ' the', ' text', 'may', ' have', ' “reapplied', '”', ',', ' “applications', '”', ',', ' etc. We', ' cannot', ' make', ' any', ' broad', 'recommendations', ' as', ' to', ' when', ' or', ' when', ' not', ' to', ' use', ' such', ' stemmers. The', 'use', 'fulness', ' of', ' stemming', ' is', ' very', ' much', ' application', '-dependent. ', 'When', ' in', 'doubt', ',', ' it', ' doesn', '’t hurt to', ' try', ' both ', 'with', ' and', ' without', ' stemming', ' if', ' one', ' has', 'the', ' resources', ' to', ' do', ' so', '.', '2.5', ' ', 'Vector ', 'Generation', ' for', ' ', 'Prediction', 'Consider', ' ', 'the', ' problem', ' ', 'of', ' categorizing', ' documents. ', 'The', ' ', 'characteristic', 'features', ' ', 'of', ' documents', ' are', ' ', 'the', ' tokens', ' or words', ' ', 'they', ' contain. ', 'So', ' without', 'any', ' deep', ' analysis', ' of', ' the', ' linguistic', ' content of', ' ', 'the', ' documents', ',', ' we can', 'choose', ' to', ' describe', ' each', ' document', ' by', ' features', ' that', ' represent', ' ', 'the', ' most', 'frequent', ' tokens. Figure', ' ', '2.4', ' describes', ' this', ' process. A version', ' of', ' this', 'process', ' is', ' available', ' in', ' ', 'the', ' accompanying', ' sof', 'tware', '.', 'The', ' ', 'collective', ' set', ' of', ' features', ' is', ' typically', ' called', ' a', ' dictionary. The', 'tokens', ' or words', ' in', ' the', ' dictionary', ' form', ' ', 'the', ' basis', ' for', ' creating', ' a', ' spread-', 'sheet', ' of', ' numeric', ' data', ' corresponding', ' to ', 'the', ' document collection. Each', 'row', ' is', ' a document', ',', ' and', ' each', ' column', ' represents', ' a', ' feature. Thus', ',', ' a cell', 'in', ' the', ' spreadsheet', ' is', ' a', ' measurement', ' of', ' a', ' ', 'feature', ' ', '(', 'corres', 'ponding', ' to', 'the', ' ', 'column)', ' for a', ' document', ' ', '(', 'corres', 'ponding to', ' a', ' row). We', ' will', ' soon', 'introduce', ' the', ' predic', 'tive', ' methods', ' that', ' learn', ' ', 'from', ' such', ' data. ', 'But let', ' us', 'ﬁrst', ' explore', ' ', 'the', ' various', ' nuances', ' of', ' ', 'this', ' data', ' model', ' and', ' how', ' it', ' might', 'in', 'ﬂuence', ' ', 'the', ' learning', ' methods. ', 'In', ' the', ' most', ' basic', ' model', ' of', ' such', ' data', ',', 'we simply', ' check for the', ' presence', ' or', ' absen', 'ce', ' of', ' words', ',', ' and', ' the', ' cell', 'entries', ' are', ' binary', ' entries', ' corres', 'ponding', ' to', ' a', ' document', ' and', ' a', ' word', '.', 'The', ' dictiona', 'ry of', ' words', ' covers', ' all', ' ', 'the', ' possibilities', ' and corresponds', 'to', ' the', ' number of', ' columns', ' in', ' the', ' spre', 'adsheet. The', ' cells', ' will', ' all have', 'ones', ' or zeros', ',', ' depending', ' on whe', 'ther', ' the', ' words', ' were', ' encountered', ' in', 'the', ' document', '.', 'If a', ' learning', ' me', 'thod', ' can', ' deal', ' with', ' ', 'the', ' high dimensions', ' of', ' such', 'a global', ' dictionary', ',', ' this', ' simple', ' model', ' of', ' data', ' can', ' be', ' very', ' ', 'effective', '.', 'Checking', ' for words', ' is', ' simple', ' because', ' we', ' do', ' not', ' actually', ' check', ' each', 'word', ' in', ' the', ' dictionary. We', ' build', ' a', ' hash', ' table', ' ', 'of', ' the', ' dictiona', 'ry', ' words', 'and', ' see', ' whe', 'ther', ' the', ' document', '’s', ' words', ' are', ' in ', 'the', ' hash', ' table. ', 'Large', 'sam', 'ples', ' of', ' digital', ' documents', ' are', ' readily', ' available. ', 'This', ' gives', ' us con', 'ﬁ', '-', 'dence', ' ', 'that many', ' variations', ' and', ' combinations', ' of', ' words', ' will', ' show', ' up', ' in', 'the', ' sample. This', ' expectation', ' argues', ' for spending', ' less', ' computational', 'time', ' preparing', ' the', ' data', ' to', ' look', ' for similar words', ' or', ' remove', ' weak', '\\x0c26', '2. From', ' ', 'Textual', ' ', 'Information', ' to', ' ', 'Numerical', ' ', 'Vectors', 'Input', ':', 'ts', ',', ' all', ' the', ' tokens', ' in', ' the', ' document', ' collection', 'k', ',', ' the', ' number', ' of', ' features', ' desired', 'Output', ':', 'fs', ',', ' a set', ' of', ' k', ' features', 'Initialize', ':', 'hs', ' ', ':', '= empty', ' hashtable', 'for', ' each', ' tok', ' in', ' ts', ' do', 'If hs', ' contains', ' tok', ' then', 'i ', ':', '= value', ' of', ' tok', ' in', ' hs', 'incre', 'ment', ' i', ' by', ' 1', 'else', 'i ', ':', '= 1', 'endif', 'store', ' i', ' as', ' value', ' of', ' tok', ' in', ' hs', 'endfor', 'sk ', ':', '= keys', ' in', ' hs', ' sorted', ' by', ' decreasing', ' value', 'fs ', ':', '= top', ' k', ' keys', ' in sk', 'out', 'put', ' fs', 'Figure', ' ', '2.4. ', 'Generating', ' ', 'Features', ' from', ' ', 'Tokens', 'words. ', 'Let', ' the', ' speedy', ' com', 'puter', ' ', 'ﬁnd', ' its', ' own', ' way', ' during', ' the', ' learning', 'process', '.', 'But', ',', ' in', ' many', ' circumstances', ',', ' we', ' may', ' want to', ' work', ' with', ' a', ' smalle', 'r dic', '-', 'tionary. The', ' sample', ' may', ' be', ' relatively', ' small', ',', ' or a', ' large', ' dictionary', ' may', 'be', ' unwieldy. In', ' such', ' cases', ',', ' we', ' might', ' try', ' to', ' reduce', ' the', ' size', ' of', ' the', ' dic', '-', 'tionary', ' by', ' various', ' transformations', ' of', ' a', ' dictiona', 'ry', ' and', ' its constituent', 'words. ', 'Depend', 'ing', ' on', ' the', ' learning', ' method', ',', ' many', ' of', ' ', 'these', ' transforma', '-', 'tions', ' can', ' improve', ' predictive', ' performance. Table', ' 2.1', ' lists', ' some', ' of', ' the', 'trans', 'formations', ' that', ' can', ' be', ' per', 'formed', '.', 'If prediction', ' is', ' our', ' goal', ',', ' we', ' need', ' one', ' more', ' column', ' for', ' the', ' correct', 'answer ', '(', 'or', ' class)', ' for', ' each document. In', ' preparing', ' data', ' for', ' a', ' learning', 'method', ',', ' this', ' information', ' will', ' be available', ' from', ' ', 'the', ' document', ' labels', '.', 'Our', ' labels', ' are', ' generally', ' binary', ',', ' and', ' the', ' smaller', ' class', ' is', ' al', 'most', ' always', 'the', ' one', ' of', ' interest. ', 'Instead', ' of', ' generating', ' a', ' global', ' dictionary', ' for', ' bo', 'th', 'classes', ',', ' we', ' may', ' consid', 'er', ' only', ' words', ' found', ' in', ' the', ' ', 'class', ' ', 'that', ' we', ' are', 'trying', ' to', ' predict. If', ' this', ' class', ' is', ' far', ' smaller', ' than', ' the', ' negative', ' class', ',', 'which', ' is', ' typical', ',', ' such', ' a', ' local dictionary', ' will', ' be', ' far smaller', ' ', 'than', ' the', 'global', ' dictionary', '.', 'Another', ' obvious', ' reduction', ' in', ' dictionary', ' size', ' is', ' to compile', ' a', ' list', ' of', 'stopwords', ' and remove', ' them', ' ', 'from', ' ', 'the', ' dictionary. ', 'These', ' are', ' words', ' that', '\\x0c2.5 ', 'Vector ', 'Generation', ' for Prediction', '27', 'Table', ' ', '2.1. Dictionary', ' ', 'Reduction', ' ', 'Techniques', 'Local', ' Dictionary', 'Stopwords', 'Frequent', ' ', 'Words', 'Feature', ' ', 'Selection', 'Token', ' Reduction', ':', ' Stemming', ',', ' Synonyms', 'al', 'most', ' never have', ' any', ' predictive', ' capability', ',', ' such', ' as', ' artic', 'les', ' a', ' and', 'the', ' and', ' pronouns', ' such', ' as', ' it', ' and', ' ', 'they. ', 'These', ' common', ' words', ' can', ' be', 'discarded', ' before', ' the', ' feature', ' generation', ' process', ',', ' but', ' it', '’s', ' more', ' effective', 'to', ' generate', ' the', ' features', ' ﬁrst', ',', ' apply', ' all', ' the', ' other', ' trans', 'formations', ',', ' and', 'at', ' the', ' very', ' last stage', ' reject the', ' ones', ' that corres', 'pond', ' to', ' stop', 'words', '.', 'Frequen', 'cy', ' information', ' on', ' the', ' word', ' counts', ' can', ' be', ' quite', ' use', 'ful', ' in', 'reducing', ' dictionary', ' size', ' and', ' can', ' sometimes', ' improve', ' predictive', ' perfor', '-', 'mance', ' for some', ' me', 'thods. The', ' most', ' frequent', ' words', ' are', ' often', ' stop', 'words', 'and', ' can', ' be', ' deleted. The', ' remaining', ' most', ' frequently', ' used', ' words', ' are', 'of', 'ten', ' the', ' important words', ' that', ' should', ' remain', ' in', ' a', ' local', ' dictionary', '.', 'The', ' very', ' rare', ' words', ' are', ' often', ' typos', ' and', ' can', ' also', ' be', ' dismissed. For', 'some', ' learning', ' methods', ',', ' a', ' local', ' dictionary', ' of', ' the', ' most', ' frequent', ' words', ',', 'perhaps', ' less', ' than', ' 200', ',', ' can', ' be', ' surprisingly', ' effective', '.', 'An', ' alternative', ' approach to', ' local', ' dictiona', 'ry', ' generation', ' is', ' to', ' generate', 'a', ' global', ' dictionary', ' from', ' all', ' documents', ' in the', ' ', 'collection. Special', ' feature', 'selection routines', ' will', ' attempt', ' to', ' select', ' a', ' subset', ' of', ' words', ' that', ' appear', 'to', ' have', ' the', ' greatest', ' potential', ' for', ' prediction. These', ' selection', ' me', 'thods', 'are', ' often', ' complicated', ' and', ' independent', ' of', ' ', 'the', ' prediction', ' method. Gen-', 'erally', ',', ' we', ' do', ' not', ' use', ' them', ' and', ' rely', ' on', ' just', ' frequen', 'cy', ' in', 'formation', ',', 'which', ' is', ' quite', ' easy', ' to', ' determine. ', 'Any', ' of', ' ', 'the', ' feature', ' selection', ' meth-', 'ods', ' that have', ' been', ' used', ' in alternative', ' statistical', ' or machine-learning', 'settings', ' may be', ' tried. Many', ' of', ' ', 'these', ' have', ' been', ' deve', 'loped', ' for', ' real', 'variables', ' and', ' without', ' an', ' emphasis', ' on', ' discrete', ' or', ' binary', ' attributes', '.', 'Some', ' text-speciﬁc methods', ' will', ' be', ' described', ' later', ' on', ' in', ' ', 'Section', ' 2.5.3', ',', 'but', ' many', ' ', 'of', ' the', ' prediction', ' methods', ' have', ' alrea', 'dy', ' been', ' adjusted', ' for', 'text', ' to', ' deal', ' with', ' larger', ' dictionaries', ' rather', ' than', ' repeated', 'ly', ' generating', 'smaller', ' dictionaries. If', ' many', ' classes', ' must', ' be', ' determined', ',', ' then', ' the', 'generation', ' of', ' a smaller', ' dictionary', ' must', ' be', ' repeated', ' for each', ' predic-', 'tion', ' problem. For', ' example', ',', ' if', ' we', ' have', ' 100 topics', ' to', ' categorize', ',', ' then', 'we', ' have', ' 100 binary', ' prediction', ' problems', ' to solve. ', 'Our choices', ' are', ' 100', 'small', ' dictionaries', ' or', ' one', ' big one. Typically', ',', ' the', ' vectors', ' implied', ' by', ' a', 'spread', 'sheet', ' model', ' will', ' also', ' be', ' regene', 'rated', ' ', 'to', ' correspond', ' to', ' ', 'the', ' small', 'dictionary', '.', '\\x0c28', '2. From', ' ', 'Textual', ' ', 'Information', ' to', ' ', 'Numerical', ' ', 'Vectors', 'Instead', ' of', ' placing', ' every', ' possible', ' word', ' in', ' the', ' dictionary', ',', ' we might', ' fol', '-', 'low', ' ', 'the', ' path ', 'of the', ' printed', ' dictionary', ' and', ' avoid', ' storing', ' eve', 'ry', ' variation', 'of', ' the', ' same', ' word. ', 'The', ' rationale', ' for ', 'this', ' is', ' that', ' all', ' ', 'the', ' variants', ' really', 'refer to', ' the', ' same', ' concept. ', 'There', ' is', ' no', ' need', ' for', ' singular', ' and', ' plural', '.', 'Many', ' verbs', ' can be', ' stored', ' in', ' their', ' stem', ' form. ', 'Extending', ' the', ' concept', ',', ' w', 'e', 'can', ' also', ' map', ' synonyms', ' to', ' the', ' same', ' token. Of', ' course', ',', ' this', ' adds', ' a', ' layer', 'of', ' complexity to', ' the', ' processing', ' of', ' text. The', ' gains', ' in', ' predictive', ' perfor-', 'mance', ' are', ' relative', 'ly', ' modest', ',', ' but', ' the', ' dictionary', ' size', ' will', ' obviously', ' be', 'reduced. Stemming', ' can', ' occasional', 'ly', ' be', ' harm', 'ful', ' for', ' some', ' words. If', ' we', 'apply', ' a', ' universal', ' procedure', ' that', ' e', 'ffectively', ' trims', ' words', ' to', ' ', 'their', ' root', 'form', ',', ' we', ' will', ' encounter', ' occasions', ' where', ' a', ' subtle', ' difference', ' in', ' meaning', 'is', ' missed. The', ' words', ' ', '“exit', '” and', ' ', '“exiting', '”', ' may', ' appear', ' to', ' have', ' identical', 'roots', ',', ' but', ' in', ' the', ' context', ' of', ' programming', ' and', ' error messages', ',', ' they', 'may', ' have', ' different', ' meaning', 's. Overall', ',', ' stemming', ' will', ' achieve', ' a', ' large', 'reduction', ' in dictionary', ' size', ' and', ' is', ' modestly', ' bene', 'ﬁcial', ' for', ' predictive', 'performance', ' when', ' using', ' a', ' smaller', ' dictionary', '.', 'In', ' general', ',', ' the', ' smaller the', ' dictionary', ',', ' the', ' more', ' intelligence', ' in', ' its', 'composition', ' is needed', ' to', ' capture', ' the', ' most', ' and', ' best', ' words. ', 'The', ' use', ' of', 'tokens', ' and', ' stemming', ' are', ' examples', ' of', ' hel', 'pful', ' procedures', ' in composing', 'smaller', ' dictionaries. All', ' ', 'these', ' efforts', ' will', ' pay', ' off', ' in', ' improved', ' manage', '-', 'ability', ' of', ' learning', ' and', ' perhaps', ' improved', ' accuracy. If', ' nothing', ' else', ' is', 'gained', ',', ' learning', ' can', ' proceed', ' more', ' rapid', 'ly', ' with', ' smaller', ' dictionaries', '.', 'Once', ' the', ' set of', ' features', ' has', ' been', ' determined', ',', ' the', ' document', ' col', '-', 'lection', ' can', ' be', ' converted', ' to', ' spread', 'sheet', ' format. ', 'Figure', ' ', '2.5', ' shows', ' an', 'exam', 'ple', ' of', ' how this', ' can', ' be', ' done', ' for binary', ' features. An', ' implementa', '-', 'tion', ' ', 'of', ' a', ' similar', ' algorithm', ' is', ' available', ' in', ' the', ' accompanying', ' sof', 'tware', '.', 'Each', ' column', ' in the', ' spread', 'sheet', ' corres', 'ponds', ' to', ' a', ' feature. For', ' inter', '-', 'pretability', ',', ' we', ' will', ' need', ' to', ' keep', ' the', ' list of', ' features', ' to', ' tran', 'slate', ' from', 'column', ' number to', ' feature', ' name. And', ',', ' of', ' course', ',', ' we', ' will', ' still', ' need', ' the', 'document', ' collection', ' to be', ' able', ' to', ' refer back', ' to the', ' original', ' documents', 'from', ' the', ' rows', '.', 'We', ' have', ' presented', ' a', ' model', ' ', 'of', ' data', ' for predictive', ' text mining', ' in', 'terms', ' ', 'of', ' a', ' spread', 'sheet', ' that', ' is', ' populated', ' by', ' ones', ' or', ' zeros. ', 'These', 'cells represent the', ' presence', ' of', ' the', ' dictionary', '’s', ' words', ' in', ' a', ' docu', '-', 'ment', ' collection. To', ' achieve', ' the', ' best', ' predictive', ' accuracy', ',', ' we might', 'consider', ' additional', ' trans', 'formations', ' from', ' ', 'this', ' representation. ', 'Table', '2.2', ' lists', ' three', ' different', ' trans', 'for', 'mations', ' that', ' may', ' improve', ' predictive', 'capabilities', '.', 'Word', ' pairs', ' and', ' collo', 'cations', ' are', ' sim', 'ple', ' examples', ' of', ' multiword', ' fea', '-', 'tures', ' discussed', ' in', ' more', ' detail', ' in', ' ', 'Section', ' 2.5.1. They', ' serve', ' to', ' increase', '\\x0c2.5 ', 'Vector ', 'Generation', ' for Prediction', '29', 'Input', ':', 'fs', ',', ' a set', ' of', ' k', ' features', 'dc', ',', ' a', ' collection', ' of', ' n', ' documents', 'Output', ':', ' ss', ',', ' a spre', 'ad', 'sheet', ' with', ' n', ' rows', ' and', ' k', ' columns', 'Initialize', ':', ' i ', ':', '= 1', 'for', ' each', ' document', ' d', ' in', ' dc', ',', ' do', 'j ', ':', '= 1', 'for', ' each', ' feature', ' f', ' in', ' fs', ',', ' do', 'm', ' ', ':', '= number', ' ', 'of', ' occurrences', ' of', ' f in d', 'if ', '(', 'm >', ' 0)', ' then ss', '(', 'row=i', ',', ' col', '=j)', ' ', ':', '= 1', ';', 'else ss', '(', 'row', '=', 'i', ',', ' col', '=j)', ' ', ':', '= 0 ', ';', 'endif', 'increment', ' j', ' by', ' 1', 'endfor', 'incre', 'ment', ' i', ' by', ' 1', 'endfor', 'out', 'put', ' ss', 'Figure', ' ', '2.5. Converting', ' ', 'Documents', ' to a ', 'Spreadsheet', 'Table', ' ', '2.2. Dictionary', ' ', 'Feature', ' ', 'Trans', 'formations', 'Word', ' Pairs', ',', ' Collocations', 'Frequencies', 'tf-idf', 'the', ' size', ' ', 'of', ' the', ' dictionary', ' but', ' can', ' improve', ' predictive', ' performance', ' in', 'certain', ' scenarios', '.', 'Instead', ' of', ' zeros', ' or', ' ones', ' as', ' entries', ' in', ' the', ' cells', ' of', ' the', ' spreadsheet', ',', 'the', ' actual', ' frequency', ' of', ' occurrence', ' of', ' the', ' word', ' could', ' be', ' used. If', ' a', ' word', 'occurs', ' ten', ' times', ' in', ' a', ' document', ',', ' this', ' count', ' would', ' be', ' entered', ' in', ' the', 'cell. We', ' have', ' all', ' the', ' information', ' of', ' a', ' bina', 'ry', ' representation', ',', ' and', ' we', 'have', ' some', ' additional', ' information', ' to', ' contrast', ' with', ' other', ' documents', '.', 'For some', ' learning', ' methods', ',', ' the', ' count does', ' give', ' a', ' slightly', ' better', ' result', '.', 'It', ' also', ' may', ' lead', ' to', ' more', ' compact solutions', ' because', ' it', ' includes', ' the', ' same', 'solution', ' space', ' as', ' the', ' binary', ' data', ' model', ',', ' yet', ' the', ' additional', ' frequency', 'in', 'formation', ' may', ' yield', ' a simpler', ' solution. This', ' is', ' especially', ' true', ' of', 'some', ' learning', ' methods', ' whose', ' solutions', ' use', ' only', ' a', ' small', ' subset', ' of', ' the', 'dictionary', ' words. Overall', ',', ' the', ' frequencies', ' are', ' helpful', ' in', ' prediction', ' but', 'add', ' complexity to', ' the', ' proposed', ' solutions. One', ' compromise', ' that works', 'quite', ' well', ' is to have', ' a three-valued', ' system', ' for cell', ' entries', ':', ' a one', ' or', ' zero', '\\x0c30', '2. From', ' ', 'Textual', ' ', 'Information', ' to', ' ', 'Numerical', ' ', 'Vectors', 'Table', ' ', '2.3. ', 'Thresholding', ' ', 'Frequencies', ' to ', 'Three', ' ', 'Values', '0 - word', ' did', ' not', ' occur', '1 - word', ' occurred', ' once', '2 - word', ' occurred', ' 2', ' or more', ' times', 'as', ' in', ' the', ' binary', ' representation', ',', ' with', ' ', 'the', ' additional', ' possibility', ' of', ' a', ' ', '2', '.', 'Table', ' ', '2.3', ' lists', ' the', ' ', 'three', ' possibilities', ',', ' where', ' we', ' map', ' all', ' occurrences', ' of', 'more', ' than', ' two', ' times', ' into', ' a', ' maximum', ' value', ' ', 'of', ' 2. Such', ' a', ' scheme', ' seems', 'to', ' capture', ' much', ' of', ' the', ' added', ' value', ' of', ' frequency', ' information', ' without', 'adding', ' much complexity', ' to ', 'the', ' model. Another', ' variant', ' involves', ' zeroing', 'values', ' below a certain', ' threshold', ' on', ' ', 'the', ' plausible', ' grounds', ' that tokens', 'should', ' have', ' a minimum', ' frequen', 'cy', ' be', 'fore', ' being', ' considered', ' of', ' any', ' use', '.', 'This', ' can', ' reduce', ' the', ' complexity', ' of', ' the', ' spreadsheet', ' signi', 'ﬁcantly', ' and', 'might', ' be', ' a', ' necessity', ' for some', ' data-mining', ' algorith', 'ms. Besides', ' simple', 'thresholding', ',', ' there', ' are', ' a', ' variety', ' of', ' more', ' sophisticated', ' ', 'methods', ' to re', '-', 'duce', ' the', ' spread', 'sheet', ' complexity', ' such', ' as', ' ', 'the', ' use', ' of', ' chi-square', ',', ' mutual', 'information', ',', ' odds', ' ratio', ' and', ' others. Mutual', ' information', ' can', ' be', ' helpful', 'when', ' considering', ' multiword', ' features', '.', 'The', ' next', ' step', ' beyond', ' counting', ' the', ' frequen', 'cy', ' of', ' a', ' word', ' in', ' a', ' document', 'is', ' to', ' modify', ' ', 'the', ' ', 'count', ' by', ' ', 'the', ' perceived', ' importance', ' of', ' that word. The', 'well-known', ' tf-idf', ' formulation', ' has', ' been', ' used', ' to', ' com', 'pute', ' weightings', ' or', 'scores', ' for words. Once', ' again', ',', ' the', ' values', ' will', ' be', ' positive', ' numbers', ' so', 'that we', ' capture', ' the', ' presence', ' or', ' absence', ' ', 'of', ' ', 'the', ' word', ' in', ' a', ' document', '.', 'In Equation', ' ', '(', '2.1', ')', ',', ' we', ' see', ' that the', ' tf-idf', ' weight', ' assigned', ' to word', ' j', ' is', 'the', ' term', ' frequency', ' ', '(', 'i.e.', ',', ' the word', ' count)', ' modi', 'ﬁed', ' by', ' a', ' scale', ' factor', 'for', ' the', ' importance', ' of', ' the', ' word. ', 'The', ' scale', ' factor', ' is', ' called', ' the', ' inverse', 'document', ' frequency', ',', ' which', ' is', ' given', ' in ', 'Equation', ' ', '(', '2.2', '). It simply', ' checks', 'the', ' number', ' of', ' documents', ' containing', ' word', ' j ', '(', 'i.e.', ',', ' df ', '(', ' j))', ' and reverses', 'the', ' scaling. Thus', ',', ' when', ' a', ' word', ' appears', ' in', ' many', ' documents', ',', ' it is', ' con', '-', 'sidered', ' unimportant and', ' the', ' scale', ' is lowered', ',', ' perhaps', ' near', ' zero. When', 'the', ' word', ' is relatively', ' unique', ' and', ' appears', ' in', ' few', ' documents', ',', ' the', ' scale', 'factor', ' zooms', ' upward', ' because', ' it', ' appears', ' important', '.', 'tf-idf', '(', ' j)', ' = tf', '(', ' j', ')', 'idf', '(', ' j', ')', '.', 'idf', '(', ' j)', ' = log', '∗', 'N', 'df', '(', ' j', ')', '.', '(', '2.1', ')', '(', '2.2', ')', '!', 'Alternative', ' versions', ' of the', ' basic', ' tf-idf', ' formulation', ' exist', ',', ' but', ' the', ' gen', '-', 'eral', ' motivation', ' is', ' the', ' same. The', ' net', ' result of', ' ', 'this', ' process', ' is a positive', 'score', ' that replaces', ' the', ' simple', ' frequen', 'cy', ' or binary', ' true-or-false', ' entry', ' in', 'the', ' cell', ' ', 'of', ' our spre', 'ad', 'sheet. The', ' bigger', ' the', ' score', ',', ' the', ' more', ' important its', '\"', '\\x0c2.5 ', 'Vector ', 'Generation', ' for Prediction', '31', 'expected', ' value', ' to', ' the', ' learning', ' method. ', 'Although', ' ', 'this', ' trans', 'formation', ' is', 'only', ' a', ' slight modi', 'ﬁcation', ' of', ' our', ' original', ' binary-feature', ' model', ',', ' it', ' does', 'lose', ' the', ' clarity', ' and', ' simplicity', ' of', ' the', ' earlier', ' presentation', '.', 'Another', ' variant', ' is', ' to', ' weight', ' the', ' tokens', ' from', ' d', 'ifferent', ' parts', ' of', ' the', 'document', ' differently. For', ' example', ',', ' the', ' words', ' in', ' the', ' subject', ' line', ' of', ' a', 'document could receive', ' additional', ' weight. An', ' e', 'ffective', ' variant', ' is to', 'generate', ' separate', ' sets', ' of', ' features', ' for the', ' categories', ' ', '(', 'for', ' each', ' category', ',', 'the', ' set', ' of', ' features', ' is', ' derived', ' only', ' from', ' ', 'the', ' tokens', ' of', ' documents', ' of', ' that', 'category)', ' and then', ' pool', ' all the', ' feature', ' sets', ' toge', 'ther', '.', 'All', ' ', 'of', ' these', ' models', ' of', ' data', ' are', ' modest', ' variations', ' of', ' the', ' basic', ' binary', 'model', ' for ', 'the', ' presence', ' or absence', ' of', ' words. Which', ' of', ' the', ' data', ' transfor-', 'mations', ' are', ' best', '?', ' We', ' will', ' not give', ' a', ' universal', ' answer. ', 'Experience', ' has', 'shown', ' that the', ' best', ' prediction', ' accuracy', ' is', ' dependent', ' on', ' mating', ' one', 'of', ' these', ' variations', ' to a', ' speci', 'ﬁc', ' learning', ' method. ', 'The', ' best', ' variation', 'for', ' one', ' method', ' may', ' not', ' be', ' ', 'the', ' one', ' for another', ' me', 'thod. Is', ' it', ' neces', '-', 'sary', ' to', ' test', ' all', ' variations', ' with', ' all', ' methods', '?', ' ', 'When', ' we', ' describe', ' the', 'learning', ' methods', ',', ' we', ' will', ' give', ' guidelines', ' for the', ' individual', ' methods', 'based', ' on', ' general', ' research', ' experience. ', 'Moreover', ',', ' some', ' methods', ' have', 'a', ' natural', ' relationship', ' to', ' one', ' of', ' these', ' representations', ',', ' and that', ' alone', 'would', ' make', ' them', ' the', ' preferred', ' approach', ' to', ' representing', ' data', '.', 'Much', ' effort', ' has been', ' expended', ' in', ' ', 'trans', 'forming', ' ', 'this', ' word', ' model', 'of', ' data', ' into', ' a', ' some', 'what more', ' ', 'cryptic', ' presentation. ', 'The', ' data', ' remain', 'entries', ' in', ' the', ' spre', 'ad', 'sheet', ' cells', ',', ' but', ' their value', ' may', ' be', ' less', ' intelligible', '.', 'Some', ' ', 'of', ' these', ' trans', 'formations', ' are', ' techniques', ' for', ' reducing', ' duplication', 'and', ' dimensions. Others', ' are', ' based', ' on', ' care', 'ful', ' empirical', ' experimenta-', 'tion', ' that supports', ' their', ' value', ' in', ' increased', ' predictive', ' capabilities. We', 'will', ' discuss', ' seve', 'ral', ' classes', ' of', ' prediction', ' method', 's. ', 'They', ' tend', ' to', ' work', 'better', ' with different', ' types', ' ', 'of', ' data', ' transformations', '.', 'Al', 'though', ' we', ' describe', ' data', ' as', ' populating', ' a', ' spread', 'sheet', ',', ' we', ' expect', 'that', ' most of ', 'the', ' cells', ' will', ' be', ' zero. Most', ' documents', ' contain', ' a', ' small', 'subset', ' ', 'of', ' the', ' dictionary', '’s', ' words. In', ' the', ' case', ' ', 'of', ' text', ' classi', 'ﬁcation', ',', ' a text', 'corpus', ' might have', ' thousands', ' of', ' word', ' types. Each', ' individual', ' document', ',', 'however', ',', ' has', ' only', ' a', ' few', ' hundred', ' unique', ' tokens. So', ',', ' in', ' the', ' spreadsheet', ',', 'al', 'most', ' all', ' of', ' the', ' entries', ' for', ' that', ' document will', ' be', ' zero. Rather', ' than', 'store', ' all', ' the', ' zeros', ',', ' it', ' is', ' better', ' to', ' represent', ' the', ' spreadsheet', ' as', ' a', ' set', 'of', ' sparse', ' vectors', ',', ' where', ' a', ' row is represented', ' by', ' a', ' list', ' of', ' pairs', ',', ' one', 'element', ' of', ' the', ' pair', ' being', ' a', ' column', ' number', ' and', ' ', 'the', ' other', ' element', 'being', ' the', ' corres', 'ponding', ' nonzero', ' feature', ' value. By', ' not', ' storing the', ' ze', '-', 'ros', ',', ' savings', ' in memory', ' can', ' be', ' immense. Processing', ' programs', ' can', ' be', 'easily', ' adapted', ' to', ' handle', ' ', 'this', ' format. ', 'Figure', ' ', '2.6', ' gives', ' a simple', ' exam', 'ple', '\\x0c32', '2. From', ' ', 'Textual', ' ', 'Information', ' to', ' ', 'Numerical', ' ', 'Vectors', 'Spread', 'sheet', '0', '12', '8', '15', '0', '0', '0', '0', '5', '3', '0', '2', 'Sparse', ' ', 'Vectors', '(', '2', ',', '15)', ' ', '(', '4', ',', '3', ')', '(', '1', ',', '12', ')', '(', '1', ',', '8)', ' ', '(', '3', ',', '5)', ' ', '(', '4', ',', '2', ')', 'Figure', ' ', '2.6. ', 'Spreadsheet', ' to', ' ', 'Sparse', ' ', 'Vectors', 'of', ' how', ' a', ' spread', 'sheet', ' is', ' transformed', ' into', ' sparse', ' vectors. All', ' of', ' our', 'proposed', ' data representations', ' are', ' consistent', ' with', ' ', 'such', ' a', ' sparse', ' data', 'representation', '.', '2.5.1', ' ', 'Multiword', ' ', 'Features', 'Generally', ',', ' features', ' are', ' associated', ' with single', ' words', ' ', '(', 'tokens', ' delimited', 'by', ' white', ' space)', '. ', 'Al', 'though', ' this', ' is', ' reas', 'ona', 'ble', ' most', ' of', ' the', ' time', ',', ' there', 'are', ' cases', ' where', ' it', ' helps', ' to ', 'consider', ' a', ' group', ' ', 'of', ' words', ' as', ' a', ' feature. This', 'happens', ' when', ' a number', ' of', ' words', ' are', ' used', ' to describe', ' a concept', ' that', 'must', ' be', ' made', ' into', ' a', ' feature. The', ' sim', 'plest', ' scenario', ' is', ' where', ' ', 'the', ' feature', 'space', ' is', ' extended', ' to', ' include', ' pairs', ' of', ' words. Instead', ' of', ' just', ' separate', 'features', ' for', ' bon', ' and', ' vivant', ',', ' we', ' ', 'could', ' also', ' have', ' a', ' feature', ' for', ' bon', ' vi', '-', 'vant. ', 'But', ' why', ' stop', ' at', ' pairs', '?', ' ', 'Why', ' not', ' consid', 'er', ' more', ' general', ' multiword', 'features', '?', 'The', ' most', ' common', ' exam', 'ple', ' of', ' this', ' is', ' a', ' named', ' entity', ',', ' for', ' example', ' ', 'Don', 'Smith', ' or ', 'United', ' States', ' of', ' America. ', 'Unlike', ' word', ' pairs', ',', ' the', ' words', ' need', 'not', ' necessarily', ' be', ' consecutive. ', 'For', ' example', ',', ' in', ' specifying', ' Don', ' Smith', 'as', ' a', ' feature', ',', ' we', ' may', ' want to', ' ignore', ' ', 'the', ' fact', ' that', ' he', ' has', ' a middle', ' name', 'of', ' ', 'Leroy', ' that may', ' appear', ' in', ' some', ' references', ' to', ' the', ' person. ', 'Another', 'exam', 'ple', ' ', 'of', ' a', ' multiword', ' feature', ' is', ' an', ' adjective', ' followed', ' by', ' a', ' noun', ',', ' such', 'as', ' broken', ' vase. In', ' this', ' case', ',', ' to', ' accommodate', ' many', ' references', ' to', ' the', 'noun', ' that', ' involve', ' a', ' number', ' ', 'of', ' adjectives', ' with', ' ', 'the', ' desired', ' adjective', ' not', 'necessarily', ' adjacent', ' to', ' the', ' noun', ',', ' we', ' must', ' permit some', ' ', 'ﬂexibility', ' in', ' the', 'distance', ' between', ' the', ' adjective', ' and', ' noun. In', ' the', ' same', ' exam', 'ple', ' of', ' the', 'vase', ',', ' we', ' want to', ' accept', ' a', ' phrase', ' such', ' as', ' broken', ' and', ' dirty', ' vase', ' as', ' an', 'instance', ' ', 'of', ' broken', ' vase. An', ' even', ' more', ' abstract', ' case', ' is', ' when', ' words', ' sim', '-', 'ply', ' happen', ' to be highly', ' correlated', ' in', ' ', 'the', ' text. ', 'For', ' instance', ',', ' in stories', 'about', ' ', 'Germany', ' boy', 'cotting', ' product', ' Y', ',', ' the', ' word-stem', ' ', 'German', ' would', ' be', 'highly', ' correlated', ' with', ' ', 'the', ' wordstem', ' boy', 'cott within', ' a', ' small', ' windo', 'w', '(', 'say', ',', ' ﬁve', ' words). Thus', ',', ' more', ' generally', ',', ' multiword', ' features', ' consist', ' of', ' x', 'number', ' ', 'of', ' words', ' occurring', ' within', ' a', ' maximum', ' window size', ' of', ' y ', '(', 'with', 'y', '≥', 'The', ' key', ' question', ' is', ' how', ' such', ' ', 'features', ' can', ' be', ' extracted', ' from', ' text', '.', 'How', ' smart', ' do we', ' have', ' to', ' be', ' in ', 'ﬁnding', ' such', ' ', 'features', '?', ' Named', ' enti', '-', 'x naturally', ')', '.', '\\x0c2.5 ', 'Vector ', 'Generation', ' for Prediction', '33', 'Input', ':', 'ts', ',', ' sequen', 'ce', ' of', ' tokens', ' in', ' the', ' document', ' collection', 'k', ',', ' the', ' number', ' of', ' features', ' desired', 'mwl', ',', ' maximum', ' leng', 'th', ' of', ' multiword', 'mws', ',', ' maximum', ' span', ' ', 'of', ' words', ' in', ' multiword', 'slvl', ',', ' correlation', ' ', 'thre', 'shold', ' for multiword', ' features', 'mfreq', ',', ' frequency', ' threshold', ' for accepting', ' features', 'Output', ':', 'fs', ',', ' a set', ' of', ' k', ' features', 'Initialize', ':', 'hs', ' ', ':', '= empty', ' hashtable', 'for', ' each', ' tok', ' in', ' ts', ' do', 'Generate', ' a list of', ' multiword', ' tokens', ' ending', ' in', ' tok', '.', 'This', ' list', ' includes', ' the', ' single-word', ' tok', ' and', ' uses', ' the', ' in', 'puts', ' mws', ' and', ' mwl', '.', 'Call', ' this', ' list', ' mlist', '.', 'for', ' each', ' mtok', ' in', ' mlist', ' do', 'If hs', ' contains', ' mtok', ' then', 'i ', ':', '= value', ' ', 'of', ' mtok', ' in', ' hs', 'incre', 'ment', ' i', ' by', ' 1', 'else', 'i ', ':', '= 1', 'endif', 'store', ' i', ' as', ' value', ' ', 'of', ' mtok', ' in', ' hs', 'endfor', 'endfor', 'sk ', ':', '= keys', ' in', ' hs', ' sorted', ' by', ' decreasing', ' value', 'delete', ' elements', ' in', ' sk with', ' a', ' frequency', ' < mfreq', 'delete', ' multiword', ' elements', ' in', ' sk with', ' an', ' association', ' mea', 'sure', ' < slvl', 'fs ', ':', '= top', ' k', ' keys', ' in sk', 'out', 'put', ' fs', 'Figure', ' ', '2.7. ', 'Generating', ' ', 'Multiword', ' ', 'Features', ' from', ' ', 'Tokens', 'ties', ' can', ' be', ' extracted', ' using', ' specialized', ' method', 's. ', 'For', ' other', ' mul', 'tiword', 'features', ',', ' a', ' more', ' general', ' approach', ' might be', ' to', ' treat', ' ', 'them', ' like', ' single', '-', 'word', ' features. If', ' we', ' use', ' a', ' frequen', 'cy', ' approach', ',', ' then', ' we', ' will', ' only', 'in', 'clude', ' those', ' combinations', ' of', ' words', ' ', 'that', ' occur', ' relatively', ' frequently', '.', 'A straightforward', ' implementation', ' would', ' simply', ' examine', ' all', ' combina-', 'tions', ' ', 'of', ' up', ' to', ' x', ' words', ' within', ' a', ' window', ' ', 'of', ' y', ' words. Clearly', ',', ' the', ' number', 'of', ' potential features', ' grows', ' signi', 'ﬁcantly', ' when', ' multiword', ' fea', 'tures', ' are', 'considered', '.', 'Measuring', ' the', ' value', ' of', ' multiword', ' features', ' is', ' typically', ' done', ' by', 'considering', ' correlation', ' between', ' the', ' words', ' in potential', ' multiword', ' fea', '-', 'tures. A', ' variety', ' of', ' measures', ' based', ' on mutual', ' information', ' or', ' the', '\\x0c3', '4', '2. From', ' ', 'Textual', ' ', 'Information', ' to', ' ', 'Numerical', ' ', 'Vectors', 'likelihood', ' ratio', ' may', ' be', ' used', ' for', ' this', ' purpose. ', 'In', ' ', 'the', ' accom', 'panying', 'software', ',', ' Equation', ' ', '(', '2.3)', ',', ' which', ' computes', ' an', ' association', ' measure', ' AM', 'for the', ' multiword', ' T', ',', ' is', ' used', ' for', ' evaluating', ' multiword', ' features', ',', ' where', 'size', '(', 'T) is', ' the', ' number of', ' words', ' in', ' phrase', ' ', 'T and', ' freq', '(', 'T) is', ' the', ' number', 'of times', ' phrase', ' T', ' occurs', ' in', ' ', 'the', ' document', ' collection', '.', '(', '2.3', ')', 'AM', '(', 'T) ', '=', 'size', '(', 'T)', ' log', '10', '(', 'freq(T))', 'freq', '(', 'T)', 'T freq', '(', 'wordi', ')', 'wordi', '∈', 'Other', ' variations', ' can', ' occur', ' depending', ' on', ' whe', 'ther', ' stopwords', ' are', 'excluded', ' before', ' building', ' multiword', ' features', '.', '#', 'An', ' algorithm', ' for', ' generating', ' mul', 'tiword', ' features', ' is', ' shown', ' in', ' ', 'Figure', '2.7', ',', ' which', ' extends', ' ', 'Figure', ' ', '2.4', ' to', ' mul', 'tiword', ' features. A straightfor', 'ward', 'implementation', ' can', ' consume', ' a', ' lot', ' of', ' memory', ',', ' but', ' a', ' more', ' ef', 'ﬁcient', ' im', '-', 'plementation', ' uses', ' a', ' sliding', ' window', ' to generate', ' potential', ' mul', 'tiwords', 'in', ' a sing', 'le', ' pass', ' over', ' the', ' input', ' text without having', ' to store', ' too', ' many', 'words', ' in', ' memory. A', ' version', ' of', ' ', 'this', ' is', ' implemented', ' in', ' the', ' accompanying', 'software', '.', 'Generally', ',', ' multiword', ' features', ' are', ' not', ' found', ' too', ' frequently', ' in a', 'document', ' collection', ',', ' but', ' when', ' they do', ' occur', ' they', ' are', ' often', ' highly', ' pre', '-', 'dictive. They are', ' also', ' particularly', ' satis', 'fying', ' for', ' explaining', ' a learning', 'me', 'thod', '’s', ' proposed', ' solution. The', ' down', 'side', ' ', 'to', ' using', ' multiwords', ' is', ' that', 'they', ' add', ' an additional', ' layer', ' of', ' complexity to', ' ', 'the', ' processing', ' of', ' text', ',', 'and', ' some', ' practitioners', ' may', ' feel', ' it', '’s', ' the', ' job', ' of', ' the', ' learning', ' methods', ' to', 'combine', ' the', ' words', ' without', ' a', ' preprocessing', ' step', ' to', ' compose', ' multiword', 'features. ', 'However', ',', ' if', ' the', ' learning', ' method', ' is', ' not', ' capable', ' of', ' doing', ' this', ',', 'the', ' extra', ' effort', ' may', ' be', ' wor', 'thwhile', ' because', ' multiwords', ' are', ' often highly', 'predictive', ' and', ' enhance', ' the', ' interpretability of', ' results', '.', '2.5.2', ' Labels', ' for the', ' ', 'Right', ' Answers', 'For', ' prediction', ',', ' an', ' extra', ' column', ' must', ' be', ' added', ' to', ' ', 'the', ' spreadsheet. This', 'last', ' column', ' of', ' the', ' spre', 'ad', 'sheet', ',', ' containing', ' the', ' label', ',', ' looks', ' no', ' different', 'from', ' the', ' others. It', ' is', ' a', ' one', ' or zero', ' indicating', ' that', ' the', ' correct', ' answer', ' is', 'either', ' true', ' or false. What', ' is', ' ', 'the', ' label', '?', ' Traditional', 'ly', ',', ' this', ' label has', ' been', 'a', ' topic', ' to', ' ind', 'ex', ' the', ' document. ', 'Sports', ' or ', 'ﬁnancial', ' stories', ' are', ' examples', 'of', ' topics. We', ' are', ' not', ' making', ' this', ' semantic', ' distinction. Any', ' answer', 'that', ' can', ' be', ' measured', ' as', ' true', ' or', ' false', ' is', ' acceptable. It', ' could', ' be', ' a', ' topic', 'or', ' category', ',', ' or', ' it', ' could', ' be', ' an', ' article', ' that', ' appeared', ' prior', ' to', ' a', ' stock', 'price', '’s rise. As', ' long', ' as', ' the', ' answers', ' are', ' labeled', ' correctly', ' relative', ' to', ' the', 'concept', ',', ' the', ' format', ' is', ' acceptable. Of', ' course', ',', ' that', ' doesn', '’t', ' mean', ' that', ' the', 'problem', ' can readily', ' be', ' solved. In', ' ', 'the', ' sparse', ' vector', ' format', ',', ' the', ' labels', '\\x0c2.5 ', 'Vector ', 'Generation', ' for Prediction', '35', 'are', ' appended', ' to', ' each', ' vector', ' separately', ' as', ' either', ' a', ' one ', '(', 'positive', ' class', ')', 'or', ' a', ' zero', ' ', '(', 'negative', ' class)', '.', '2.5.3', ' ', 'Feature', ' ', 'Selection', ' by', ' Attribute', ' ', 'Ranking', 'In', ' addition', ' to the', ' frequency-based', ' approaches', ' mentioned', ' earlier', ',', ' fea', '-', 'ture', ' selection can', ' be', ' done', ' in', ' a', ' number', ' ', 'of', ' different ways. In', ' general', ',', 'we', ' want to', ' select', ' a', ' set', ' of', ' features', ' for', ' each', ' category', ' to', ' form', ' a', ' local', 'dictionary', ' for the', ' category. A relative', 'ly', ' simple', ' and', ' quite', ' use', 'ful', ' me', 'thod', 'for', ' doing', ' so', ' is by', ' independently', ' ranking', ' feature', ' attributes', ' according', ' to', 'their', ' predictive', ' abilities', ' for the', ' category', ' under', ' consideration. In this', 'approach', ',', ' we', ' can', ' simply', ' select', ' the', ' top-ranking', ' features', '.', 'The', ' predictive', ' ability', ' ', 'of', ' an', ' attribute', ' can', ' be', ' measured', ' by', ' a', ' certain', 'quantity', ' ', 'that indicates', ' how correlated', ' a', ' ', 'feature', ' is with ', 'the', ' class', ' label', '.', 'Assume', ' that we', ' have', ' n', ' documents', ',', ' and', ' xj', ' is', ' the', ' presence', ' or', ' absence', 'of', ' attribute', ' j', ' in', ' a', ' document', ' x. We', ' also', ' use', ' y to', ' denote', ' ', 'the', ' label', ' of', 'the', ' document', ';', ' that is', ',', ' the', ' last', ' column', ' in', ' our', ' spread', 'sheet', ' model. A', 'commonly', ' used', ' ranking', ' score', ' is', ' ', 'the', ' information', ' gain', ' criterion', ',', ' which', 'can', ' be', ' de', 'ﬁned', ' as', 'IG', '(', ' j)', ' = Llabel', ' ', '−', 'L', '(', ' j)', ',', 'where', '1', 'Llabel', ' ', '=', 'Pr', '(', 'y = c)', ' log2', '$c', '=0', '1', '1', '1', 'Pr', '(', 'y = c', ')', ',', '(', '2.4', ')', 'L', '(', ' j)', ' ', '=', 'Pr', '(', 'xj = v)', 'Pr', '(', 'y = c', '$', 'v=0', '$c', '=0', 'xj = v)', ' log', '2', '|', '1', 'Pr', '(', 'y = c', 'xj = v)', '|', '.', '(', '2.5)', 'The', ' quantity', ' L', '(', ' j)', ' is', ' the', ' number', ' of', ' bits', ' required', ' to', ' encode', ' ', 'the', ' label', 'and', ' the', ' attribute', ' j', ' minus', ' ', 'the', ' number', ' ', 'of', ' bits', ' required', ' to', ' encode', ' the', 'attribute', ' j. That', ' is', ',', ' L', '(', ' j)', ' is', ' the', ' number', ' of', ' bits', ' needed', ' to', ' encode', ' the', 'l', 'abel', ' given', ' that we', ' know', ' the', ' attribute', ' j. Therefore', ',', ' the', ' information', 'L', '(', ' j)', ' is', ' ', 'the', ' number', ' of', ' bits', ' we', ' can', ' save', ' for', ' encoding', ' the', 'gain', ' ', 'Llabel', ' ', '−', 'class', ' label', ' if we', ' know', ' the', ' feature', ' j. Clearly', ',', ' it', ' measures', ' how', ' use', 'ful', ' a', 'feature', ' j is', ' from', ' the', ' information-theoretical', ' point', ' of', ' view', '.', 'Since', ' ', 'Llabel', ' is', ' the', ' same', ' for', ' all', ' j', ',', ' we', ' can', ' simply', ' com', 'pute', ' L', '(', ' j)', ' for', ' all', ' at', '-', 'tributes', ' j and', ' select', ' the', ' ones', ' with', ' ', 'the', ' smallest', ' values. ', 'Quantities', ' that', 'are', ' needed', ' to', ' compute', ' L', '(', ' j)', ' in ', 'Equation', ' ', '(', '2.5) can', ' be', ' easily', ' estimated', '\\x0c36', '2. From', ' ', 'Textual', ' ', 'Information', ' to', ' ', 'Numerical', ' ', 'Vectors', 'using', ' the', ' following', ' plug-in', ' estimators', ':', 'Pr', '(', 'xj = v)', ' ', '=', 'Pr', '(', 'y = c', 'xj = v) ', '=', '|', ',', 'freq', '(', 'xj = v)', ' + 1', 'n + 2', 'freq', '(', 'xj = v', ',', ' label = c) + 1', 'freq', '(', 'xj = v)', ' + 2', '.', '2.6', ' ', 'Sentence', ' Boundary', ' ', 'Determination', 'If', ' the', ' XML', ' markup', ' for a', ' corpus', ' does', ' not mark', ' sentence', ' boundaries', ',', 'it', ' is often', ' necessary', ' for', ' ', 'these', ' to', ' be', ' marked. At', ' the', ' very', ' least', ',', ' it is', 'necessary', ' to determine', ' when', ' a', ' period', ' is', ' part', ' of', ' a', ' token', ' and', ' when', ' it', 'is', ' not. ', 'For more', ' sophisticated', ' linguistic', ' parsing', ',', ' the', ' algorith', 'ms', ' often', 'require', ' a com', 'plete', ' sentence', ' as', ' input. ', 'We', ' shall', ' also', ' see', ' other', ' in', 'forma', '-', 'tion', ' extraction', ' algorithms', ' that', ' operate', ' on text', ' a', ' sentence', ' at a time', '.', 'Input', ':', ' a text', ' with', ' periods', 'Output', ':', ' same', ' text', ' with ', 'End-of-Sentence', ' ', '(', 'EOS)', ' periods', ' identi', 'ﬁed', 'Overall', ' Strategy', ':', '1. Replace', ' all', ' identi', 'ﬁable', ' non-EOS', ' periods', ' with', ' another', ' character', '2. Apply', ' rules', ' to', ' all', ' ', 'the', ' periods', ' in text', ' and', ' mark', ' EOS periods', '3. Retransform', ' the', ' characters', ' in', ' step', ' 1', ' to', ' non-EOS', ' periods', '4. ', 'Now', ' the', ' text', ' has', ' all', ' EOS periods', ' clearly', ' identi', 'ﬁed', 'Rules', ':', 'All ', '?', ' ', '!', ' are', ' EOS', 'If \"', ' or ’ appears', ' before', ' period', ',', ' it is', ' EOS', 'If', ' the', ' following', ' character', ' is', ' not', ' white', ' space', ',', ' it is', ' not', ' EOS', 'If )}]', ' before', ' period', ',', ' it is', ' EOS', 'If the', ' token', ' to which', ' ', 'the', ' period', ' is', ' attached', ' is', ' capitalize', 'd', 'and', ' is', ' < 5', ' characters', ' and', ' the', ' next', ' token', ' begins', ' uppercase', ',', 'it is', ' not', ' EO', 'S', 'If the', ' token', ' to which', ' ', 'the', ' period', ' is', ' attached', ' has', ' other', ' periods', ',', 'it is', ' not', ' EO', 'S', 'If the', ' token', ' to which', ' ', 'the', ' period', ' is', ' attached', ' begins', ' with', ' a', ' lowercase', 'letter', ' and', ' the', ' next', ' token', ' following', ' whites', 'pace', ' is', ' uppercase', ',', 'it is', ' EOS', 'If the', ' token to', ' which', ' the', ' period', ' is', ' attached', ' has', ' < 2', ' characters', ',', 'it is', ' not', ' EO', 'S', 'If the', ' next', ' token', ' following', ' whites', 'pace', ' begins', ' with', ' ', '$', '(', '{[\"’ it', ' is', ' EOS', 'Otherwise', ',', ' the', ' period', ' is', ' not', ' EO', 'S', 'Figure', ' ', '2.8. End-of-Sentence', ' ', 'Detection', ' ', 'Algorithm', '\\x0c2.7 Part-Of-Spee', 'ch ', 'Tagging', '37', 'For', ' these', ' algorith', 'ms', ' to', ' per', 'form', ' optimally', ',', ' clearly', ' the', ' sentences', ' must', 'be', ' identi', 'ﬁed', ' correctly. ', 'Sentence', ' boundary', ' determination', ' is', ' essentially', 'the', ' problem of deciding', ' which instances', ' ', 'of', ' a', ' period', ' followed', ' by white', '-', 'space', ' are', ' sentence', ' delimiters', ' and', ' which', ' are', ' not', ' since', ' we', ' assume', ' that', 'the', ' characters', ' ', '?', ' and ', '!', ' are', ' unambiguous', ' sentence', ' bound', 'aries. Since', 'this', ' is', ' a', ' classi', 'ﬁcation', ' problem', ',', ' one', ' can', ' natural', 'ly', ' invoke', ' standard', 'classi', 'ﬁcation', ' sof', 'tware', ' on', ' training', ' data', ' and', ' achieve', ' accuracy', ' of', ' more', 'than', ' 98%. This is', ' discussed', ' at some', ' leng', 'th', ' in', ' Section', ' 2.12. ', 'However', ',', ' if', 'training', ' data', ' are', ' not', ' available', ',', ' one', ' can', ' use', ' a hand-crafted', ' algorithm', '.', 'Figure', ' ', '2.8', ' gives', ' an', ' algorithm', ' ', 'that', ' will', ' achieve', ' an', ' accuracy', ' of', ' more', 'than', ' 90%', ' on', ' news', 'wire', ' text. Adjustments', ' to', ' the', ' algorithm', ' for', ' other', 'corpora', ' may be', ' necessary', ' to', ' get', ' better', ' per', 'forman', 'ce. ', 'Notice', ' how', ' the', 'algorithm', ' is', ' implicitly', ' tailored', ' for ', 'English. A d', 'ifferent', ' language', ' would', 'have', ' a completely', ' different', ' procedure', ' but', ' would', ' still', ' involve', ' the', ' basic', 'idea', ' ', 'of', ' rules', ' that', ' examine', ' the', ' context', ' ', 'of', ' potential', ' sentence', ' bound', 'aries', '.', 'A', ' more', ' ', 'thorough', ' implementation', ' of', ' ', 'this', ' algorithm', ' is', ' available', ' in', ' the', 'accompanying', ' sof', 'tware', '.', '2.7', ' Part-Of-Spee', 'ch ', 'Tagging', 'Once', ' a', ' text', ' has been', ' broken', ' into', ' tokens', ' and', ' sentences', ',', ' the', ' next', ' step', 'depends', ' on', ' what', ' is', ' to', ' be', ' done', ' with', ' ', 'the', ' text. ', 'If', ' no', ' further', ' linguistic', 'analy', 'sis', ' is', ' necessary', ',', ' one', ' might', ' proceed', ' directly', ' to', ' feature', ' generation', ',', 'in', ' which the', ' features', ' will', ' be', ' obtained', ' from', ' ', 'the', ' tokens. ', 'However', ',', ' if', 'the', ' goal', ' is', ' more', ' speci', 'ﬁc', ',', ' say', ' recognizing', ' names', ' of', ' people', ',', ' places', ',', ' and', 'organizations', ',', ' it', ' is', ' usually', ' desirable', ' to', ' per', 'form', ' additional', ' linguistic', 'analy', 'ses', ' ', 'of', ' ', 'the', ' text', ' and', ' extract', ' more', ' sophisticated', ' features. ', 'Toward', 'this', ' end', ',', ' the', ' next logical', ' step', ' is', ' to', ' determine', ' ', 'the', ' part', ' of', ' speech', ' ', '(', 'POS)', 'of', ' each', ' token', '.', 'In', ' any', ' natural', ' language', ',', ' words', ' are', ' organized', ' into', ' grammatical', 'cl', 'asses', ' or parts', ' of', ' spee', 'ch. Almost', ' all', ' languages', ' will', ' have', ' at', ' least', 'the', ' categories', ' we', ' would', ' call', ' nouns', ' and', ' verbs. ', 'The', ' exact', ' number', ' of', 'categories', ' in a', ' given', ' language', ' is', ' not', ' some', 'thing', ' intrinsic', ' but depends', 'on', ' how', ' the', ' language', ' is', ' analyzed', ' by', ' an', ' individual', ' linguist', '.', 'In English', ',', ' some', ' analyses', ' may', ' use', ' as', ' few', ' as', ' six', ' or', ' seven', ' categories', 'and', ' others', ' nearly', ' one', ' hundred. Most', ' ', 'English grammars', ' would', ' have', ' as', 'a minimum', ' noun', ',', ' verb', ',', ' adjective', ',', ' adverb', ',', ' preposition', ',', ' and', ' conjunction', '.', 'A bigger set of', ' 36', ' categories', ' is', ' used', ' in', ' ', 'the', ' Penn', ' ', 'Tree', ' ', 'Bank', ',', ' constructed', 'from', ' ', 'the', ' Wall Street', ' ', 'Journal', ' corpus', ' discussed', ' later', ' on. Table', ' 2.4', ' shows', 'some', ' of', ' ', 'these', ' categories', '.', '\\x0c38', '2. From', ' ', 'Textual', ' ', 'Information', ' to', ' ', 'Numerical', ' ', 'Vectors', 'Table', ' ', '2.4. Some', ' of', ' the', ' ', 'Categories', ' in', ' the', ' ', 'Penn', ' ', 'Tree', ' ', 'Bank POS ', 'Set', 'Preposition', ' or subordinating', ' conjunction', 'Adjective', 'Adjective', ',', ' comparative', 'Adjective', ',', ' superlative', 'List', ' item', ' marker', 'Description', 'Coordinating', ' conjunction', 'Cardinal', ' number', 'Determiner', 'Existential', ' there', 'Tag', 'C', 'C', 'C', 'D', 'D', 'T', 'E', 'X', 'FW ', 'Foreign', ' word', 'I', 'N', 'J', 'J', 'JJR', 'JJS', 'L', 'S', 'MD ', 'Modal', 'N', 'N', 'NNS ', 'Noun', ',', ' plural', 'POS', 'U', 'H', 'V', 'B', 'VBD', 'VBG', 'VBN Verb', ',', ' past', ' participle', 'VBP', 'WDT Wh-determiner', 'Noun', ',', ' singular', ' or', ' mass', 'Possessive', ' ending', 'Interjection', 'Verb', ',', ' base', ' form', 'Verb', ',', ' past', ' tense', 'Verb', ',', ' gerund', ' or present', ' participle', 'Verb', ',', ' non-3rd', ' person', ' singular', ' present', 'Dictionaries', ' showing', ' word', '–POS corres', 'pondence', ' can', ' be', ' use', 'ful', ' but', 'are', ' not', ' suf', 'ﬁcient. All', ' dictionaries', ' have', ' gaps', ',', ' but', ' even', ' for', ' words', ' found', 'in', ' ', 'the', ' dictionary', ',', ' several', ' parts', ' of', ' speech', ' are', ' usually', ' possible. ', 'Return-', 'ing', ' to', ' an', ' earlier', ' example', ',', ' “bore', '” ', 'could', ' be', ' a', ' noun', ',', ' a', ' present', ' tense', ' verb', ',', 'or', ' a', ' past', ' tense', ' verb. ', 'The', ' goal', ' ', 'of', ' POS', ' tagging', ' is', ' to', ' determine', ' which', ' of', 'these', ' possibilities', ' is', ' realized', ' in', ' a', ' particular', ' text', ' instance', '.', 'Al', 'though', ' it', ' is', ' possible', ',', 'in', ' principle', ',', ' to', ' manually', ' construct', ' a', 'part-of-spee', 'ch', ' tagger', ',', ' the', ' most', ' success', 'ful', ' systems', ' are', ' generated', 'automatically by', ' machine-learning', ' algorithms', ' from', ' annotated', ' cor', '-', 'pora. Almost all', ' POS taggers', ' have', ' been', ' trained', ' on', ' the', ' Wall', ' Street', 'Journal', ' corpus', ' available', ' from LDC ', '(', 'Linguistic', ' Data ', 'Corporation', ',', 'www.ldc.upenn.edu)', ' ', 'because', ' it is', ' the', ' most', ' easily', ' availa', 'ble', ' large', ' anno', '-', 'tated', ' corpus. Although', ' the', ' WSJ cor', 'pus', ' is', ' large', ' and', ' reasonably', ' diverse', ',', 'it', ' is', ' one', ' particular', ' genre', ',', ' and', ' one', ' cannot', ' assume', ' that', ' a', ' tagger', ' based', 'on', ' ', 'the', ' WSJ will', ' perform', ' as', ' well', ' on', ',', ' for', ' example', ',', ' e-mail', ' messages', '.', 'Because', ' much', ' of', ' the', ' impetus', ' for', ' work', ' on', ' in', 'formation', ' extraction', ' has', 'been', ' sponsored', ' by', ' the', ' military', ',', ' whose', ' interest', ' is', ' largely', ' in', ' the', ' pro', '-', '\\x0c2.8', ' ', 'Word', ' ', 'Sense', ' ', 'Disambiguation', '39', 'cessing', ' of voluminous', ' news', ' sources', ',', ' there', ' has', ' not been', ' much', ' support', 'for', ' generating', ' large', ' training corpora', ' in', ' other', ' domains', '.', '2.8', ' Word', ' ', 'Sense', ' ', 'Disambiguation', 'English words', ',', ' besides', ' being', ' ambiguous', ' when', ' isolated', ' from', ' ', 'their', ' POS', 'status', ',', ' are', ' also', ' very', ' often', ' ambiguous', ' as', ' to', ' their ', 'meaning', ' or', ' reference', '.', 'Returning', ' once', ' again', ' to', ' the', ' example', ' ', '“bore', ',', '”', ' one', ' cannot', ' tell', ' with', 'out', 'context', ',', ' even', ' after', ' POS', ' tagging', ',', ' if', ' the', ' word', ' is', ' referring', ' to', ' a', ' person', '—', '“he', ' is', ' a', ' bore', '”', '—or', ' a', ' reference', ' to', ' a', ' hole', ',', ' as', ' in', ' “the', ' bore', ' is', ' not', ' large', 'enough.” ', 'The', ' main', ' function', ' of', ' ordinary', ' dictionaries', ' is', ' to', ' catalog', ' the', 'various', ' meanings', ' of', ' a', ' word', ',', ' but', ' they', ' are', ' not', ' organized', ' for', ' use', ' by', ' a', 'computer', ' program', ' for disambiguation. A large', ',', ' long-running', ' project', 'that', ' focused', ' on word', ' meanings', ' and', ' their', ' interrelation', 'ships', ' is', ' Word', '-', 'net', ',', ' which', ' aimed', ' to ', 'ﬁll', ' in', ' ', 'this', ' gap. As', ' use', 'ful', ' as', ' ', 'Wordnet', ' is', ',', ' by', ' itself it', 'does', ' not', ' provide', ' an', ' algorithm', ' for', ' selecting', ' a', ' particular meaning', ' for a', 'word', ' in', ' context. In', ' spite', ' ', 'of', ' substantial', ' work', ' over', ' a', ' long', ' period', ' of', ' time', ',', 'there', ' are', ' no', ' algorithms', ' ', 'that can', ' com', 'ple', 'tely', ' disambiguate', ' a text. In', 'large', ' part', ',', ' this', ' is', ' due', ' to', ' the', ' lack', ' of', ' a huge', ' corpus', ' ', 'of', ' disambiguated', 'text to serve', ' as a training', ' corpus', ' for machine-learning', ' algorithms', '.', 'Available', ' corpora', ' focus', ' on', ' relatively', ' few', ' words', ',', ' with', ' the', ' aim', ' ', 'of', ' testing', 'the', ' ef', 'ﬁcacy', ' ', 'of', ' particular', ' procedures. Unless', ' a', ' particular text-mining', 'project', ' can', ' be', ' shown', ' to require', ' word', ' sense', ' disambiguation', ',', ' it is', ' best', 'to', ' proceed', ' without', ' such', ' a', ' step', '.', '2.9 Phrase', ' ', 'Recognition', 'Once', ' the', ' tokens', ' of', ' a', ' senten', 'ce', ' have', ' been', ' assigned', ' POS tags', ',', ' the', ' next', 'step', ' is', ' to', ' group', ' individual', ' tokens', ' into', ' units', ',', ' generally', ' called', ' phrases', '.', 'This', ' is', ' useful', ' both', ' for', ' creating', ' a ', '“partial', ' parse', '”', ' of', ' a', ' sentence', ' and', ' as a', 'step', ' in', ' identifying', ' the', ' ', '“named', ' entities', '”', ' occurring', ' in a sentence', ',', ' a topic', 'we', ' will', ' return', ' to', ' in', ' greater', ' detail', ' later', ' on. There', ' are', ' standard', ' corpora', 'and', ' test', ' sets', ' for', ' deve', 'loping', ' and', ' evaluating', ' phrase', ' recognition', ' systems', 'that were', ' developed', ' for various', ' research workshops. Systems', ' are', ' sup-', 'posed', ' to scan a text', ' and', ' mark the', ' beginnings', ' and', ' ends', ' of', ' phrases', ',', ' of', 'which', ' the', ' most', ' important', ' are', ' noun', ' phrases', ',', ' verb', ' phrases', ',', ' and', ' preposi', '-', 'tional', ' phrases. ', 'There', ' are', ' a', ' number', ' of', ' conventions', ' for marking', ',', ' but', ' the', 'most', ' common', ' is to', ' mark', ' a', ' word', ' inside', ' a', ' phrase', ' ', 'with', ' I', '-', ',', ' a word', ' at', ' the', 'beginning', ' ', 'of', ' a phrase', ' adjacent to', ' another', ' phrase', ' with B- and', ' a', ' word', '\\x0c40', '2. From', ' ', 'Textual', ' ', 'Information', ' to', ' ', 'Numerical', ' ', 'Vectors', 'outside', ' any phrase', ' with O. The', ' I- and', ' B- tags', ' can', ' be', ' extended', ' with', 'a', ' code', ' for', ' ', 'the', ' phrase', ' type', ':', ' I-NP', ',', ' B-NP', ',', ' I-VP', ',', ' B-VP', ',', ' etc. ', 'Formulated', ' in', 'this', ' way', ',', ' the', ' phrase', ' identi', 'ﬁcation', ' problem', ' is', ' reduced', ' to', ' a', ' classi', 'ﬁcation', 'problem', ' for', ' the', ' tokens', ' of', ' a', ' sentence', ',', ' in', ' which ', 'the', ' proce', 'dure', ' must', 'supply', ' the', ' correct', ' class', ' for', ' each', ' ', 'token', '.', 'Performance', ' varies', ' widely', ' over', ' phrase', ' type', ',', ' although', ' ove', 'rall', ' per', '-', 'formance', ' measures', ' on', ' benchmark test', ' sets', ' are', ' quite', ' good. A sim', 'ple', 'statistical approach to', ' recognizing', ' signi', 'ﬁcant', ' phrases', ' might be', ' to', 'consider multiword', ' tokens. If', ' a', ' particular sequence', ' of', ' words', ' occurs', 'frequent', 'ly', ' enough', ' in', ' the', ' corpora', ',', ' it', ' will', ' be', ' identi', 'ﬁed', ' as', ' a', ' use', 'ful', ' token', '.', '2.10 Named', ' Entity', ' ', 'Recognition', 'A specialization', ' of', ' phrase', ' ', 'ﬁnding', ',', ' in', ' particular', ' noun', ' phrase', ' ', 'ﬁnding', ',', ' is', 'the', ' recognition of', ' particular types', ' of', ' proper', ' noun', ' phrases', ',', ' speci', 'ﬁcally', 'persons', ',', ' organizations', ',', ' and', ' locations', ',', ' sometimes', ' along', ' with', ' money', ',', 'dates', ',', ' times', ',', ' and', ' percentages. ', 'The', ' importance', ' of', ' these', ' recognizers', ' for', 'intelligence', ' applications', ' is', ' easy', ' to', ' see', ',', ' but', ' they', ' have', ' more', ' mundane', 'uses', ',', ' particularly', ' in', ' turning', ' verbose', ' text', ' data', ' into', ' a', ' more', ' compact', 'structural', ' form', '.', 'From', ' ', 'the', ' point of', ' view', ' of', ' technique', ',', ' this', ' is', ' very', ' like', ' the', ' phrase', 'recognition problem. One', ' might', ' even', ' want', ' to', ' identify', ' noun', ' phrases', 'as', ' a ﬁrst', ' step. ', 'The', ' same', ' sort', ' of', ' token-encoding', ' pattern', ' can', ' be', ' used', '(', 'B-person', ',', ' B-location', ',', ' I-person', ',', ' etc)', ',', ' and', ' the', ' problem', ' is', ' then', ' one', ' of', 'assigning', ' the', ' correct', ' class', ' code', ' to', ' each token', ' in', ' a', ' sentence. ', 'We', ' shall', 'dis', 'cuss', ' this', ' problem', ' in', ' detail', ' in', ' ', 'Chapter', ' 6', ' on', ' information', ' extraction', '.', '2.11', ' Parsing', 'The', ' most', ' sophisticated', ' kind', ' of', ' text', ' processing', ' we', ' will', ' consider', ',', ' brie', 'ﬂy', ',', 'is', ' the', ' step', ' of', ' producing', ' a', ' full', ' parse', ' ', 'of', ' a', ' sentence. By', ' this', ',', ' we mean', 'that', ' each word', ' in', ' a', ' sentence', ' is', ' connected', ' to', ' a', ' single', ' structure', ',', ' usuall', 'y', 'a', ' tree', ' but', ' sometimes', ' a', ' directed', ' acyclic', ' graph. From', ' the', ' parse', ',', ' we can', 'ﬁnd', ' the', ' relation of', ' each', ' word', ' in', ' a', ' sentence', ' to', ' all', ' ', 'the', ' others', ',', ' and', 'typically', ' also', ' its', ' function', ' in', ' the', ' sentence', ' ', '(', 'e', '.g. subject', ',', ' object', ',', ' etc.)', '.', 'There', ' are', ' very', ' many', ' different kinds', ' of', ' parses', ',', ' each', ' ', 'associated', ' with', 'a', ' linguistic', ' theory', ' of', ' language. ', 'This', ' is', ' not', ' the', ' place', ' ', 'to', ' dis', 'cuss', ' ', 'these', 'various', ' theories. ', 'For', ' our', ' purposes', ',', ' we', ' can', ' restrict', ' attention', ' to', ' ', 'the', ' so-', 'called', ' “context-free', '”', ' parses. One', ' can', ' envision', ' a', ' parse', ' of', ' this', ' kind', ' as', '\\x0c2.11', ' Parsing', '41', 'N', 'P', 'N', 'Johnson', 'S', 'V', 'P', 'AUX', 'PPART', 'was', 'replace', 'd', 'V', 'P', 'P', 'P', 'PREP', 'PNOUN', 'at', 'PNOUN', 'PNOUN', 'XYZ', 'Corp', 'P', 'P', 'PREP', 'PNOUN', 'by', 'Smith', 'Figure', ' ', '2.9. Simple', ' ', 'Parse', ' Tree', 'Johnson', ' was replaced', ' at', ' XYZ ', 'Corp. by', ' ', 'Smith ', '.', '------------------------------------------------------------------', 'noun', ' propn', ' sg h m', ' gname', ' sname', '.----- subj', '(', 'n', ')', 'verb', ' vfin', ' vpast sg vsubj', 'o', '----- top', '‘----- pred', '(', 'en', ')', '‘--- vprep', '| ‘- objprep', '(', 'n', ')', '‘--- subj', '(', 'agent)', ' by1', '(', '7', ',', '8', ')', 'Johnson1', '(', '1', ')', 'be', '(', '2', ',', '1', ',', '3', ')', 'replace1', '(', '3', ',', '7', ',', '1', ',', 'u)', ' verb ven', ' vpass', 'at', '1', '(', '4', ',', '6', ')', 'XYZ Corp.1', '(', '6', ')', 'prep', ' pprefv', ' staticp', 'noun', ' propn', ' sg', ' glom', ' ctitle', 'prep', ' pprefv', 'noun', ' propn', ' sg h sname', '‘- objprep', '(', 'n', ')', 'Smith1', '(', '8', ')', 'Figure', ' ', '2.10. Parse', ' ', 'Tree', '—', 'English ', 'Slot', ' ', 'Grammar', 'a', ' tree', ' ', 'of', ' nodes', ' in', ' which', ' ', 'the', ' le', 'af', ' nodes', ' are', ' the', ' words', ' of', ' a', ' sentence', ',', 'the', ' phrases', ' into', ' which', ' the', ' words', ' are', ' grouped', ' are', ' internal', ' nodes', ',', ' and', 'there', ' is', ' one', ' top', ' node', ' at', ' the', ' root', ' of', ' the', ' ', 'tree', ',', ' which', ' usually', ' has', ' the', ' label', 'S. ', 'There', ' are', ' a number', ' of', ' algorithms', ' for producing', ' such', ' a', ' tree', ' from', 'the', ' words', ' of', ' a sentence. Considerable', ' resear', 'ch', ' has', ' been', ' done', ' on con-', 'structing', ' parsers', ' from', ' a', ' statistical', ' analysis', ' of', ' tree', ' banks', ' of', ' sentences', 'parsed', ' by hand. ', 'The', ' best-known', ' and', ' most', ' widely', ' used', ' tree', ' bank', ' is', ' of', 'parsed', ' sentences', ' from', ' ', 'the', ' ', 'Wall', ' ', 'Street', ' ', 'Journal', ' and', ' is', ' availa', 'ble', ' from', 'LDC', '.', 'The', ' reason', ' for considering', ' such', ' a', ' comparatively', ' expensive', ' process', ' is', 'that', ' it', ' provides', ' information', ' that', ' phrase', ' identi', 'ﬁcation', ' or partial', ' pars-', 'ing', ' cannot provide. Consider', ' a', ' sentence', ' such', ' as', ' “', 'Johnson', ' was', ' replaced', 'at', ' XYZ ', 'Corp. by Smith.”', ' for', ' which', ' a', ' simple', ' parse', ' is', ' shown', ' in', ' ', 'Figure', '2.9', '.', '\\x0c42', '2. From', ' ', 'Textual', ' ', 'Information', ' to', ' ', 'Numerical', ' ', 'Vectors', 'From', ' ', 'the', ' linear', ' order', ' of', ' phrases', ' in', ' a', ' partial', ' parse', ',', ' one', ' might', 'con', 'clude', ' that ', 'Johnson', ' replaced', ' Smith. A', ' parse', ' that', ' identi', 'ﬁes', ' the', ' sen', '-', 'ten', 'ce', ' as passive', ' has', ' information', ' allowing', ' the', ' extraction', ' of', ' the', ' correct', '“Smith', ' replaced', ' ', 'Johnson.” An', ' exam', 'ple', ' ', 'of', ' a', ' parse', ' giving', ' more', ' informa-', 'tion', ' than', ' a sim', 'ple', ' tree', ' is', ' shown', ' in', ' ', 'Figure', ' 2.10. ', 'The', ' parse', ' is', ' the', ' output', 'of', ' the', ' ', 'English ', 'Slot', ' ', 'Grammar. In', ' this', ' example', ',', ' the', ' tree', ' is', ' drawn', ',', ' using', 'printer', ' characters', ',', ' on its side', ',', ' with', ' the', ' top', ' of', ' the', ' tree', ' to', ' the', ' lef', 't. ', 'Notice', 'in', ' particular that the', ' ', '“by', '”', ' phrase', ' containing', ' “Smith”', ' is', ' identiﬁed', ' as', 'the', ' agent', ',', ' although', ' “', 'Johns', 'on', '”', ' is', ' marked', ' as', ' ', 'the', ' ', 'subject', '.', '2.12 ', 'Feature', ' ', 'Generation', 'Al', 'though', ' we', ' em', 'phasized', ' that', ' our', ' focus', ' is', ' on', ' statistical', ' methods', ',', ' the', 'reason', ' for the', ' linguistic', ' processing', ' described', ' earlier', ' is', ' to', ' identify', 'features', ' that can', ' be', ' useful', ' for', ' text', ' mining. As', ' an', ' example', ',', ' we will', 'consider', ' how good', ' features', ' depend', ' on', ' the', ' type', ' ', 'of', ' object', ' to', ' be', ' classi', 'ﬁed', 'and', ' how', ' such', ' features', ' can', ' be', ' obtained', ' using', ' the', ' processes', ' dis', 'cussed', 'so', ' far', '.', 'Let', ' us', ' consider the', ' problem', ' of', ' segmenting', ' a text', ' into', ' sentences', '.', 'A', ' hand-crafted', ' algorithm', ' for this', ' task', ' was', ' given', ' earlier. However', ',', 'let', '’s', ' say', ' we', ' want to', ' learn', ' a', ' set', ' of', ' similar rules', ' from', ' training data', 'instead. What sorts', ' of', ' features', ' would', ' we', ' generate', '?', ' ', 'Since', ' the', ' object', ' to', 'be', ' classi', 'ﬁed', ' is', ' a', ' period', ',', ' each', ' feature', ' vector', ' corres', 'ponds', ' to a', ' period', 'occurring', ' in the', ' text. Now', ' we', ' need', ' to consider', ' what', ' characteristics', ' of', 'the', ' surrounding', ' text', ' are', ' use', 'ful', ' features. From', ' ', 'the', ' algorithm', ',', ' we can', 'see', ' that ', 'the', ' useful', ' features', ' are', ' ', 'the', ' characters', ' or character', ' classes', ' near', 'the', ' period', ',', ' including', ' the', ' ', 'character', ' of', ' ', 'the', ' token', ' to', ' which', ' ', 'the', ' period', 'is', ' attached', ' and the', ' ', 'characters', ' of', ' the', ' following', ' token. ', 'Therefore', ',', ' the', 'necessary', ' linguistic', ' processing', ' only', ' involves', ' tokenization', ' of', ' the', ' text', '.', 'A', ' more', ' sophisticated', ' example', ' would', ' be', ' ', 'the', ' identi', 'ﬁcation', ' of', ' the', ' part', 'of', ' speech', ' ', '(', 'POS)', ' ', 'of', ' each', ' word', ' in', ' a', ' text. ', 'This', ' is', ' normally', ' done', ' on', ' a', ' text', 'that', ' has', ' ', 'ﬁrst been', ' segmented', ' into', ' sentences. ', 'The', ' in', 'ﬂuence', ' of', ' one', ' word', 'on', ' ', 'the', ' part-of-spee', 'ch', ' ', 'of', ' another', ' does', ' not', ' cross', ' sentence', ' boundaries', '.', 'The', ' object', ' that a', ' feature', ' vector', ' represents', ' is', ' a', ' token. ', 'Features', ' that', 'might', ' be', ' useful', ' in', ' identifying', ' ', 'the', ' POS', ' are', ',', ' for', ' example', ',', ' whether or', 'not', ' the', ' ﬁrst letter', ' is', ' capitalized', ' ', '(', 'marking a proper', ' noun)', ',', ' if', ' all', ' the', 'characters', ' are', ' digits', ',', ' periods', ',', ' or', ' commas', ' ', '(', 'marking', ' a', ' number)', ',', ' if the', 'characters', ' are', ' alternating', ' uppercase', ' letters', ' and', ' periods', ' ', '(', 'an', ' abbrevia', '-', 'tion)', ',', ' and', ' so', ' on. We', ' might', ' have', ' information', ' from', ' a', ' dictionary', ' as', ' to', ' the', 'possible', ' parts', ' of', ' speech', ' for', ' a', ' token. If', ' we', ' assume', ' the', ' POS assignment', '\\x0c2.12 ', 'Feature', ' ', 'Generation', '43', 'goes', ' left', ' to', ' right', ' through', ' a', ' sentence', ',', ' we', ' have', ' POS', ' assignments', ' of', 'tokens', ' to', ' the', ' left', ' as', ' possible', ' features. In', ' any', ' case', ',', ' we', ' have', ' the', ' identity', 'of', ' tokens', ' to', ' the', ' left', ' and', ' to', ' ', 'the', ' right. ', 'For', ' example', ',', ' “the', '”', ' most', ' likely', 'precedes', ' either', ' a', ' noun', ' or', ' an', ' adjective. So', ',', ' for this', ' task', ',', ' we', ' basically', 'need', ' tokenization', ' plus', ' analysis', ' of', ' the', ' tokens', ',', ' plus', ' perhaps', ' some', ' dic', '-', 'tionaries', ' that tell', ' what', ' the', ' possibilities', ' are', ' for', ' each', ' token', ' in', ' order', ' to', 'create', ' a', ' feature', ' vector', ' for', ' each', ' ', 'token', '.', 'The', ' feature', ' vector', ' for a', ' document', ' is', ' assigned', ' a', ' particular', ' class', ' ', '(', 'or', 'set', ' of', ' classes). The', ' feature', ' vector', ' for classifying', ' periods', ' as', ' End-Of-', 'Sentence', ' or', ' not', ' is', ' assigned', ' to', ' one', ' of', ' two', ' classes. The', ' feature', ' vector', 'for', ' POS', ' assignment', ' has', ' one', ' ', 'of', ' a', ' ', 'ﬁnite', ' set', ' of', ' classes. The', ' class', ' of', ' the', 'feature', ' vector for', ' each token', ' in', ' ', 'the', ' partial', ' parsing', ' task was', ' outlined', 'above. ', 'These', ' classes', ' are', ' not', ' intrinsic', ' or commonly', ' agreed', ' properties', 'of', ' a', ' token. They', ' are', ' invented', ' constructs', ' speci', 'ﬁc', ' to', ' the', ' problem. Let', ' us', 'consider', ' what features', ' are', ' important', ' for ', 'the', ' feature', ' vector', ' for', ' partial', 'parsing. Token', ' identity', ' is', ' clearly', ' one', ' of', ' these', ',', ' as', ' is', ' the', ' token', ' POS', '.', 'Additional', 'ly', ',', ' the', ' identity', ' and', ' POS', ' of', ' the', ' tokens', ' to', ' ', 'the', ' ', 'left', ' and', ' right', 'of', ' the', ' token', ' whose', ' vector', ' is', ' being', ' constructed', ' are', ' important', ' features', '.', 'So', ' is', ' the', ' phrasal', ' class', ' of', ' tokens', ' to', ' the', ' left', ' that', ' have', ' already', ' been', ' as-', 'signed. ', 'Sentence', ' boundaries', ' are', ' particularly', ' important', ' since', ' phrases', 'do', ' not', ' cross', ' them. Individual', ' token', ' features', ',', ' on', ' the', ' other hand', ',', ' are', ' not', 'important', ' because', ' they', ' have', ' already', ' been', ' taken', ' into', ' account', ' for', ' POS', 'assignment', '.', 'For', ' named', ' entity', ' detection', ',', ' the', ' same', ' kind', ' of', ' token', ' class', ' encoding', 'scheme', ' can', ' be', ' used', ' as', ' in', ' the', ' chunking', ' task', ' ', '(', 'i', '.e.', ',', ' B-Person', ',', ' I-Person', ',', 'etc.). ', 'All', ' the', ' named', ' entities', ' are', ' noun', ' phrases', ',', ' so', ' it', ' is', ' possible', ' but', ' not', 'necessary', ' that a sentence', ' will', ' ', 'ﬁrst', ' be', ' segmented', ' into', ' noun', ' phrases', '.', 'This', ' might result', ' in', ' unnecessary', ' work', ' since', ' named', ' entities', ' are', ' typ', '-', 'ically', ' made', ' up of', ' proper', ' nouns. For', ' this', ' task', ',', ' dictionaries', ' can', ' be', 'particularly', ' important. One', ' can', ' identify', ' tokens', ' as', ' instanc', 'es', ' of', ' titles', ',', 'such', ' as', ' “', 'Doctor”', ' or', ' “', 'President', ',', '”', ' providing', ' clues', ' as', ' to', ' the', ' class', ' ', 'of', ' proper', 'noun', ' phrases', ' to', ' the', ' right. Other', ' dictionaries', ' can', ' list', ' words', ' that', ' are', 'ty', 'pically', ' ', 'the', ' end', ' of', ' organization', ' names', ',', ' like', ' “Company', ',', '” “Inc', '.', ',', '” or', ' “De', '-', 'partment.” There', ' are', ' also', ' widely', ' available', ' gazetteers', ' ', '(', 'i.e.', ',', ' lists', ' of', ' place', 'names', ' and', ' lists', ' of', ' organization', ' names', '). ', 'Identifying', ' a token', ' as being', ' a', 'pie', 'ce', ' of', ' such', ' a', ' dictionary', ' entry', ' is', ' use', 'ful', ' but', ' not', ' de', 'ﬁnitive', ' because', 'of', ' ambiguity', ' ', 'of', ' names', ' ', '(', 'e.g.', ',', ' “', 'Arthur ', 'Anders', 'on', '”', ' might', ' be', ' referring', ' to', 'a', ' person', ' or to', ' a', ' company)', '. Besides', ' the', ' dictionary', ' information', ',', ' other', 'use', 'ful', ' features', ' are', ' POS', ',', ' sentence', ' bound', 'aries', ',', ' and', ' the', ' class', ' of', ' tokens', 'already', ' assigned', '.', '\\x0c4', '4', '2. From', ' ', 'Textual', ' ', 'Information', ' to', ' ', 'Numerical', ' ', 'Vectors', '2.13', ' Historical', ' and', ' ', 'Bibliographical', 'Remarks', 'A detailed', ' account', ' of', ' linguistic', ' processing', ' issues', ' can', ' be', ' found', ' in', ' [Man-', 'ning', ' and', ' Schütze', ',', ' 1999', ']', ' and', ' [Jurafsky', ' and', ' Martin', ',', ' ', '2000', ']. Current', 'URLs', ' for', ' organizations', ' such', ' as', ' LDC', ',', ' ICAME', ',', ' TEI', ',', ' the', ' ', 'Oxford', ' ', 'Text', 'Archive', ',', ' and', ' ', 'the', ' like', ' are', ' easily', ' found', ' ', 'through', ' a', ' sear', 'ch', ' engine. ', 'The', ' new', 'Reuters', ' corpus', ',', ' RCV1', ',', ' is', ' discussed', ' in', ' [', 'Lewis', ' et al.', ',', ' 2004', ']', ' and', ' is avail', '-', 'able', ' directly from ', 'Reuters. The', ' older', ' ', 'Reuters-21578', ' Distribution', ' 1.0', 'corpus', ' is', ' also', ' available', ' on the', ' ', 'Internet', ' at several', ' sites. Using', ' a', ' search', 'engine', ' with the', ' query', ' ', '“download', ' ', 'Reuters', ' ', '21578', '” will', ' provide', ' a', ' list', ' of', 'a', ' number', ' ', 'of', ' sites', ' where', ' ', 'this', ' corpus', ' can', ' be', ' obtained. There', ' are', ' a', ' num', '-', 'ber', ' of', ' ', 'Web', ' sites', ' that', ' have', ' many', ' links', ' to', ' corpora', ' in', ' many', ' languages', '.', 'Again', ',', ' use', ' ', 'of', ' a', ' sear', 'ch', ' engine', ' with the', ' query', ' ', '“corpus', ' linguistics', '”', 'will', ' give', ' the', ' URLs', ' of', ' active', ' sites. ', 'There', ' are', ' many', ' books', ' on', ' XML', ';', 'for', ' example', ' [Ray', ',', ' 2001', ']. The', ' best-known', ' algorithm', ' for', ' derivational', 'regularization', ' is', ' the', ' ', 'Porter', ' stemmer', ' [Porter', ',', ' ', '1980', ']', ',', ' which', ' is', ' in', ' the', 'public', ' domain. At', ' the', ' time', ' of', ' ', 'this', ' writing', ',', ' it', ' can', ' be', ' down', 'loaded', ' from', 'martin/Porter', 'Stemmer. ANSI C', ',', ' Java', ',', ' and Perl', 'http', ':', '//www.tartarus.org/', 'versions are', ' available. Another', ' variation', ' on', ' stemming', ' is to', ' base', ' the', 'uni', 'ﬁcation', ' of', ' tokens', ' into stems', ' on', ' corpus', ' statistics', ' ', '(', 'i.e.', ',', ' the', ' stemming', 'is', ' corpus', ' based)', ' [', 'Xu', ' and Croft', ',', ' 1998', ']. For', ' information', ' retrieval', ',', ' this', 'algorithm', ' is', ' said', ' to provide', ' better', ' results', ' than', ' the', ' more', ' aggressive', 'Porter', ' stemmer', '.', '∼', 'For', ' end-of-sentence', ' determination', ',', ' [', 'Walker', ' et', ' al', '.', ',', ' 2001', ']', ' com', 'pares', 'a hard-coded', ' program', ',', ' a rule-based', ' system', ',', ' and', ' a', ' machine-learning', 'solution', ' on', ' ', 'the', ' periods', ' and', ' some', ' other', ' characters', ' in', ' a', ' collection', ' of', 'documents', ' from the', ' ', 'Web. The', ' machine-learning', ' system', ' was', ' best', ' of', ' the', 'three. The', ' F-measures', ' ', '(', 'see', ' ', 'Section', ' 3.5.1', ')', ' were', ' 98.37', ' for the', ' machine-', 'learning', ' system', ',', ' 95.82', ' for the', ' rule-based', ' system', ',', ' and 92.45', ' for the', 'program. Adjusting', ' a', ' machine-learning', ' solution', ' to', ' a', ' new', ' corpus', ' is', 'dis', 'cussed', ' in', ' [Zhang', ' et al', '.', ',', ' 2003', ']', '.', 'Examples', ' of', ' part-of-speech', ' taggers', ' are', ' [Ratna', 'parkhi', ',', ' 1995', ']', ' and', '[', 'Brill', ',', ' 1995', ']. The', ' Brill', ' tagger', ' is', ' in', ' ', 'the', ' public', ' domain', ' and', ' is', ' in', ' wide', 'use', '.', 'For', ' a', ' survey', ' of', ' work', ' on', ' word', ' sense', ' disambiguation', ',', ' see', ' [Ide', 'and', ' V', 'éronis', ',', ' 1998', ']. Wordnet', ' is', ' discussed', ' in', ' [', 'Feldbaum', ',', ' 1998', ']. The', 'database', ' and', ' a', ' program', ' to', ' use', ' it', ' can', ' be', ' obtained', ' from', ' the', ' ', 'Internet', 'at', ' http', ':', '//www.cogsci.princeton.edu/', 'wn/obtain.shtml', '.', '∼', '\\x0c2.13', ' Historical', ' and', ' ', 'Bibliographical', ' ', 'Remarks', '45', 'Phrase', ' recognition', ' is', ' also', ' known', ' as', ' “text', ' chunking', '” [Sang', ' and', ' ', 'Buch-', 'holz', ',', ' 2000]. A number', ' of', ' researchers', ' have', ' investigated', ' ', 'this', ' problem', 'as', ' ', 'classi', 'ﬁcation', ',', ' beginning', ' with', ' ', 'Ramshaw', ' and', ' Marcus', ' [Ramshaw', 'and', ' M', 'arcus', ',', ' 1995', ']. A', ' variety', ' of', ' machine-learning', ' algorithms', ' have', 'been', ' used', ':', ' support', ' vector machines', ' [Kudoh', ' and', ' Matsumoto', ',', ' 2000', ']', ',', 'trans', 'formation-based', ' learning', ' [Ramshaw', ' and', ' Marcus', ',', ' ', '1995', ']', ',', ' a linear', 'classi', 'ﬁer', ' [Zhang', ' et al', '.', ',', ' 2002', ']', ',', ' and', ' others. Many', ' more', ' details', ' can', ' be', 'signll', '/', 'conll.html', '.', 'found', ' at', ' http', ':', '//pi0657.uvt.nl', '/', 'Work', ' on', ' named', ' entity', ' recognition', ' has', ' been', ' heavily', ' funded', ' by', ' ', 'the', 'U.S. gove', 'rnment', ',', ' beginning', ' with', ' the', ' ', 'Message', ' ', 'Understanding', ' Con-', 'ferences', ' and', ' continuing', ' with', ' the', ' ACE project', ' ', 'Further', ' details', ' can', ' be', 'obtained', ' from', ' the', ' following', ' sites', ':', '∼', 'http', ':', '//www.itl.nist.gov/iaui/894.02/related_projects/muc/index.html', 'http', ':', '//www.itl.nist.gov/iaui/894.01/tests/ace/index.htm', 'A', ' number', ' ', 'of', ' named', ' entity', ' recognition', ' systems', ' are', ' availa', 'ble', ' for', ' license', ',', 'such', ' as', ' the', ' ', 'Nymble', ' system', ' from', ' BBN [Bikel', ' et', ' al.', ',', ' 1997', ']. State', ' ', 'of', ' the', 'art', ' is', ' about', ' 90%', ' in', ' recognition', ' accuracy', '.', 'Algorithms', ' for constructing', ' context-free', ' parse', ' trees', ' are', ' dis', 'cussed', 'in', ' [Earley', ',', ' 1970', ']', ' and', ' [Tomita', ',', ' 1985', ']. Constructing', ' parsers', ' from', ' tree', '-', 'banks', ' is', ' discussed', ' in', ' [', 'Charniak', ',', ' ', '1997', ']', ',', ' [', 'Ratnaparkhi', ',', ' ', '1999', ']', ',', ' [', 'Chiang', ',', '2000', ']', ',', ' and', ' others. A desc', 'ription', ' ', 'of', ' ', 'English', ' ', 'Slot', ' Grammar', ' can', ' be', ' found', 'in', ' [McCord', ',', ' ', '1989', ']. A number', ' of', ' organizations', ' have', ' posted', ' demo', ' ver', '-', 'sions', ' of', ' their', ' parsers', ' on', ' ', 'Web', ' sites. ', 'These', ' can', ' be', ' used', ' to', ' compare', ' the', 'out', 'put', ' of', ' different', ' parsers', ' on', ' the', ' same', ' input', ' sentence. Three', ' examples', 'of', ' such', ' sites', ' are', ':', 'http', ':', '//www.cs.kun.nl/agﬂ/', 'http', ':', '//www.link.cs.cmu.edu/link/submit-sentence-4.html', 'http', ':', '//www.lings', 'oft.ﬁ/cgi-bin/engcg', 'Early', ' work', ' in feature', ' generation', ' for document', ' classi', 'ﬁcation', ' includes', '[', 'Lewis', ',', ' ', '1992', ']', ',', ' [', 'Apt', 'é et', ' al', '.', ',', ' ', '1994', ']', ',', ' and', ' others. [Tan', ' et', ' al', '.', ',', ' 2002', ']', ' showed', 'that', ' bigrams', ' plus', ' single', ' words', ' improved', ' categorization', ' per', 'formance', 'in', ' a', ' collection of', ' ', 'Web', ' pages', ' from', ' ', 'the', ' ', 'Yahoo', ' ', 'Science', ' groups. ', 'The', ' better', 'per', 'formance', ' was attributed', ' to', ' an', ' increase', ' in recall. A special', ' issue', 'of', ' the', ' ', 'Journal', ' of', ' ', 'Machine', ' ', 'Learning', ' Research', ',', ' in 2003', ' was', ' devoted', ' to', 'feature', ' selection', ' and', ' is', ' available', ' online. One', ' of', ' the', ' papers', ' [Forman', ',', '2003]', ' presents', ' experiments', ' on', ' various', ' methods', ' for', ' feature', ' reduction', '.', 'A', ' use', 'ful', ' reference', ' on', ' word', ' selection', ' methods', ' for', ' dimensionality re', '-', 'duction', ' is', ' [Yang', ' and', ' ', 'Pedersen', ',', ' ', '1997', ']', ',', ' which', ' discusses', ' a', ' wide', ' variety', '\\x0c46', '2. From', ' ', 'Textual', ' ', 'Information', ' to', ' ', 'Numerical', ' ', 'Vectors', 'of', ' methods', ' for', ' selecting', ' words', ' use', 'ful', ' in', ' categorization. It', ' concludes', 'that', ' document frequency', ' is', ' com', 'parable', ' in', ' per', 'formance', ' to', ' expensive', 'me', 'thods', ' such', ' as', ' information', ' gain', ' or', ' chi-square', '.', '\\x0c']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u0n3vWEKIync",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 314
        },
        "outputId": "41d95883-9bf1-41eb-805c-299153e744f3"
      },
      "source": [
        "from scipy.spatial.distance import cosine\n",
        "query_2={\"Understand Retrieval Model, regular expression and textual information\"}\n",
        "#query_token_raw_2= list(jieba.cut(query_2))\n",
        "query_token_raw_2 = (ws_driver(query_2, use_delim=True, batch_size=256, max_length=128))\n",
        "\n",
        "flat_list = list()\n",
        "query_token_raw_2 = flatten_list(query_token_raw_2)\n",
        "\n",
        "query_token_2 = set(query_token_raw_2).intersection(bag_words_2)\n",
        "#[term for term in query_token_raw_2 if term in bag_words_2]\n",
        "tfidf_query_2 =compute_tfidf_query(list(query_token_2),bag_words_idf_2) #calculate tfidf for query text\n",
        "\n",
        "# add tfidf of query text to tfidf of all doc and convert to dataframe\n",
        "tfidf_2[\"query\"]=tfidf_query_2\n",
        "import pandas as pd\n",
        "tfidf_df_2 = pd.DataFrame(tfidf_2).transpose()\n",
        "tfidf_df_2= tfidf_df_2.fillna(0) # replace all NaN by zero\n",
        "cosine_sim ={}\n",
        "for row in tfidf_df_2.index:\n",
        "  if row != \"query\":\n",
        "    cosine_sim[row]= 1-cosine(tfidf_df_2.loc[row],tfidf_df_2.loc[\"query\"])\n",
        "\n",
        "# the top 10 relevant document\n",
        "cosine_sim_top20 = dict(sorted(cosine_sim.items(), key=lambda item: item[1],reverse=True)[:20])\n",
        "\n",
        "#plot barchart\n",
        "import matplotlib.pyplot as plt\n",
        "data = cosine_sim_top20\n",
        "plt.barh(range(len(data)), list(data.values()), align='center', alpha=0.8)\n",
        "plt.yticks(range(len(data)), list(data.keys())) # label for y axis\n",
        "plt.xlabel('Smimilarity score')\n",
        "plt.ylabel('Course')\n",
        "# save graph\n",
        "plt.savefig(mydrive+\"barchart.png\", bbox_inches='tight', dpi=1200)\n",
        "plt.show()"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 925.08it/s]\n",
            "Inference: 100%|██████████| 1/1 [00:00<00:00, 36.69it/s]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bag_words_idf_2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hygQvNn_duDM",
        "outputId": "64c387c5-b8e7-46db-b366-19ebff0d4b7a"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'Verb': 0.6989700043360189,\n",
              " 'does': 0.6989700043360189,\n",
              " 'Corp. by': 0.6989700043360189,\n",
              " ' 2002': 0.6989700043360189,\n",
              " '  Stream': 0.6989700043360189,\n",
              " 'Additional': 0.6989700043360189,\n",
              " 'tiword': 0.6989700043360189,\n",
              " ' are so highly': 0.6989700043360189,\n",
              " ' sure': 0.6989700043360189,\n",
              " ' columns': 0.6989700043360189,\n",
              " '\\x0c2. Word': 0.6989700043360189,\n",
              " ' cells of': 0.6989700043360189,\n",
              " ' others. It': 0.6989700043360189,\n",
              " ' Tokens': 0.6989700043360189,\n",
              " '//github.com/ckiplab/ckiptagger': 0.6989700043360189,\n",
              " ' constant': 0.6989700043360189,\n",
              " ' b d g k m': 0.6989700043360189,\n",
              " ' company)': 0.6989700043360189,\n",
              " ' large': 0.22184874961635637,\n",
              " 'tions': 0.6989700043360189,\n",
              " ' languages': 0.3979400086720376,\n",
              " ' “in': 0.6989700043360189,\n",
              " ' on text': 0.6989700043360189,\n",
              " '個': 0.6989700043360189,\n",
              " ' Write': 0.3979400086720376,\n",
              " 'tokens. The': 0.6989700043360189,\n",
              " 'groups': 0.6989700043360189,\n",
              " '40': 0.6989700043360189,\n",
              " ' 15': 0.6989700043360189,\n",
              " ' infinitely': 0.6989700043360189,\n",
              " 'program': 0.6989700043360189,\n",
              " '+1)': 0.6989700043360189,\n",
              " '. When': 0.6989700043360189,\n",
              " ' [Earley': 0.6989700043360189,\n",
              " ' near': 0.6989700043360189,\n",
              " ' toge': 0.6989700043360189,\n",
              " ' [Kudoh': 0.6989700043360189,\n",
              " ' tagger': 0.6989700043360189,\n",
              " 'oubled': 0.6989700043360189,\n",
              " '.g. subject': 0.6989700043360189,\n",
              " ' count)': 0.6989700043360189,\n",
              " ' interrelation': 0.6989700043360189,\n",
              " 'language': 0.6989700043360189,\n",
              " '“Dictionary': 0.6989700043360189,\n",
              " ' sof': 0.6989700043360189,\n",
              " ' trims': 0.6989700043360189,\n",
              " 'Prob. distr': 0.6989700043360189,\n",
              " ' is truly': 0.6989700043360189,\n",
              " '  1': 0.6989700043360189,\n",
              " 'stopwords': 0.6989700043360189,\n",
              " ' etc)': 0.6989700043360189,\n",
              " ' step': 0.6989700043360189,\n",
              " ' serve': 0.6989700043360189,\n",
              " ' distribution': 0.6989700043360189,\n",
              " '2.9 Phrase': 0.6989700043360189,\n",
              " 'x naturally': 0.6989700043360189,\n",
              " ' Boole': 0.6989700043360189,\n",
              " ' a simpler': 0.6989700043360189,\n",
              " '4.8)': 0.6989700043360189,\n",
              " 'magine': 0.6989700043360189,\n",
              " '\\x0c3rd': 0.6989700043360189,\n",
              " 'q “Okapi/BM25 TF”': 0.6989700043360189,\n",
              " ' transfor-': 0.6989700043360189,\n",
              " ' writing': 0.6989700043360189,\n",
              " ' For': 0.6989700043360189,\n",
              " ' bon': 0.6989700043360189,\n",
              " 'component': 0.6989700043360189,\n",
              " 'نِ اﻟﻮَطَﻦِ ': 0.6989700043360189,\n",
              " 'combine': 0.6989700043360189,\n",
              " ' select': 0.3979400086720376,\n",
              " ' boundary': 0.6989700043360189,\n",
              " ' for providing': 0.6989700043360189,\n",
              " ' a word': 0.3979400086720376,\n",
              " 'vance': 0.6989700043360189,\n",
              " ' a hand-crafted': 0.6989700043360189,\n",
              " ' is by': 0.6989700043360189,\n",
              " 'val': 0.3979400086720376,\n",
              " ' node': 0.6989700043360189,\n",
              " 'features)': 0.6989700043360189,\n",
              " 'large': 0.6989700043360189,\n",
              " ' by ': 0.6989700043360189,\n",
              " 'cy': 0.22184874961635637,\n",
              " 'dictionary': 0.6989700043360189,\n",
              " ' with different': 0.6989700043360189,\n",
              " ' used. In': 0.6989700043360189,\n",
              " 'of the': 0.6989700043360189,\n",
              " 'ij': 0.6989700043360189,\n",
              " ' further': 0.22184874961635637,\n",
              " ' ind': 0.6989700043360189,\n",
              " ' Your': 0.6989700043360189,\n",
              " ' they do': 0.6989700043360189,\n",
              " ' 0.00001': 0.6989700043360189,\n",
              " 'forma': 0.6989700043360189,\n",
              " ' phrases. ': 0.6989700043360189,\n",
              " '\\x0c4': 0.6989700043360189,\n",
              " ' feedback and': 0.6989700043360189,\n",
              " ' for document': 0.6989700043360189,\n",
              " 'Concepts': 0.6989700043360189,\n",
              " 'Otherwise': 0.6989700043360189,\n",
              " ' boundaries': 0.6989700043360189,\n",
              " ' consider the': 0.6989700043360189,\n",
              " ' like': 0.22184874961635637,\n",
              " 'freq(T))': 0.6989700043360189,\n",
              " ' Word': 0.6989700043360189,\n",
              " 'Consider': 0.6989700043360189,\n",
              " ' correct': 0.6989700043360189,\n",
              " ' result of': 0.6989700043360189,\n",
              " ' token. If': 0.6989700043360189,\n",
              " ' application. ': 0.6989700043360189,\n",
              " 'd1 3.5': 0.3979400086720376,\n",
              " 'XYZ Corp.1': 0.6989700043360189,\n",
              " ' researchers': 0.6989700043360189,\n",
              " ' come': 0.6989700043360189,\n",
              " '1999': 0.6989700043360189,\n",
              " ' vie': 0.6989700043360189,\n",
              " ' 98.37': 0.6989700043360189,\n",
              " ' preparation. We': 0.6989700043360189,\n",
              " ' characteristics': 0.6989700043360189,\n",
              " ' object': 0.6989700043360189,\n",
              " ' application': 0.6989700043360189,\n",
              " ' 7': 0.6989700043360189,\n",
              " 'plete': 0.6989700043360189,\n",
              " ' particular meaning': 0.6989700043360189,\n",
              " ' artifact': 0.6989700043360189,\n",
              " ' considering': 0.6989700043360189,\n",
              " ' value. By': 0.6989700043360189,\n",
              " ' d5 ': 0.6989700043360189,\n",
              " ' C': 0.6989700043360189,\n",
              " ' a “hint”': 0.6989700043360189,\n",
              " ' rather': 0.6989700043360189,\n",
              " ' pass': 0.6989700043360189,\n",
              " ' Words': 0.6989700043360189,\n",
              " ' 50%': 0.6989700043360189,\n",
              " ' text mining': 0.6989700043360189,\n",
              " 'justified': 0.6989700043360189,\n",
              " ' arise': 0.6989700043360189,\n",
              " ' Law': 0.6989700043360189,\n",
              " ' storing': 0.6989700043360189,\n",
              " ' part rule-based': 0.6989700043360189,\n",
              " 'ﬁcant': 0.6989700043360189,\n",
              " 'Monetary': 0.6989700043360189,\n",
              " ' NIST': 0.6989700043360189,\n",
              " ' representing': 0.6989700043360189,\n",
              " 'French': 0.6989700043360189,\n",
              " 'Ad': 0.6989700043360189,\n",
              " 'put': 0.6989700043360189,\n",
              " ' analy': 0.6989700043360189,\n",
              " ' formats': 0.6989700043360189,\n",
              " ' adjacent to': 0.6989700043360189,\n",
              " 'Nymble': 0.6989700043360189,\n",
              " ' classes). The': 0.6989700043360189,\n",
              " ' discussions': 0.6989700043360189,\n",
              " ' structure. A com': 0.6989700043360189,\n",
              " 'ber': 0.6989700043360189,\n",
              " ' than': 0.09691001300805642,\n",
              " ' references': 0.6989700043360189,\n",
              " ' adjectives': 0.6989700043360189,\n",
              " '(otherwise': 0.6989700043360189,\n",
              " ' eed': 0.6989700043360189,\n",
              " '\\x0cCKIP ': 0.6989700043360189,\n",
              " 'If current': 0.6989700043360189,\n",
              " 'D7': 0.6989700043360189,\n",
              " 'nutrition': 0.6989700043360189,\n",
              " ' details': 0.6989700043360189,\n",
              " ' practically': 0.6989700043360189,\n",
              " ' and': 0.0,\n",
              " 'VBG': 0.6989700043360189,\n",
              " ' not been': 0.6989700043360189,\n",
              " ' augmented': 0.6989700043360189,\n",
              " 'net': 0.6989700043360189,\n",
              " ' corres': 0.3979400086720376,\n",
              " 'corres': 0.6989700043360189,\n",
              " 'Averaging': 0.6989700043360189,\n",
              " 'abel': 0.6989700043360189,\n",
              " 'effective': 0.6989700043360189,\n",
              " 'ros': 0.6989700043360189,\n",
              " ' their': 0.6989700043360189,\n",
              " ' comparative': 0.6989700043360189,\n",
              " 'Boole': 0.6989700043360189,\n",
              " ' carry': 0.6989700043360189,\n",
              " 'User': 0.3979400086720376,\n",
              " '0.3': 0.6989700043360189,\n",
              " ' tag': 0.6989700043360189,\n",
              " ' quotes': 0.6989700043360189,\n",
              " ' containing': 0.6989700043360189,\n",
              " ' accept': 0.6989700043360189,\n",
              " 'with ': 0.6989700043360189,\n",
              " '  text': 0.6989700043360189,\n",
              " ' module': 0.6989700043360189,\n",
              " ' trivial': 0.6989700043360189,\n",
              " ' occurring': 0.6989700043360189,\n",
              " ' first': 0.3979400086720376,\n",
              " ' Remove': 0.6989700043360189,\n",
              " ' surprisingly': 0.6989700043360189,\n",
              " ' verb ven': 0.6989700043360189,\n",
              " 'Tokenization': 0.6989700043360189,\n",
              " ' window': 0.6989700043360189,\n",
              " ' as a result': 0.6989700043360189,\n",
              " 'commonly': 0.6989700043360189,\n",
              " ' 1970s': 0.6989700043360189,\n",
              " ' long-running': 0.6989700043360189,\n",
              " ' in dictionary': 0.6989700043360189,\n",
              " 'Problems': 0.6989700043360189,\n",
              " 'one. ': 0.6989700043360189,\n",
              " ' 1970': 0.6989700043360189,\n",
              " ' independent': 0.22184874961635637,\n",
              " ' yield': 0.6989700043360189,\n",
              " ' application-dependent. For': 0.6989700043360189,\n",
              " '\\x0cNext ': 0.6989700043360189,\n",
              " '3. Retransform': 0.6989700043360189,\n",
              " '” would': 0.6989700043360189,\n",
              " 'collection. Special': 0.6989700043360189,\n",
              " ' a middle': 0.6989700043360189,\n",
              " ' Assumptions': 0.6989700043360189,\n",
              " ' others. Mutual': 0.6989700043360189,\n",
              " ' words. Which': 0.6989700043360189,\n",
              " '26': 0.6989700043360189,\n",
              " ' other hand': 0.6989700043360189,\n",
              " ' list': 0.6989700043360189,\n",
              " ' our spre': 0.6989700043360189,\n",
              " ' break': 0.6989700043360189,\n",
              " ' setting': 0.6989700043360189,\n",
              " ' specifying': 0.6989700043360189,\n",
              " ' define': 0.3979400086720376,\n",
              " 'Smith1': 0.6989700043360189,\n",
              " '\\x0c': 0.0,\n",
              " ' models': 0.22184874961635637,\n",
              " 'Boolean': 0.6989700043360189,\n",
              " ' complexity to': 0.6989700043360189,\n",
              " ' computation': 0.6989700043360189,\n",
              " ' include': 0.6989700043360189,\n",
              " ' [Porter': 0.6989700043360189,\n",
              " 'Flat': 0.6989700043360189,\n",
              " '\\x0cIDF ': 0.6989700043360189,\n",
              " 'delete': 0.6989700043360189,\n",
              " ' parse': 0.6989700043360189,\n",
              " 'All ': 0.6989700043360189,\n",
              " '0.9': 0.6989700043360189,\n",
              " '2.8        3.3': 0.6989700043360189,\n",
              " '“importance': 0.6989700043360189,\n",
              " ' POS': 0.3979400086720376,\n",
              " ' circumstances': 0.6989700043360189,\n",
              " 'Wide': 0.6989700043360189,\n",
              " 'feedback': 0.3979400086720376,\n",
              " 'so': 0.6989700043360189,\n",
              " ' simple. For': 0.6989700043360189,\n",
              " ' over': 0.22184874961635637,\n",
              " 'T) ': 0.6989700043360189,\n",
              " 'To mine': 0.6989700043360189,\n",
              " ' words. Instead': 0.6989700043360189,\n",
              " 'Logical': 0.6989700043360189,\n",
              " ' particular that the': 0.6989700043360189,\n",
              " '------------------------------------------------------------------': 0.6989700043360189,\n",
              " 'Breaking': 0.6989700043360189,\n",
              " ' delimiters': 0.6989700043360189,\n",
              " ' appear': 0.6989700043360189,\n",
              " ' American': 0.6989700043360189,\n",
              " 'cessing': 0.6989700043360189,\n",
              " '   1': 0.6989700043360189,\n",
              " 'supply': 0.6989700043360189,\n",
              " 'Standard': 0.6989700043360189,\n",
              " 'National': 0.09691001300805642,\n",
              " 'roversy': 0.6989700043360189,\n",
              " ' hashtable': 0.6989700043360189,\n",
              " ' complicated': 0.6989700043360189,\n",
              " ' refer to': 0.6989700043360189,\n",
              " '51': 0.6989700043360189,\n",
              " 'their': 0.6989700043360189,\n",
              " 'slate': 0.6989700043360189,\n",
              " '21': 0.3979400086720376,\n",
              " ' xi': 0.6989700043360189,\n",
              " 'most': 0.6989700043360189,\n",
              " 'Science': 0.22184874961635637,\n",
              " '•': 0.22184874961635637,\n",
              " '\" ': 0.6989700043360189,\n",
              " 'dure': 0.6989700043360189,\n",
              " ' preposi': 0.6989700043360189,\n",
              " '\\x0c2.1': 0.3979400086720376,\n",
              " ' Stopwords': 0.6989700043360189,\n",
              " '50': 0.6989700043360189,\n",
              " ' p-norm': 0.6989700043360189,\n",
              " '“partial': 0.6989700043360189,\n",
              " 'ﬁlters': 0.6989700043360189,\n",
              " 'ffective': 0.6989700043360189,\n",
              " ' instance': 0.6989700043360189,\n",
              " ']. The': 0.6989700043360189,\n",
              " ' units/terms': 0.6989700043360189,\n",
              " 'product': 0.6989700043360189,\n",
              " ' \"': 0.3979400086720376,\n",
              " 'fs ': 0.6989700043360189,\n",
              " ' in memory': 0.6989700043360189,\n",
              " 'M': 0.6989700043360189,\n",
              " ' 2.4': 0.6989700043360189,\n",
              " ' extracted': 0.6989700043360189,\n",
              " ' must decide': 0.6989700043360189,\n",
              " ' broad': 0.6989700043360189,\n",
              " '’s rise. As': 0.6989700043360189,\n",
              " 'time': 0.6989700043360189,\n",
              " ' perfor-': 0.6989700043360189,\n",
              " '’s study': 0.6989700043360189,\n",
              " '.----- subj': 0.6989700043360189,\n",
              " '. Besides': 0.6989700043360189,\n",
              " ' q = q1': 0.6989700043360189,\n",
              " 'Retrieval': 0.22184874961635637,\n",
              " ' to by-pass': 0.6989700043360189,\n",
              " 'scores': 0.6989700043360189,\n",
              " ' average': 0.3979400086720376,\n",
              " ' project': 0.22184874961635637,\n",
              " ' element. ': 0.6989700043360189,\n",
              " 'beginning': 0.6989700043360189,\n",
              " ' Jieba': 0.6989700043360189,\n",
              " 'Since': 0.6989700043360189,\n",
              " '=True': 0.6989700043360189,\n",
              " '---': 0.6989700043360189,\n",
              " 'ous': 0.6989700043360189,\n",
              " ' experiment': 0.6989700043360189,\n",
              " 'generated': 0.6989700043360189,\n",
              " '2.5': 0.6989700043360189,\n",
              " ' dk}': 0.6989700043360189,\n",
              " 'sk ': 0.6989700043360189,\n",
              " ' al.': 0.6989700043360189,\n",
              " 'say': 0.6989700043360189,\n",
              " ' prior': 0.6989700043360189,\n",
              " '3. Doc5': 0.6989700043360189,\n",
              " 'rall': 0.6989700043360189,\n",
              " 'Different': 0.6989700043360189,\n",
              " 'Notice': 0.6989700043360189,\n",
              " ' testing': 0.3979400086720376,\n",
              " ' words': 0.09691001300805642,\n",
              " ' static': 0.6989700043360189,\n",
              " ' analyzed': 0.6989700043360189,\n",
              " 'George': 0.6989700043360189,\n",
              " ' purpose': 0.3979400086720376,\n",
              " ' che': 0.6989700043360189,\n",
              " ' + 2': 0.6989700043360189,\n",
              " ' human': 0.6989700043360189,\n",
              " ' reach': 0.6989700043360189,\n",
              " ' ambiguity': 0.3979400086720376,\n",
              " 'Topic': 0.6989700043360189,\n",
              " ' documents': 0.22184874961635637,\n",
              " ' <SUBJECT>': 0.6989700043360189,\n",
              " '= number': 0.6989700043360189,\n",
              " 'a difference. In': 0.6989700043360189,\n",
              " 'return': 0.6989700043360189,\n",
              " ' location': 0.6989700043360189,\n",
              " 'Penn': 0.6989700043360189,\n",
              " 'Py': 0.6989700043360189,\n",
              " 'Sense': 0.6989700043360189,\n",
              " ' sequen': 0.6989700043360189,\n",
              " 'q static': 0.6989700043360189,\n",
              " ' a text. In': 0.6989700043360189,\n",
              " 'L2': 0.6989700043360189,\n",
              " 'parts': 0.6989700043360189,\n",
              " ' assessed': 0.6989700043360189,\n",
              " ' string': 0.22184874961635637,\n",
              " ' ex': 0.6989700043360189,\n",
              " '2.3)': 0.6989700043360189,\n",
              " ' interpretability of': 0.6989700043360189,\n",
              " ' for considering': 0.6989700043360189,\n",
              " ' us': 0.22184874961635637,\n",
              " ' ch ': 0.6989700043360189,\n",
              " 'Johnson.” An': 0.6989700043360189,\n",
              " ' me': 0.6989700043360189,\n",
              " ' ware': 0.6989700043360189,\n",
              " 'Identifying': 0.6989700043360189,\n",
              " '2.3. In': 0.6989700043360189,\n",
              " ' expensive': 0.3979400086720376,\n",
              " ' Con-': 0.6989700043360189,\n",
              " 'stemming': 0.6989700043360189,\n",
              " ' leads': 0.6989700043360189,\n",
              " '\\\\': 0.6989700043360189,\n",
              " 'ject': 0.6989700043360189,\n",
              " ' null': 0.6989700043360189,\n",
              " '= substring': 0.6989700043360189,\n",
              " 'vant. ': 0.6989700043360189,\n",
              " ' grows': 0.6989700043360189,\n",
              " ' vector machines': 0.6989700043360189,\n",
              " ' on word': 0.6989700043360189,\n",
              " ' examples': 0.3979400086720376,\n",
              " ' previous': 0.6989700043360189,\n",
              " ' line': 0.3979400086720376,\n",
              " '2.4 ': 0.6989700043360189,\n",
              " 'you': 0.3979400086720376,\n",
              " ' ask me': 0.6989700043360189,\n",
              " 'Determiner': 0.6989700043360189,\n",
              " ' interest': 0.6989700043360189,\n",
              " 'any': 0.6989700043360189,\n",
              " ' ten': 0.6989700043360189,\n",
              " ' zeta': 0.6989700043360189,\n",
              " ' a closing': 0.6989700043360189,\n",
              " '  all': 0.6989700043360189,\n",
              " ' recognition of': 0.6989700043360189,\n",
              " ' parses': 0.6989700043360189,\n",
              " 'Vectors': 0.6989700043360189,\n",
              " ' natural': 0.22184874961635637,\n",
              " 'Weighting': 0.6989700043360189,\n",
              " ' 98)': 0.6989700043360189,\n",
              " ' document will': 0.6989700043360189,\n",
              " 'roduct': 0.6989700043360189,\n",
              " 'Corp': 0.6989700043360189,\n",
              " ' when': 0.6989700043360189,\n",
              " ' expansion': 0.6989700043360189,\n",
              " ' zeroing': 0.6989700043360189,\n",
              " ' not counted': 0.6989700043360189,\n",
              " ' re.sub': 0.6989700043360189,\n",
              " ' into machine': 0.6989700043360189,\n",
              " ' to conve': 0.6989700043360189,\n",
              " ' relevant to': 0.6989700043360189,\n",
              " 'Service': 0.22184874961635637,\n",
              " ' parses. One': 0.6989700043360189,\n",
              " 'do': 0.3979400086720376,\n",
              " 'exam': 0.6989700043360189,\n",
              " 'Register': 0.6989700043360189,\n",
              " ' exceeds': 0.6989700043360189,\n",
              " ' evident': 0.6989700043360189,\n",
              " 'NNS ': 0.6989700043360189,\n",
              " 'types. As': 0.6989700043360189,\n",
              " ' organized': 0.6989700043360189,\n",
              " 'ﬂy': 0.6989700043360189,\n",
              " 'INDEXING': 0.6989700043360189,\n",
              " 'Copy': 0.6989700043360189,\n",
              " 'Position': 0.6989700043360189,\n",
              " 'score': 0.6989700043360189,\n",
              " ' general': 0.3979400086720376,\n",
              " 'forman': 0.6989700043360189,\n",
              " 'Google': 0.3979400086720376,\n",
              " ' to solve. ': 0.6989700043360189,\n",
              " ' down': 0.3979400086720376,\n",
              " ' fol': 0.6989700043360189,\n",
              " 'Element': 0.6989700043360189,\n",
              " 'phrases)': 0.6989700043360189,\n",
              " ' 0.3': 0.6989700043360189,\n",
              " ' ch is not': 0.6989700043360189,\n",
              " 'when': 0.6989700043360189,\n",
              " 'United': 0.6989700043360189,\n",
              " '1 ': 0.6989700043360189,\n",
              " ' B-NP': 0.6989700043360189,\n",
              " 'most-frequently-used': 0.6989700043360189,\n",
              " 'tion': 0.6989700043360189,\n",
              " ' Mapping': 0.6989700043360189,\n",
              " ' centroid': 0.6989700043360189,\n",
              " ' ambiguity. ': 0.6989700043360189,\n",
              " ' same': 0.22184874961635637,\n",
              " ' cle': 0.6989700043360189,\n",
              " ' text': 0.22184874961635637,\n",
              " ' numeric': 0.6989700043360189,\n",
              " ' core': 0.6989700043360189,\n",
              " ' construct': 0.6989700043360189,\n",
              " ' 0.29': 0.6989700043360189,\n",
              " ' Collocations': 0.6989700043360189,\n",
              " 'treat': 0.6989700043360189,\n",
              " 'Linguistic': 0.6989700043360189,\n",
              " ' found': 0.3979400086720376,\n",
              " ')': 0.0,\n",
              " 'ples': 0.6989700043360189,\n",
              " '3. Doc3  ': 0.6989700043360189,\n",
              " 'choice. Pseudocode': 0.6989700043360189,\n",
              " ' l': 0.6989700043360189,\n",
              " ' importance': 0.3979400086720376,\n",
              " '= request': 0.6989700043360189,\n",
              " 'Blind': 0.6989700043360189,\n",
              " 'values': 0.3979400086720376,\n",
              " '\\x0cP': 0.6989700043360189,\n",
              " 'dk 0.5': 0.3979400086720376,\n",
              " ' well': 0.6989700043360189,\n",
              " 'Some': 0.6989700043360189,\n",
              " 'ment': 0.6989700043360189,\n",
              " 'ling': 0.6989700043360189,\n",
              " 'Local': 0.6989700043360189,\n",
              " ' readily': 0.6989700043360189,\n",
              " ' end-of-sentence': 0.6989700043360189,\n",
              " ' memory. A': 0.6989700043360189,\n",
              " 'Word Rank': 0.6989700043360189,\n",
              " ' sharing': 0.6989700043360189,\n",
              " ' agencies': 0.6989700043360189,\n",
              " ' form. ': 0.6989700043360189,\n",
              " ' algorithms': 0.6989700043360189,\n",
              " ' dash between': 0.6989700043360189,\n",
              " ' training': 0.6989700043360189,\n",
              " ' d8': 0.6989700043360189,\n",
              " 'Each': 0.3979400086720376,\n",
              " ' - ’ may': 0.6989700043360189,\n",
              " ' do ': 0.6989700043360189,\n",
              " ' use': 0.0,\n",
              " ' manual': 0.6989700043360189,\n",
              " ' similar rules': 0.6989700043360189,\n",
              " ' 200': 0.6989700043360189,\n",
              " ' difference': 0.3979400086720376,\n",
              " ' others': 0.3979400086720376,\n",
              " 'retrie': 0.6989700043360189,\n",
              " 'LOB)': 0.6989700043360189,\n",
              " 'else': 0.6989700043360189,\n",
              " ' scraped': 0.6989700043360189,\n",
              " ' intrinsic': 0.6989700043360189,\n",
              " 'r ': 0.6989700043360189,\n",
              " 'Stopwords': 0.3979400086720376,\n",
              " ' that collects': 0.6989700043360189,\n",
              " 'of times': 0.6989700043360189,\n",
              " 'ods': 0.6989700043360189,\n",
              " ' value': 0.6989700043360189,\n",
              " '+ ': 0.3979400086720376,\n",
              " ' subset': 0.3979400086720376,\n",
              " ' Collections': 0.6989700043360189,\n",
              " ' feature. Thus': 0.6989700043360189,\n",
              " 'True': 0.6989700043360189,\n",
              " ' ’ and': 0.6989700043360189,\n",
              " 'Our choices': 0.6989700043360189,\n",
              " ' contrast': 0.6989700043360189,\n",
              " ' accessible': 0.6989700043360189,\n",
              " ' into stems': 0.6989700043360189,\n",
              " ' looks': 0.6989700043360189,\n",
              " 'varying': 0.6989700043360189,\n",
              " '\"': 0.22184874961635637,\n",
              " 'me': 0.6989700043360189,\n",
              " 'regarding': 0.6989700043360189,\n",
              " ' restrict': 0.6989700043360189,\n",
              " ' getting': 0.6989700043360189,\n",
              " 'structure': 0.6989700043360189,\n",
              " ' scenarios': 0.6989700043360189,\n",
              " ' extracting': 0.6989700043360189,\n",
              " 'Punctuation': 0.6989700043360189,\n",
              " 'Principle': 0.6989700043360189,\n",
              " ' problem. Let': 0.6989700043360189,\n",
              " ' from': 0.0,\n",
              " ' varies': 0.3979400086720376,\n",
              " ' resources': 0.6989700043360189,\n",
              " ' words. In': 0.6989700043360189,\n",
              " 'restrictive': 0.6989700043360189,\n",
              " ' it possible': 0.6989700043360189,\n",
              " 'Singhal': 0.6989700043360189,\n",
              " 'Byte': 0.6989700043360189,\n",
              " ' language-dependent. ': 0.6989700043360189,\n",
              " ' pattern': 0.3979400086720376,\n",
              " ' simpli': 0.6989700043360189,\n",
              " ' + dynamic': 0.6989700043360189,\n",
              " '.0': 0.6989700043360189,\n",
              " '∼': 0.6989700043360189,\n",
              " ' popular words': 0.6989700043360189,\n",
              " 'n Top': 0.6989700043360189,\n",
              " '9': 0.6989700043360189,\n",
              " ' Research': 0.6989700043360189,\n",
              " 'Rare': 0.6989700043360189,\n",
              " ' available. ': 0.6989700043360189,\n",
              " ' in Practice': 0.6989700043360189,\n",
              " ' must': 0.3979400086720376,\n",
              " ' capability': 0.6989700043360189,\n",
              " ' algorith': 0.6989700043360189,\n",
              " ' corresponding': 0.6989700043360189,\n",
              " 'it is': 0.6989700043360189,\n",
              " 'نَ أﻣِﯿْﻨَﺎً وَﺻَﺎدِﻗَﺎً ﻣَﻊَ ﻧَﻔْﺴِﮫِ وَﻣَﻊَ أَھْﻠِﮫِ ': 0.6989700043360189,\n",
              " ' earlier. However': 0.6989700043360189,\n",
              " ' vase. In': 0.6989700043360189,\n",
              " 'Input': 0.6989700043360189,\n",
              " ' model. Another': 0.6989700043360189,\n",
              " ' measured': 0.6989700043360189,\n",
              " ' scale': 0.6989700043360189,\n",
              " ' ab*c': 0.6989700043360189,\n",
              " 'GPT2': 0.6989700043360189,\n",
              " ' “iPhone': 0.6989700043360189,\n",
              " ' needs': 0.6989700043360189,\n",
              " 'used': 0.3979400086720376,\n",
              " '\\x0c32': 0.6989700043360189,\n",
              " ' or words': 0.6989700043360189,\n",
              " 'clude': 0.6989700043360189,\n",
              " ' sim': 0.6989700043360189,\n",
              " ' convert': 0.6989700043360189,\n",
              " ' In': 0.6989700043360189,\n",
              " 'mance': 0.6989700043360189,\n",
              " ' image': 0.6989700043360189,\n",
              " 'j ': 0.6989700043360189,\n",
              " 'Formula': 0.6989700043360189,\n",
              " ' institution': 0.6989700043360189,\n",
              " ' a ranking': 0.6989700043360189,\n",
              " ' occurrences': 0.09691001300805642,\n",
              " ' gen': 0.6989700043360189,\n",
              " ' server': 0.6989700043360189,\n",
              " 'Non-over': 0.6989700043360189,\n",
              " 'domain': 0.6989700043360189,\n",
              " 'loaded': 0.6989700043360189,\n",
              " ' NII': 0.6989700043360189,\n",
              " '<AUTHOR>': 0.6989700043360189,\n",
              " 'University': 0.0,\n",
              " ' conjunction': 0.6989700043360189,\n",
              " ' reducing': 0.6989700043360189,\n",
              " ' providing': 0.6989700043360189,\n",
              " ' spelling': 0.6989700043360189,\n",
              " ' in a sentence': 0.6989700043360189,\n",
              " 'Docs Filtered': 0.6989700043360189,\n",
              " ' modestly': 0.6989700043360189,\n",
              " '</DOC>': 0.6989700043360189,\n",
              " ' weights': 0.6989700043360189,\n",
              " ' contains': 0.6989700043360189,\n",
              " ' ζ': 0.6989700043360189,\n",
              " ' feature': 0.3979400086720376,\n",
              " ' P': 0.6989700043360189,\n",
              " ' predic-': 0.6989700043360189,\n",
              " ' often': 0.6989700043360189,\n",
              " 'ﬁable': 0.6989700043360189,\n",
              " 'Inter': 0.6989700043360189,\n",
              " ' correlated': 0.3979400086720376,\n",
              " ' rein': 0.6989700043360189,\n",
              " 'thorough': 0.6989700043360189,\n",
              " 'idf)': 0.6989700043360189,\n",
              " ' know': 0.3979400086720376,\n",
              " '“he': 0.6989700043360189,\n",
              " ' grounds': 0.6989700043360189,\n",
              " '5. ': 0.6989700043360189,\n",
              " ' TREC': 0.6989700043360189,\n",
              " ' before': 0.3979400086720376,\n",
              " 'Rankings': 0.6989700043360189,\n",
              " 'cluding': 0.6989700043360189,\n",
              " ' processors': 0.6989700043360189,\n",
              " 'frequency/document': 0.6989700043360189,\n",
              " ' “us': 0.6989700043360189,\n",
              " ' method': 0.3979400086720376,\n",
              " 'that the': 0.3979400086720376,\n",
              " 'tionary. The': 0.6989700043360189,\n",
              " ' dimension': 0.6989700043360189,\n",
              " 'onants': 0.6989700043360189,\n",
              " 'we': 0.6989700043360189,\n",
              " 'be': 0.6989700043360189,\n",
              " ' laptop': 0.6989700043360189,\n",
              " '2.12 ': 0.6989700043360189,\n",
              " ' remain': 0.6989700043360189,\n",
              " ' integrity': 0.6989700043360189,\n",
              " ' currently available': 0.6989700043360189,\n",
              " ' vi': 0.6989700043360189,\n",
              " 'fect': 0.6989700043360189,\n",
              " 'Markup': 0.6989700043360189,\n",
              " '\\x0cPseudo': 0.6989700043360189,\n",
              " ' organizations': 0.6989700043360189,\n",
              " ' com': 0.6989700043360189,\n",
              " ' it occurs': 0.6989700043360189,\n",
              " '2.10. Parse': 0.6989700043360189,\n",
              " 'Key': 0.6989700043360189,\n",
              " 'much more': 0.6989700043360189,\n",
              " ' samples': 0.6989700043360189,\n",
              " '4. ': 0.6989700043360189,\n",
              " 'see': 0.6989700043360189,\n",
              " ' “': 0.22184874961635637,\n",
              " ' placing': 0.6989700043360189,\n",
              " ' aim': 0.6989700043360189,\n",
              " 'ﬁer': 0.6989700043360189,\n",
              " '11': 0.6989700043360189,\n",
              " ' within': 0.6989700043360189,\n",
              " \"'t\": 0.6989700043360189,\n",
              " 'aries. Since': 0.6989700043360189,\n",
              " ' normalization': 0.22184874961635637,\n",
              " 'text to serve': 0.6989700043360189,\n",
              " ' topic. ': 0.6989700043360189,\n",
              " ' parts. ': 0.6989700043360189,\n",
              " 'f': 0.3979400086720376,\n",
              " '*IDF': 0.6989700043360189,\n",
              " ' sophisticated': 0.3979400086720376,\n",
              " '>': 0.3979400086720376,\n",
              " ' or clustering': 0.6989700043360189,\n",
              " 'n R': 0.6989700043360189,\n",
              " ' \"pseudo\"': 0.6989700043360189,\n",
              " ' abstracts': 0.6989700043360189,\n",
              " 'that': 0.22184874961635637,\n",
              " ' 0.02': 0.6989700043360189,\n",
              " ' it will': 0.3979400086720376,\n",
              " ' a distinguishing': 0.6989700043360189,\n",
              " ' feature. For': 0.6989700043360189,\n",
              " 'Journal': 0.6989700043360189,\n",
              " 'Belief': 0.6989700043360189,\n",
              " ' adds': 0.6989700043360189,\n",
              " ' performance': 0.6989700043360189,\n",
              " ' word segmentation': 0.6989700043360189,\n",
              " ' small': 0.3979400086720376,\n",
              " '2. punkt': 0.6989700043360189,\n",
              " 'structural': 0.6989700043360189,\n",
              " 'D3': 0.6989700043360189,\n",
              " ' availa': 0.6989700043360189,\n",
              " ' im': 0.6989700043360189,\n",
              " 'onably': 0.6989700043360189,\n",
              " 'part': 0.6989700043360189,\n",
              " ' in recall. A special': 0.6989700043360189,\n",
              " ' nature': 0.6989700043360189,\n",
              " ' stream': 0.6989700043360189,\n",
              " ' VS ': 0.6989700043360189,\n",
              " ' again': 0.6989700043360189,\n",
              " ' others. Many': 0.6989700043360189,\n",
              " 'Rejected': 0.6989700043360189,\n",
              " ' non-relevant': 0.6989700043360189,\n",
              " 'fully': 0.3979400086720376,\n",
              " ' Recall': 0.6989700043360189,\n",
              " ' three': 0.3979400086720376,\n",
              " 'idf': 0.6989700043360189,\n",
              " ' unlikely': 0.6989700043360189,\n",
              " ' occurren': 0.6989700043360189,\n",
              " ' top': 0.3979400086720376,\n",
              " 'Finally': 0.6989700043360189,\n",
              " 'ffectively': 0.6989700043360189,\n",
              " ' require': 0.6989700043360189,\n",
              " 'JJR': 0.6989700043360189,\n",
              " 'row': 0.6989700043360189,\n",
              " 'about': 0.6989700043360189,\n",
              " ' system': 0.22184874961635637,\n",
              " ' a phrase': 0.6989700043360189,\n",
              " ' pad': 0.6989700043360189,\n",
              " ' rele': 0.6989700043360189,\n",
              " 'proposed': 0.6989700043360189,\n",
              " ' categorize': 0.6989700043360189,\n",
              " 'Repeated': 0.6989700043360189,\n",
              " ' interpret': 0.6989700043360189,\n",
              " 'applications. The': 0.6989700043360189,\n",
              " ' 01a)': 0.6989700043360189,\n",
              " 'over': 0.6989700043360189,\n",
              " ' source': 0.6989700043360189,\n",
              " 'Without': 0.6989700043360189,\n",
              " ' interest. ': 0.6989700043360189,\n",
              " ' means': 0.6989700043360189,\n",
              " 'Docs': 0.6989700043360189,\n",
              " ' likely': 0.3979400086720376,\n",
              " '7': 0.22184874961635637,\n",
              " 'ponding': 0.6989700043360189,\n",
              " ' adapted': 0.6989700043360189,\n",
              " '  TF': 0.6989700043360189,\n",
              " ' j is': 0.6989700043360189,\n",
              " ' in a': 0.6989700043360189,\n",
              " ' from LDC ': 0.6989700043360189,\n",
              " ' fixed': 0.6989700043360189,\n",
              " ' license': 0.6989700043360189,\n",
              " 'food': 0.6989700043360189,\n",
              " '” ': 0.3979400086720376,\n",
              " 'Generalized': 0.3979400086720376,\n",
              " ' check': 0.3979400086720376,\n",
              " ' pick': 0.6989700043360189,\n",
              " ' j)': 0.6989700043360189,\n",
              " 'sary': 0.6989700043360189,\n",
              " 'Parliament': 0.6989700043360189,\n",
              " ' right': 0.3979400086720376,\n",
              " ' AND ': 0.6989700043360189,\n",
              " 'txc': 0.6989700043360189,\n",
              " 'Section': 0.6989700043360189,\n",
              " ' body': 0.6989700043360189,\n",
              " ' problem. Predictions': 0.6989700043360189,\n",
              " ' Shen': 0.6989700043360189,\n",
              " 'normalization': 0.6989700043360189,\n",
              " ' under': 0.6989700043360189,\n",
              " ' wait': 0.6989700043360189,\n",
              " ' Tradition': 0.6989700043360189,\n",
              " ' with B- and': 0.6989700043360189,\n",
              " ' feature. The': 0.6989700043360189,\n",
              " ' stem': 0.6989700043360189,\n",
              " ' data-mining': 0.6989700043360189,\n",
              " ' language': 0.22184874961635637,\n",
              " ' for some': 0.6989700043360189,\n",
              " 'tiwords': 0.6989700043360189,\n",
              " ' is lowered': 0.6989700043360189,\n",
              " '“seek': 0.6989700043360189,\n",
              " ' classification': 0.3979400086720376,\n",
              " 'Vector Space': 0.6989700043360189,\n",
              " 'For': 0.3979400086720376,\n",
              " ' most': 0.22184874961635637,\n",
              " 'must': 0.6989700043360189,\n",
              " ' side': 0.6989700043360189,\n",
              " ' try': 0.3979400086720376,\n",
              " 'cell. We': 0.6989700043360189,\n",
              " '\\x0cSummary': 0.3979400086720376,\n",
              " 'factor': 0.3979400086720376,\n",
              " ' as a training': 0.6989700043360189,\n",
              " '=0': 0.22184874961635637,\n",
              " ' 77': 0.6989700043360189,\n",
              " ' relevant doc': 0.6989700043360189,\n",
              " '  log': 0.6989700043360189,\n",
              " ' or through ': 0.6989700043360189,\n",
              " ' such': 0.22184874961635637,\n",
              " 'relevance': 0.6989700043360189,\n",
              " '1)': 0.3979400086720376,\n",
              " 'ability': 0.6989700043360189,\n",
              " ' versions': 0.6989700043360189,\n",
              " ' a completely': 0.6989700043360189,\n",
              " 'pling': 0.6989700043360189,\n",
              " ' National': 0.6989700043360189,\n",
              " '\\x0cProbability Ranking': 0.6989700043360189,\n",
              " ' by y': 0.6989700043360189,\n",
              " '\\x0c3': 0.6989700043360189,\n",
              " ' ALBERT': 0.6989700043360189,\n",
              " ' hand': 0.6989700043360189,\n",
              " ' EO': 0.6989700043360189,\n",
              " 'Coordination': 0.6989700043360189,\n",
              " ' f': 0.22184874961635637,\n",
              " ' is to': 0.6989700043360189,\n",
              " ' features. If': 0.6989700043360189,\n",
              " ' cons': 0.6989700043360189,\n",
              " 'Statistic ': 0.6989700043360189,\n",
              " 'Klein': 0.6989700043360189,\n",
              " 'past': 0.6989700043360189,\n",
              " ' page': 0.3979400086720376,\n",
              " 'per': 0.6989700043360189,\n",
              " ' us con': 0.6989700043360189,\n",
              " 'discarded': 0.6989700043360189,\n",
              " ' particular countries. A similar set': 0.6989700043360189,\n",
              " ' search': 0.3979400086720376,\n",
              " ' which': 0.22184874961635637,\n",
              " '= charAt': 0.6989700043360189,\n",
              " ' not give': 0.6989700043360189,\n",
              " ' collect': 0.6989700043360189,\n",
              " ' constructs': 0.6989700043360189,\n",
              " ' obtain': 0.6989700043360189,\n",
              " 'ﬁ': 0.6989700043360189,\n",
              " 'part-of-spee': 0.6989700043360189,\n",
              " ' estimated': 0.6989700043360189,\n",
              " 'Frequent': 0.6989700043360189,\n",
              " ' showed': 0.6989700043360189,\n",
              " ' comma': 0.6989700043360189,\n",
              " ' and remove': 0.6989700043360189,\n",
              " ' Method': 0.6989700043360189,\n",
              " ' is reasona': 0.6989700043360189,\n",
              " 'Archive': 0.6989700043360189,\n",
              " '\\x0c1. Regular ': 0.6989700043360189,\n",
              " 'English. For': 0.6989700043360189,\n",
              " ' all have': 0.6989700043360189,\n",
              " ' somewhat': 0.6989700043360189,\n",
              " ' sparse': 0.6989700043360189,\n",
              " ' simply': 0.6989700043360189,\n",
              " '1995': 0.6989700043360189,\n",
              " ' = tf': 0.6989700043360189,\n",
              " ' presents': 0.6989700043360189,\n",
              " 'Loper': 0.6989700043360189,\n",
              " 'dy': 0.6989700043360189,\n",
              " 'NLP tools': 0.6989700043360189,\n",
              " ' being': 0.6989700043360189,\n",
              " ' example)': 0.6989700043360189,\n",
              " 'particularly': 0.6989700043360189,\n",
              " 'g': 0.6989700043360189,\n",
              " ' parsers': 0.6989700043360189,\n",
              " 'n So': 0.6989700043360189,\n",
              " 'q+ +': 0.6989700043360189,\n",
              " 'distance': 0.6989700043360189,\n",
              " 'Components': 0.3979400086720376,\n",
              " ' 500': 0.6989700043360189,\n",
              " 'log': 0.3979400086720376,\n",
              " 'Disambiguation': 0.6989700043360189,\n",
              " ' evaluated': 0.6989700043360189,\n",
              " ' approach': 0.3979400086720376,\n",
              " ' is to have': 0.6989700043360189,\n",
              " ' intent': 0.6989700043360189,\n",
              " 'jumping': 0.6989700043360189,\n",
              " 'word': 0.22184874961635637,\n",
              " 'Bank': 0.6989700043360189,\n",
              " 'adding': 0.6989700043360189,\n",
              " ' appears': 0.6989700043360189,\n",
              " 'Work': 0.6989700043360189,\n",
              " ' or binary': 0.6989700043360189,\n",
              " ' America. ': 0.6989700043360189,\n",
              " ' et': 0.3979400086720376,\n",
              " ' these': 0.22184874961635637,\n",
              " ' unstructured': 0.6989700043360189,\n",
              " ' dictiona': 0.6989700043360189,\n",
              " '= empty': 0.6989700043360189,\n",
              " ' usuall': 0.6989700043360189,\n",
              " '斷詞': 0.6989700043360189,\n",
              " ' step. ': 0.6989700043360189,\n",
              " ' representing ': 0.6989700043360189,\n",
              " 'Johnson1': 0.6989700043360189,\n",
              " 'doc3': 0.6989700043360189,\n",
              " 'reduced': 0.6989700043360189,\n",
              " ' whether or': 0.6989700043360189,\n",
              " ' desired': 0.6989700043360189,\n",
              " 'English grammars': 0.6989700043360189,\n",
              " ' aimed': 0.6989700043360189,\n",
              " ' class)': 0.6989700043360189,\n",
              " 'Bird': 0.6989700043360189,\n",
              " 'Cooper': 0.6989700043360189,\n",
              " ' person. ': 0.6989700043360189,\n",
              " 'A': 0.3979400086720376,\n",
              " ' available': 0.6989700043360189,\n",
              " 'information': 0.3979400086720376,\n",
              " ' coordinate': 0.6989700043360189,\n",
              " ' Classifiers': 0.6989700043360189,\n",
              " ' sentence. Properly': 0.6989700043360189,\n",
              " ' on the': 0.6989700043360189,\n",
              " '\\x0cThree': 0.6989700043360189,\n",
              " ' lists have': 0.6989700043360189,\n",
              " ' unknown': 0.6989700043360189,\n",
              " ' content/topic': 0.6989700043360189,\n",
              " 'Which': 0.6989700043360189,\n",
              " \"'\": 0.3979400086720376,\n",
              " ' called': 0.6989700043360189,\n",
              " ' mechanism': 0.6989700043360189,\n",
              " 'N is': 0.6989700043360189,\n",
              " 'boolean': 0.6989700043360189,\n",
              " ' potential features': 0.6989700043360189,\n",
              " ' and corresponds': 0.6989700043360189,\n",
              " ' size': 0.6989700043360189,\n",
              " 'reduce': 0.6989700043360189,\n",
              " ' for marking': 0.6989700043360189,\n",
              " 'intelligence': 0.6989700043360189,\n",
              " ' Part-Of-Spee': 0.6989700043360189,\n",
              " '<DOC>': 0.6989700043360189,\n",
              " '的': 0.6989700043360189,\n",
              " ' lack': 0.6989700043360189,\n",
              " ' consecutive. ': 0.6989700043360189,\n",
              " 'n System-based': 0.6989700043360189,\n",
              " ' “applications': 0.6989700043360189,\n",
              " ' py': 0.6989700043360189,\n",
              " ' Martin': 0.6989700043360189,\n",
              " ' for predictive': 0.6989700043360189,\n",
              " ' or ': 0.3979400086720376,\n",
              " ' numbers': 0.6989700043360189,\n",
              " 'n': 0.22184874961635637,\n",
              " ' generate': 0.3979400086720376,\n",
              " ' A possible': 0.6989700043360189,\n",
              " 'entries': 0.6989700043360189,\n",
              " ' discuss': 0.6989700043360189,\n",
              " 'If prediction': 0.6989700043360189,\n",
              " ' articles': 0.6989700043360189,\n",
              " ' rank': 0.6989700043360189,\n",
              " 'science': 0.6989700043360189,\n",
              " 'category)': 0.6989700043360189,\n",
              " ' interesting': 0.6989700043360189,\n",
              " 'puter': 0.6989700043360189,\n",
              " ' etc.)': 0.6989700043360189,\n",
              " ' single-word': 0.6989700043360189,\n",
              " ' indirect': 0.6989700043360189,\n",
              " ' for multiword': 0.6989700043360189,\n",
              " '1': 0.0,\n",
              " ' justifications': 0.6989700043360189,\n",
              " ' to which': 0.3979400086720376,\n",
              " 'Similarly': 0.6989700043360189,\n",
              " 'rent': 0.6989700043360189,\n",
              " ' µ similarity': 0.6989700043360189,\n",
              " ' determined': 0.6989700043360189,\n",
              " 'Root': 0.6989700043360189,\n",
              " ' j': 0.6989700043360189,\n",
              " ' di = di1': 0.6989700043360189,\n",
              " 'Instead': 0.6989700043360189,\n",
              " ' commonly': 0.3979400086720376,\n",
              " 'Detection': 0.6989700043360189,\n",
              " ' arbit': 0.6989700043360189,\n",
              " ' represented': 0.3979400086720376,\n",
              " 'adsheet. The': 0.6989700043360189,\n",
              " 'hat': 0.3979400086720376,\n",
              " ' if': 0.09691001300805642,\n",
              " ' producing': 0.6989700043360189,\n",
              " 'If SS': 0.6989700043360189,\n",
              " ' has a': 0.6989700043360189,\n",
              " '“do': 0.6989700043360189,\n",
              " '2.5)': 0.6989700043360189,\n",
              " ' exact': 0.6989700043360189,\n",
              " 'Extended': 0.3979400086720376,\n",
              " ' \"or': 0.6989700043360189,\n",
              " ' expended': 0.6989700043360189,\n",
              " 'Proximal': 0.6989700043360189,\n",
              " 'doc3)': 0.6989700043360189,\n",
              " ' patte': 0.6989700043360189,\n",
              " '//bit.ly/3vwjDGi': 0.6989700043360189,\n",
              " 'Term-weighting': 0.6989700043360189,\n",
              " 'V': 0.3979400086720376,\n",
              " ' fs': 0.6989700043360189,\n",
              " ' content of': 0.6989700043360189,\n",
              " ' sentence. Three': 0.6989700043360189,\n",
              " ' * frequency': 0.6989700043360189,\n",
              " 'sam': 0.6989700043360189,\n",
              " ' your': 0.3979400086720376,\n",
              " ' logging': 0.6989700043360189,\n",
              " 'Pennsylvania': 0.6989700043360189,\n",
              " '<AUTHORS>': 0.6989700043360189,\n",
              " ' vase. An': 0.6989700043360189,\n",
              " ' whose': 0.6989700043360189,\n",
              " '43': 0.3979400086720376,\n",
              " ' speedy': 0.6989700043360189,\n",
              " '+': 0.22184874961635637,\n",
              " ' re.findall': 0.6989700043360189,\n",
              " 'VBD': 0.6989700043360189,\n",
              " ' Case': 0.6989700043360189,\n",
              " ' &  similarity': 0.6989700043360189,\n",
              " ' [Ray': 0.6989700043360189,\n",
              " ' corresponds': 0.6989700043360189,\n",
              " ' continuing': 0.6989700043360189,\n",
              " ' present/past': 0.6989700043360189,\n",
              " ' LISTSERVs. ': 0.6989700043360189,\n",
              " 'Chiang': 0.6989700043360189,\n",
              " '29': 0.3979400086720376,\n",
              " ' see': 0.6989700043360189,\n",
              " ' 1980s': 0.6989700043360189,\n",
              " ' differ': 0.6989700043360189,\n",
              " '53': 0.6989700043360189,\n",
              " 'generated. ': 0.6989700043360189,\n",
              " ' = f': 0.6989700043360189,\n",
              " 'vide': 0.6989700043360189,\n",
              " 'replace1': 0.6989700043360189,\n",
              " ' at several': 0.6989700043360189,\n",
              " ' practitioners': 0.6989700043360189,\n",
              " ' hundred. Most': 0.6989700043360189,\n",
              " ' for words': 0.6989700043360189,\n",
              " ' token to': 0.6989700043360189,\n",
              " 'af': 0.6989700043360189,\n",
              " ' in randomly': 0.6989700043360189,\n",
              " 'Robertson': 0.6989700043360189,\n",
              " 'q}': 0.3979400086720376,\n",
              " 'thods': 0.6989700043360189,\n",
              " ' what features': 0.6989700043360189,\n",
              " 'retrieval models': 0.6989700043360189,\n",
              " '” query': 0.6989700043360189,\n",
              " 'ﯾَﺠِﺐُ ﻋَﻠَﻰ اﻹﻧْﺴَﺎ': 0.6989700043360189,\n",
              " ' both': 0.09691001300805642,\n",
              " ...}"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    }
  ]
}